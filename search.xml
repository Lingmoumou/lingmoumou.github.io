<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ambari中Heartbeat lost问题</title>
    <url>/p/2019/06/11/2992bf74/</url>
    <content><![CDATA[<p>公司采用HDP的大数据平台，周六周日突然停电后，周一上班通过ambari发现组件heartbeat都lost。查了一圈，发现应该是正好碰到了低版本HDP的bug。下面是实践后有效的解决方式。<br><a id="more"></a></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/2992bf74/ed3e16f24c88e74bffe8d2f1df0846ce.png?x-oss-process=style/blog" alt="ambari-agent.log"></p>
<ol>
<li>确保防火墙关闭，能够ping通ambari-server服务所在机器的IP</li>
<li><p>升级openssl服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum upgrade openssl</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭 openssl 的检查</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'s/verify=platform_default/verify=disable/'</span> /etc/python/cert-verification.cfg</span><br></pre></td></tr></table></figure>
</li>
<li><p>先停掉ambari-server，然后依次停掉各个ambari-agent</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ambari-server stop</span><br><span class="line">ambari-agent stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>在所有ambari-agent节点上，修改ambari-agent.ini文件的[security]，新增force_https_protocol如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ambari-agent/conf/ambari-agent.ini</span><br><span class="line">[security]</span><br><span class="line">force_https_protocol=PROTOCOL_TLSv1_2</span><br></pre></td></tr></table></figure>
</li>
<li><p>依次开启ambari-agent，最后开启ambari-server</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ambari-server start</span><br><span class="line">ambari-agent start</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<p>参考：<br><a href="https://community.hortonworks.com/questions/121978/openssl-compatibility.html?childToView=138080#answer-138080" target="_blank" rel="noopener">https://community.hortonworks.com/questions/121978/openssl-compatibility.html?childToView=138080#answer-138080</a></p>
]]></content>
      <categories>
        <category>FAQ</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hdp</tag>
        <tag>ambari</tag>
        <tag>heartbeat</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式简介</title>
    <url>/p/2020/02/05/8a7312b/</url>
    <content><![CDATA[<blockquote>
<p>软件设计模式（Design pattern），又称设计模式，是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性、程序的重用性。</p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>设计模式(英语 design pattern)是对面向对象设计中<strong>反复出现的问题</strong>的解决方案。这个术语是在1990年代由Erich Gamma等人从建筑设计领域引入到计算机科学中来的。这个术语的含义还存有争议。算法不是设计模式，因为算法致力于解决问题而非设计问题。设计模式通常描述了一组相互紧密作用的类与对象。设计模式提供一种讨论软件设计的公共语言，使得熟练设计者的设计经验可以被初学者和其他设计者掌握。设计模式还为软件重构提供了目标。</p>
<h3 id="什么是-GOF（四人帮，全拼-Gang-of-Four）？"><a href="#什么是-GOF（四人帮，全拼-Gang-of-Four）？" class="headerlink" title="什么是 GOF（四人帮，全拼 Gang of Four）？"></a>什么是 GOF（四人帮，全拼 Gang of Four）？</h3><p>在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书，该书首次提到了软件开发中设计模式的概念。</p>
<p>四位作者合称 GOF（四人帮，全拼 Gang of Four）。他们所提出的设计模式主要是基于以下的面向对象设计原则。</p>
<ul>
<li><strong>对接口编程而不是对实现编程</strong>。</li>
<li><strong>优先使用对象组合而不是继承</strong>。</li>
</ul>
<a id="more"></a>
<h2 id="基本要素"><a href="#基本要素" class="headerlink" title="基本要素"></a>基本要素</h2><p>软件设计模式使人们可以更加简单方便地复用成功的设计和体系结构，它通常包含以下几个基本要素：模式名称、别名、动机、问题、解决方案、效果、结构、模式角色、合作关系、实现方法、适用性、已知应用、例程、模式扩展和相关模式等，其中最关键的元素包括以下 4 个主要部分。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/Q20200205213002.png" alt="软件模式基本结构"></p>
<ol>
<li>模式名称<br>每一个模式都有自己的名字，通常用一两个词来描述，可以根据模式的问题、特点、解决方案、功能和效果来命名。模式名称（PatternName）有助于我们理解和记忆该模式，也方便我们来讨论自己的设计。</li>
<li>问题<br>问题（Problem）描述了该模式的应用环境，即何时使用该模式。它解释了设计问题和问题存在的前因后果，以及必须满足的一系列先决条件。</li>
<li>解决方案<br>模式问题的解决方案（Solution）包括设计的组成成分、它们之间的相互关系及各自的职责和协作方式。因为模式就像一个模板，可应用于多种不同场合，所以解决方案并不描述一个特定而具体的设计或实现，而是提供设计问题的抽象描述和怎样用一个具有一般意义的元素组合（类或对象的 组合）来解决这个问题。</li>
<li>效果<br>描述了模式的应用效果以及使用该模式应该权衡的问题，即模式的优缺点。主要是对时间和空间的衡量，以及该模式对系统的灵活性、扩充性、可移植性的影响，也考虑其实现问题。显式地列出这些效果（Consequence）对理解和评价这些模式有很大的帮助。</li>
</ol>
<h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><table>
<thead>
<tr>
<th style="text-align:left">设计原则名称</th>
<th style="text-align:left">定 义</th>
<th style="text-align:center">使用频率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="#开闭原则">开闭原则 (Open-Closed Principle, OCP)</a></td>
<td style="text-align:left">软件实体应对扩展开放，而对修改关闭</td>
<td style="text-align:center">★★★★★</td>
</tr>
<tr>
<td style="text-align:left"><a href="#单一职责原则">单一职责原则 (Single Responsibility Principle, SRP)</a></td>
<td style="text-align:left">一个类只负责一个功能领域中的相应职责</td>
<td style="text-align:center">★★★★☆</td>
</tr>
<tr>
<td style="text-align:left"><a href="#里氏代换原则">里氏代换原则 (Liskov Substitution Principle, LSP)</a></td>
<td style="text-align:left">所有引用基类对象的地方能够透明地使用其子类的对象</td>
<td style="text-align:center">★★★★★</td>
</tr>
<tr>
<td style="text-align:left"><a href="#依赖倒转原则">依赖倒转原则 (Dependence Inversion Principle, DIP)</a></td>
<td style="text-align:left">抽象不应该依赖于细节，细节应该依赖于抽象</td>
<td style="text-align:center">★★★★★</td>
</tr>
<tr>
<td style="text-align:left"><a href="#接口隔离原则">接口隔离原则 (Interface Segregation Principle, ISP)</a></td>
<td style="text-align:left">使用多个专门的接口，而不使用单一的总接口</td>
<td style="text-align:center">★★☆☆☆</td>
</tr>
<tr>
<td style="text-align:left"><a href="#迪米特法则">迪米特法则 (Law of Demeter, LoD)或称最少知识原则(Least Knowledge Principle，LKP)</a></td>
<td style="text-align:left">一个软件实体应当尽可能少地与其他实体发生相互作用</td>
<td style="text-align:center">★★★☆☆</td>
</tr>
<tr>
<td style="text-align:left"><a href="#合成复用原则">合成复用原则 (Composite Reuse Principle, CRP)</a></td>
<td style="text-align:left">尽量使用对象组合，而不是继承来达到复用的目的</td>
<td style="text-align:center">★★★★☆</td>
</tr>
</tbody>
</table>
<h3 id="开闭原则"><a href="#开闭原则" class="headerlink" title="开闭原则"></a>开闭原则</h3><p>开闭原则就是说对扩展开放，对修改关闭。当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。具体来说，其作用如下：</p>
<ul>
<li>对软件测试的影响<br>软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。</li>
<li>可以提高代码的可复用性<br>粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。</li>
<li>可以提高软件的可维护性<br>遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。</li>
</ul>
<p>可以通过“<strong>抽象约束、封装变化</strong>“来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。<br>因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_3.png" alt="某CRM中客户信息图形统计模块"></p>
<h3 id="单一职责原则"><a href="#单一职责原则" class="headerlink" title="单一职责原则"></a>单一职责原则</h3><p>这里的职责是指类变化的原因，单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。单一职责原则的核心就是控制类的粒度大小、<strong>将对象解耦、提高其内聚性</strong>。如果遵循单一职责原则将有以下优点：</p>
<ul>
<li>降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。</li>
<li>提高类的可读性。复杂性降低，自然其可读性会提高。</li>
<li>提高系统的可维护性。可读性提高，那自然更容易维护了。</li>
<li>变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。</li>
</ul>
<p>单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。而发现类的多重职责需要设计人员具有较强的分析设计能力和相关重构经验。</p>
<p>单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_2.png" alt="某CRM中客户信息图形统计模块"></p>
<h3 id="里氏代换原则"><a href="#里氏代换原则" class="headerlink" title="里氏代换原则"></a>里氏代换原则</h3><p>里氏替换原则主要阐述了有关继承的一些原则，也就是什么时候应该使用继承，什么时候不应该使用继承，以及其中蕴含的原理。里氏替换原则是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。具体来说，其作用如下：</p>
<ul>
<li>里氏替换原则是实现开闭原则的重要方式之一。</li>
<li>它克服了继承中重写父类造成的可复用性变差的缺点。</li>
<li>它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性。</li>
</ul>
<p>里氏替换原则通俗来讲就是：<strong>子类可以扩展父类的功能，但不能改变父类原有的功能</strong>。也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。</p>
<ul>
<li>如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频繁时，程序运行出错的概率会非常大。</li>
<li>如果程序违背了里氏替换原则，则继承类的对象在基类出现的地方会出现运行错误。这时其修正方法是：取消原来的继承关系，重新设计它们之间的关系。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_4.png" alt="某CRM中客户信息图形统计模块"></p>
<h3 id="依赖倒转原则"><a href="#依赖倒转原则" class="headerlink" title="依赖倒转原则"></a>依赖倒转原则</h3><p>依赖倒置原则的原始定义为：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。其核心思想是：要面向接口编程，不要面向实现编程。依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。具体来说，其作用如下：</p>
<ul>
<li>依赖倒置原则可以降低类间的耦合性。</li>
<li>依赖倒置原则可以提高系统的稳定性。</li>
<li>依赖倒置原则可以减少并行开发引起的风险。</li>
<li>依赖倒置原则可以提高代码的可读性和可维护性。</li>
</ul>
<p>依赖倒置原则的目的是通过要<strong>面向接口</strong>的编程来降低类间的耦合性，所以我们在实际编程中只要遵循以下几点，就能在项目中满足这个规则。</p>
<ul>
<li>每个类尽量提供接口或抽象类，或者两者都具备。</li>
<li>变量的声明类型尽量是接口或者是抽象类。</li>
<li>任何类都不应该从具体类派生。</li>
<li>使用继承时尽量遵循里氏替换原则。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_5.png" alt="某CRM中客户信息图形统计模块"></p>
<h3 id="接口隔离原则"><a href="#接口隔离原则" class="headerlink" title="接口隔离原则"></a>接口隔离原则</h3><p>要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。</p>
<blockquote>
<p>接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的：</p>
<ul>
<li>单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。</li>
<li>单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。</li>
</ul>
</blockquote>
<p>接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下5个优点：</p>
<ul>
<li>将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。</li>
<li>接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。</li>
<li>如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。</li>
<li>使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。</li>
<li>能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。</li>
</ul>
<p>在具体应用接口隔离原则时，应该根据以下几个规则来衡量。</p>
<ul>
<li>接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。</li>
<li>为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。</li>
<li>了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。</li>
<li>提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_6.png" alt="某CRM中客户信息图形统计模块"></p>
<h3 id="迪米特法则"><a href="#迪米特法则" class="headerlink" title="迪米特法则"></a>迪米特法则</h3><p>迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。</p>
<p>迪米特法则要求限制软件实体之间通信的宽度和深度，正确使用迪米特法则将有以下两个优点:</p>
<ul>
<li>降低了类之间的耦合度，提高了模块的相对独立性。</li>
<li>由于亲合度降低，从而提高了类的可复用率和系统的扩展性。</li>
</ul>
<p><em>但是，过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。所以，在釆用迪米特法则时需要反复权衡，确保高内聚和低耦合的同时，保证系统的结构清晰。</em></p>
<p>在运用迪米特法则时要注意以下几点：</p>
<ul>
<li>在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。</li>
<li>在类的结构设计上，尽量降低类成员的访问权限。</li>
<li>在类的设计上，优先考虑将一个类设置成不变类。</li>
<li>在对其他类的引用上，将引用其他对象的次数降到最低。</li>
<li>不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。</li>
<li>谨慎使用序列化（Serializable）功能。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_7.png" alt="某CRM中业务操作窗口模块"></p>
<h3 id="合成复用原则"><a href="#合成复用原则" class="headerlink" title="合成复用原则"></a>合成复用原则</h3><p>合成复用原则又叫组合/聚合复用原则（Composition/Aggregate Reuse Principle，CARP）。它要求在软件复用时，要尽量先<strong>使用组合或者聚合等关联关系</strong>来实现，其次才考虑使用继承关系来实现。如果要使用继承关系，则必须严格遵循里氏替换原则。合成复用原则同里氏替换原则相辅相成的，两者都是开闭原则的具体实现规范。</p>
<blockquote>
<p>通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。</p>
<ol>
<li>继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。</li>
<li>子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。</li>
<li>它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。</li>
</ol>
</blockquote>
<p>采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。</p>
<ul>
<li>它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。</li>
<li>新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。</li>
<li>复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。</li>
</ul>
<p>合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/page_8.png" alt="某CRM中客户信息图形统计模块"></p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/8a7312b/1354152786_2931.jpg" alt="设计模式之间的关系"></p>
<h3 id="创建型模式（Creational-Patterns）"><a href="#创建型模式（Creational-Patterns）" class="headerlink" title="创建型模式（Creational Patterns）"></a>创建型模式（Creational Patterns）</h3><p>创建型模式的主要关注点是“怎样创建对象？”，它的主要特点是“<strong>将对象的创建与使用分离</strong>”。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。就像我们去商场购买商品时，不需要知道商品是怎么生产出来一样，因为它们由专门的厂商生产。</p>
<p>创建型模式分为以下几种。</p>
<ul>
<li><strong>单例（Singleton）模式</strong>：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。</li>
<li><strong>原型（Prototype）模式</strong>：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。</li>
<li><strong>工厂方法（FactoryMethod）模式</strong>：定义一个用于创建产品的接口，由子类决定生产什么产品。</li>
<li><strong>抽象工厂（AbstractFactory）模式</strong>：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。</li>
<li><strong>建造者（Builder）模式</strong>：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。</li>
</ul>
<p>以上 5 种创建型模式，除了工厂方法模式属于类创建型模式，其他的全部属于对象创建型模式。</p>
<h4 id="单例模式（Singleton-Pattern）"><a href="#单例模式（Singleton-Pattern）" class="headerlink" title="单例模式（Singleton Pattern）"></a>单例模式（Singleton Pattern）</h4><blockquote>
<p>在有些系统中，为了节省内存资源、保证数据内容的一致性，对某些类要求只能创建一个实例，这就是所谓的单例模式。</p>
</blockquote>
<h4 id="原型模式（Prototype-Pattern）"><a href="#原型模式（Prototype-Pattern）" class="headerlink" title="原型模式（Prototype Pattern）"></a>原型模式（Prototype Pattern）</h4><h4 id="工厂模式（Factory-Pattern）"><a href="#工厂模式（Factory-Pattern）" class="headerlink" title="工厂模式（Factory Pattern）"></a>工厂模式（Factory Pattern）</h4><h4 id="抽象工厂模式（Abstract-Factory-Pattern）"><a href="#抽象工厂模式（Abstract-Factory-Pattern）" class="headerlink" title="抽象工厂模式（Abstract Factory Pattern）"></a>抽象工厂模式（Abstract Factory Pattern）</h4><h4 id="建造者模式（Builder-Pattern）"><a href="#建造者模式（Builder-Pattern）" class="headerlink" title="建造者模式（Builder Pattern）"></a>建造者模式（Builder Pattern）</h4><h3 id="结构型模式（Structural-Patterns）"><a href="#结构型模式（Structural-Patterns）" class="headerlink" title="结构型模式（Structural Patterns）"></a>结构型模式（Structural Patterns）</h3><p>结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。<br>由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。</p>
<p>结构型模式分为以下 7 种：</p>
<ul>
<li><strong>代理（Proxy）模式</strong>：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。</li>
<li><strong>适配器（Adapter）模式</strong>：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。</li>
<li><strong>桥接（Bridge）模式</strong>：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。</li>
<li><strong>装饰（Decorator）模式</strong>：动态地给对象增加一些职责，即增加其额外的功能。</li>
<li><strong>外观（Facade）模式</strong>：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。</li>
<li><strong>享元（Flyweight）模式</strong>：运用共享技术来有效地支持大量细粒度对象的复用。</li>
<li><strong>组合（Composite）模式</strong>：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。</li>
</ul>
<p>以上 7 种结构型模式，除了适配器模式分为类结构型模式和对象结构型模式两种，其他的全部属于对象结构型模式。</p>
<h4 id="适配器模式（Adapter-Pattern）"><a href="#适配器模式（Adapter-Pattern）" class="headerlink" title="适配器模式（Adapter Pattern）"></a>适配器模式（Adapter Pattern）</h4><h4 id="桥接模式（Bridge-Pattern）"><a href="#桥接模式（Bridge-Pattern）" class="headerlink" title="桥接模式（Bridge Pattern）"></a>桥接模式（Bridge Pattern）</h4><h4 id="过滤器模式（Filter、Criteria-Pattern）"><a href="#过滤器模式（Filter、Criteria-Pattern）" class="headerlink" title="过滤器模式（Filter、Criteria Pattern）"></a>过滤器模式（Filter、Criteria Pattern）</h4><h4 id="组合模式（Composite-Pattern）"><a href="#组合模式（Composite-Pattern）" class="headerlink" title="组合模式（Composite Pattern）"></a>组合模式（Composite Pattern）</h4><h4 id="装饰器模式（Decorator-Pattern）"><a href="#装饰器模式（Decorator-Pattern）" class="headerlink" title="装饰器模式（Decorator Pattern）"></a>装饰器模式（Decorator Pattern）</h4><h4 id="外观模式（Facade-Pattern）"><a href="#外观模式（Facade-Pattern）" class="headerlink" title="外观模式（Facade Pattern）"></a>外观模式（Facade Pattern）</h4><h4 id="享元模式（Flyweight-Pattern）"><a href="#享元模式（Flyweight-Pattern）" class="headerlink" title="享元模式（Flyweight Pattern）"></a>享元模式（Flyweight Pattern）</h4><h4 id="代理模式（Proxy-Pattern）"><a href="#代理模式（Proxy-Pattern）" class="headerlink" title="代理模式（Proxy Pattern）"></a>代理模式（Proxy Pattern）</h4><h3 id="行为型模式（Behavioral-Patterns）"><a href="#行为型模式（Behavioral-Patterns）" class="headerlink" title="行为型模式（Behavioral Patterns）"></a>行为型模式（Behavioral Patterns）</h3><p>行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。</p>
<p>行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。</p>
<p>行为型模式是 GoF 设计模式中最为庞大的一类，它包含以下 11 种模式。</p>
<ul>
<li><strong>模板方法（Template Method）模式</strong>：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。</li>
<li><strong>策略（Strategy）模式</strong>：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。</li>
<li><strong>命令（Command）模式</strong>：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。</li>
<li><strong>职责链（Chain of Responsibility）模式</strong>：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。</li>
<li><strong>状态（State）模式</strong>：允许一个对象在其内部状态发生改变时改变其行为能力。</li>
<li><strong>观察者（Observer）模式</strong>：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。</li>
<li><strong>中介者（Mediator）模式</strong>：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。</li>
<li><strong>迭代器（Iterator）模式</strong>：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。</li>
<li><strong>访问者（Visitor）模式</strong>：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。</li>
<li><strong>备忘录（Memento）模式</strong>：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。</li>
<li><strong>解释器（Interpreter）模式</strong>：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。</li>
</ul>
<p>以上 11 种行为型模式，除了模板方法模式和解释器模式是类行为型模式，其他的全部属于对象行为型模式，下面我们将详细介绍它们的特点、结构与应用。</p>
<h4 id="责任链模式（Chain-of-Responsibility-Pattern）"><a href="#责任链模式（Chain-of-Responsibility-Pattern）" class="headerlink" title="责任链模式（Chain of Responsibility Pattern）"></a>责任链模式（Chain of Responsibility Pattern）</h4><h4 id="命令模式（Command-Pattern）"><a href="#命令模式（Command-Pattern）" class="headerlink" title="命令模式（Command Pattern）"></a>命令模式（Command Pattern）</h4><h4 id="解释器模式（Interpreter-Pattern）"><a href="#解释器模式（Interpreter-Pattern）" class="headerlink" title="解释器模式（Interpreter Pattern）"></a>解释器模式（Interpreter Pattern）</h4><h4 id="迭代器模式（Iterator-Pattern）"><a href="#迭代器模式（Iterator-Pattern）" class="headerlink" title="迭代器模式（Iterator Pattern）"></a>迭代器模式（Iterator Pattern）</h4><h4 id="中介者模式（Mediator-Pattern）"><a href="#中介者模式（Mediator-Pattern）" class="headerlink" title="中介者模式（Mediator Pattern）"></a>中介者模式（Mediator Pattern）</h4><h4 id="备忘录模式（Memento-Pattern）"><a href="#备忘录模式（Memento-Pattern）" class="headerlink" title="备忘录模式（Memento Pattern）"></a>备忘录模式（Memento Pattern）</h4><h4 id="观察者模式（Observer-Pattern）"><a href="#观察者模式（Observer-Pattern）" class="headerlink" title="观察者模式（Observer Pattern）"></a>观察者模式（Observer Pattern）</h4><h4 id="状态模式（State-Pattern）"><a href="#状态模式（State-Pattern）" class="headerlink" title="状态模式（State Pattern）"></a>状态模式（State Pattern）</h4><h4 id="空对象模式（Null-Object-Pattern）"><a href="#空对象模式（Null-Object-Pattern）" class="headerlink" title="空对象模式（Null Object Pattern）"></a>空对象模式（Null Object Pattern）</h4><h4 id="策略模式（Strategy-Pattern）"><a href="#策略模式（Strategy-Pattern）" class="headerlink" title="策略模式（Strategy Pattern）"></a>策略模式（Strategy Pattern）</h4><h4 id="模板模式（Template-Pattern）"><a href="#模板模式（Template-Pattern）" class="headerlink" title="模板模式（Template Pattern）"></a>模板模式（Template Pattern）</h4><h4 id="访问者模式（Visitor-Pattern）"><a href="#访问者模式（Visitor-Pattern）" class="headerlink" title="访问者模式（Visitor Pattern）"></a>访问者模式（Visitor Pattern）</h4><h3 id="J2EE-模式"><a href="#J2EE-模式" class="headerlink" title="J2EE 模式"></a>J2EE 模式</h3><h4 id="MVC-模式（MVC-Pattern）"><a href="#MVC-模式（MVC-Pattern）" class="headerlink" title="MVC 模式（MVC Pattern）"></a>MVC 模式（MVC Pattern）</h4><h4 id="业务代表模式（Business-Delegate-Pattern）"><a href="#业务代表模式（Business-Delegate-Pattern）" class="headerlink" title="业务代表模式（Business Delegate Pattern）"></a>业务代表模式（Business Delegate Pattern）</h4><h4 id="组合实体模式（Composite-Entity-Pattern）"><a href="#组合实体模式（Composite-Entity-Pattern）" class="headerlink" title="组合实体模式（Composite Entity Pattern）"></a>组合实体模式（Composite Entity Pattern）</h4><h4 id="数据访问对象模式（Data-Access-Object-Pattern）"><a href="#数据访问对象模式（Data-Access-Object-Pattern）" class="headerlink" title="数据访问对象模式（Data Access Object Pattern）"></a>数据访问对象模式（Data Access Object Pattern）</h4><h4 id="前端控制器模式（Front-Controller-Pattern）"><a href="#前端控制器模式（Front-Controller-Pattern）" class="headerlink" title="前端控制器模式（Front Controller Pattern）"></a>前端控制器模式（Front Controller Pattern）</h4><h4 id="拦截过滤器模式（Intercepting-Filter-Pattern）"><a href="#拦截过滤器模式（Intercepting-Filter-Pattern）" class="headerlink" title="拦截过滤器模式（Intercepting Filter Pattern）"></a>拦截过滤器模式（Intercepting Filter Pattern）</h4><h4 id="服务定位器模式（Service-Locator-Pattern）"><a href="#服务定位器模式（Service-Locator-Pattern）" class="headerlink" title="服务定位器模式（Service Locator Pattern）"></a>服务定位器模式（Service Locator Pattern）</h4><h4 id="传输对象模式（Transfer-Object-Pattern）"><a href="#传输对象模式（Transfer-Object-Pattern）" class="headerlink" title="传输对象模式（Transfer Object Pattern）"></a>传输对象模式（Transfer Object Pattern）</h4><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[^3]：<a href="https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/2117635?fromtitle=%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F&amp;fromid=1212549&amp;fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/2117635?fromtitle=%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F&amp;fromid=1212549&amp;fr=aladdin</a></p>
]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Design Patterns</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>design patterns</tag>
      </tags>
  </entry>
  <entry>
    <title>Task1 线性回归&amp;Softmax与分类模型&amp;多层感知机</title>
    <url>/p/2020/02/13/966c0826/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为  的样本误差的表达式为</p>
<a id="more"></a>
<h3 id="优化函数-随机梯度下降"><a href="#优化函数-随机梯度下降" class="headerlink" title="优化函数 - 随机梯度下降"></a>优化函数 - 随机梯度下降</h3><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch），然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</p>
<p>学习率: 代表在每次优化中，能够学习的步长的大小<br>批量大小: 是小批量计算中的批量大小batch size</p>
<p>总结一下，优化函数的有以下两个步骤：</p>
<p>(i)初始化模型参数，一般来说使用随机初始化；<br>(ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。</p>
<h2 id="矢量计算"><a href="#矢量计算" class="headerlink" title="矢量计算"></a>矢量计算</h2><p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p>
<p>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。<br>向量相加的另一种方法是，将这两个向量直接做矢量加法。</p>
<h2 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>实现顺序</p>
<ol>
<li>生成数据集：随机标签，指定参数，计算标准结果添加噪声</li>
<li>定义模型</li>
<li>定义损失函数</li>
<li>定义优化模型</li>
<li>训练模型<ol>
<li>设置超参，初始化模型参数</li>
<li>每次迭代中，小批量读取数据，初始化模型计算预测值，损失函数计算插值，反向传播求梯度，优化算法更新参数，参数梯度清零</li>
</ol>
</li>
</ol>
<ul>
<li>数据集一般分为训练集、验证集、测试集</li>
<li>真实预测值称为label，用来预测标签的因素feature</li>
<li>预测结果偏差：损失函数（均方误差），可以直接求和多个样本</li>
<li>调整模型参数：优化函数（梯度下降，梯度反方向），迭代后损失函数值减小</li>
</ul>
<h1 id="Softmax与分类模型"><a href="#Softmax与分类模型" class="headerlink" title="Softmax与分类模型"></a>Softmax与分类模型</h1><blockquote>
<p>Softmax逻辑回归模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签y可以取两个以上的值。 Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习无监督学习方法的结合。</p>
</blockquote>
<p>在机器学习尤其是深度学习中，softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。<br>首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是max。对于max我们都很熟悉，比如有两个变量a,b。如果a&gt;b，则max为a，反之为b。用伪码简单描述一下就是<code>if a &gt; b return a; else b</code>。<br>另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。</p>
<h2 id="Softmax的定义-3"><a href="#Softmax的定义-3" class="headerlink" title="Softmax的定义^3"></a>Softmax的定义<a href="https://blog.csdn.net/bitcarmanlee/article/details/82320853" target="_blank" rel="noopener">^3</a></h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/966c0826/5236230-12cd299a8d571d1e.png" alt="求导过程"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/966c0826/5236230-c91debe62a989306.png" alt="求导过程"></p>
<h2 id="笔记整理-1"><a href="#笔记整理-1" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>这是由于softmax函数的常数不变性，即softmax(x)=softmax(x+c)，推导如下：<br>$$<br>softmax(x_i)=\frac{exp(x_i)}{\sum_jexp(x_j)}<br>$$</p>
<p>$$<br>(softmax(x+c))_i=\frac{exp(x_i+c)}{\sum_j exp(x_j+c)}=\frac{exp(c)exp(x_i)}{exp(c)\sum_jexp(x_j)}=\frac{exp(x_i)}{\sum_jexp(x_j)}=(softmax(x))_i<br>$$<br>上面的exp(c)之所以可以消除，是因为exp(a+b)=exp(a)*exp(b)这个特性将exp(c)提取出来了。<br>在计算softmax概率的时候，为了保证数值稳定性（numerical stability），我们可以选择给输入项减去一个常数，比如x的每个元素都要减去一个x中的最大元素。当输入项很大的时候，如果不减这样一个常数，取指数之后结果会变得非常大，发生溢出的现象，导致结果出现inf。</p>
<p>softmax函数是来自于sigmoid函数在多分类情况下的推广，他们的相同之处：</p>
<ol>
<li>都具有良好的数据压缩能力是实数域R→[ 0 , 1 ]的映射函数，可以将杂乱无序没有实际含义的数字直接转化为每个分类的可能性概率。</li>
<li>都具有非常漂亮的导数形式，便于反向传播计算。</li>
<li>它们都是 soft version of max ，都可以将数据的差异明显化。</li>
</ol>
<p>相同的，他们具有着不同的特点，sigmoid函数可以看成softmax函数的特例，softmax函数也可以看作sigmoid函数的推广。</p>
<ol>
<li>sigmoid函数前提假设是样本服从伯努利 (Bernoulli) 分布的假设，而softmax则是基于多项式分布。首先证明多项分布属于指数分布族，这样就可以使用广义线性模型来拟合这个多项分布，由广义线性模型推导出的目标函数即为Softmax回归的分类模型。 </li>
<li>sigmoid函数用于分辨每一种情况的可能性，所以用sigmoid函数实现多分类问题的时候，概率并不是归一的，反映的是每个情况的发生概率，因此非互斥的问题使用sigmoid函数可以获得比较漂亮的结果；softmax函数最初的设计思路适用于首先数字识别这样的互斥的多分类问题，因此进行了归一化操作，使得最后预测的结果是唯一的。</li>
</ol>
<p>softmax基本概念</p>
<ul>
<li>分类问题: 用来预测种类，图像识别，离散数值来表示不同类别</li>
<li>权重失量</li>
<li>神经网络图<br>单层神经网络，类别与变量间对应关系</li>
<li>输出问题<ol>
<li>范围不确定</li>
<li>真实为离散值，输出不确定范围误差难以衡量</li>
</ol>
</li>
<li>softmax值变换为值为正和为1，不改变预测类别输出</li>
<li>小批量矢量计算</li>
<li>交叉熵，保证正确预测类别的值大<br><code>print(X.sum(dim=0, keepdim=True))#按相同列求和，保留列特征</code><br><code>print(X.sum(dim=1, keepdim=True))#按相同行求和，保留行特征</code></li>
</ul>
<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>多层感知器（MLP，Multilayer Perceptron）是一种前馈人工神经网络模型，其将输入的多个数据集映射到单一的输出的数据集上。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/966c0826/mlp.svg" alt="带有隐藏层的多层感知机。它含有一个隐藏层，该层中有5个隐藏单元"></p>
<p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。<br>所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，上图中的多层感知机的层数为2。由上图可见，隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。</p>
<p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$ ，其批量大小为 n ，输入个数为 d 。假设多层感知机只有一个隐藏层，其中隐藏单元个数为 h 。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为 H ，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$ 。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和$\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p>
<p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为<br>$$<br>\begin{split}\begin{aligned}<br>\boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\<br>\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,<br>\end{aligned}\end{split}<br>$$</p>
<p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到<br>$$<br>\boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p>
<p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p>
<h2 id="笔记整理-2"><a href="#笔记整理-2" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>多层感知机中最为重要的自然是“多层”，多层中涉及到的隐藏层的目的是为了将线性的神经网络复杂化，更加有效的逼近满足条件的任何一个函数。<br>因此文中先证明了一个最常见的思路，即两个线性层复合，是不可行的，无论多少层线性层复合最后得到的结果仍然是等价于线性层。这个结果的逻辑来自与线性代数中，H=XW+b 是一个仿射变换，通过W变换和b平移，而O=HW2+b2 则是通过W2变换和b2平移，最终经过矩阵的乘法和加法的运算法则（分配律）得到最终仍然是对X的仿射变换。<br>在线性层复合不行的情况下，最容易想到的思路就是将隐藏层变成非线性的，即通过一个“激励函数”将隐藏层的输出改变。<br>因此这里主要讨论一下，为什么添加激励函数后可以拟合“几乎”任意一个函数。<br>将函数分成三类：逻辑函数，分类函数，连续函数（分类的原则是输入输出形式）</p>
<ol>
<li>通过一个激励函数可以完成简单的或与非门逻辑，因此通过隐藏层中神经元复合就可以完成任何一个逻辑函数拟合。只需要通过神经网络的拟合将真值表完全表示</li>
<li>通过之前使用的线性分类器构成的线性边界进行复合便可以得到任意一个分类函数。</li>
<li>通过积分微元法的思想可以拟合任何一个普通的可积函数</li>
</ol>
<h2 id="Epoch-Batch-Iteration"><a href="#Epoch-Batch-Iteration" class="headerlink" title="Epoch, Batch, Iteration"></a>Epoch, Batch, Iteration</h2><p>深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。</p>
<p>第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。</p>
<p>另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p>
<p>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p>
<p>现在用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。<br>那 batch epoch iteration代表什么呢？</p>
<ul>
<li>batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</li>
<li>iteration：1个iteration等于使用batchsize个样本训练一次；</li>
<li>epoch：1个epoch等于使用训练集中的全部样本训练一次，通俗的讲epoch的值就是整个数据集被轮几次。</li>
</ul>
<p>比如训练集有500个样本，batchsize = 10 ，那么训练完整个样本集：iteration=50，epoch=1.<br>batch: 深度学习每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一组数据加权得到的，这一组数据的数量就是batchsize。<br>batchsize最大是样本总数N，此时就是Full batch learning；最小是1，即每次只训练一个样本，这就是在线学习（Online Learning）。当我们分批学习时，每次使用过全部训练数据完成一次Forword运算以及一次BP运算，成为完成了一次epoch。</p>
<p>mnist 数据集有 60000 张图片作为训练数据，10000 张图片作为测试数据。假设现在选择  Batch Size = 100 对模型进行训练。迭代30000次。</p>
<p>每个 Epoch 要训练的图片数量：60000(训练集上的所有图像)</p>
<ul>
<li>训练集具有的 Batch 个数： 60000/100=600</li>
<li>每个 Epoch 需要完成的 Batch 个数： 600</li>
<li>每个 Epoch 具有的 Iteration 个数： 600（完成一个Batch训练，相当于参数迭代一次）</li>
<li>每个 Epoch 中发生模型权重更新的次数：600</li>
<li>训练 10 个Epoch后，模型权重更新的次数： 600*10=6000</li>
<li>不同Epoch的训练，其实用的是同一个训练集的数据。第1个Epoch和第10个Epoch虽然用的都是训练集的60000图片，但是对模型的权重更新值却是完全不同的。因为不同Epoch的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。</li>
<li>总共完成30000次迭代，相当于完成了 30000/600=50 个Epoch</li>
</ul>
<h1 id="笔记整理-3"><a href="#笔记整理-3" class="headerlink" title="笔记整理"></a>笔记整理</h1><ul>
<li><p>训练集、验证集和测试集的比例应该怎么去进行分配呢？<br>传统上是6：2：2的比例，但是不同的情况下你的选择应当不同。这方面的研究也有很多，如果你想要知道我们在设置比例的时候应当参考那些东西，可以去看Isabelle Guyon的这篇论文：A scaling law for the validation-set training-set size ratio 。他的个人主页（<a href="http://www.clopinet.com/isabelle/）里也展示了他对于这个问题的研究。" target="_blank" rel="noopener">http://www.clopinet.com/isabelle/）里也展示了他对于这个问题的研究。</a></p>
</li>
<li><p>训练集、验证集和测试集的数据是否可以有所重合？<br>有些时候我们的数据太少了，又不想使用数据增强，那么训练集、验证集和测试集的数据是否可以有所重合呢？这方面的研究就更多了，各种交叉方法，感兴趣的话可以去看Filzmoser这一篇文章Repeated double cross validation</p>
</li>
<li><p>torch.mm 和 torch.mul 的区别？<br>torch.mm是矩阵相乘，torch.mul是按元素相乘</p>
</li>
<li><p>torch.manual_seed(1)的作用？<br>设置随机种子，使实验结果可以复现</p>
</li>
<li><p>optimizer.zero_grad()的作用？<br>使梯度置零，防止不同batch得到的梯度累加</p>
</li>
<li><p>为什么选择的激活函数普遍具有梯度消失的特点?<br>开始的时候我一直好奇为什么选择的激活函数普遍具有梯度消失的特点，这样不就让部分神经元失活使最后结果出问题吗？后来看到一篇文章的描述才发现，正是因为模拟人脑的生物神经网络的方法。在2001年有研究表明生物脑的神经元工作具有稀疏性，这样可以节约尽可能多的能量，据研究，只有大约1%-4%的神经元被激活参与，绝大多数情况下，神经元是处于抑制状态的，因此ReLu函数反而是更加优秀的近似生物激活函数。<br>所以第一个问题，抑制现象是必须发生的，这样能更好的拟合特征。<br>那么自然也引申出了第二个问题，为什么sigmoid函数这类函数不行？</p>
</li>
</ul>
<ol>
<li>中间部分梯度值过小（最大只有0.25）因此即使在中间部分也没有办法明显的激活，反而会在多层中失活，表现非常不好。</li>
<li>指数运算在计算中过于复杂，不利于运算，反而ReLu函数用最简单的梯度<br>在第二条解决之后，我们来看看ReLu函数所遇到的问题，</li>
<li>在负向部分完全失活，如果选择的超参数不好等情况，可能会出现过多神经元失活，从而整个网络死亡。</li>
<li>ReLu函数不是zero-centered，即激活函数输出的总是非负值，而gradient也是非负值，在back propagate情况下总会得到与输入x相同的结果，同正或者同负，因此收敛会显著受到影响，一些要减小的参数和要增加的参数会受到捆绑限制。<br>这两个问题的解决方法分别是</li>
<li>如果出现神经元失活的情况，可以选择调整超参数或者换成Leaky ReLu 但是，没有证据证明任何情况下都是Leaky-ReLu好</li>
<li>针对非zero-centered情况，可以选择用minibatch gradient decent 通过batch里面的正负调整，或者使用ELU(Exponential Linear Units)但是同样具有计算量过大的情况，同样没有证据ELU总是优于ReLU。<br>所以绝大多数情况下建议使用ReLu。</li>
</ol>
<h1 id="pytorch内的函数"><a href="#pytorch内的函数" class="headerlink" title="pytorch内的函数"></a>pytorch内的函数</h1><p>torch.ones()/torch.zeros()，与MATLAB的ones/zeros很接近。初始化生成<br>均匀分布<br>torch.rand(<em>sizes, out=None) → Tensor<br>返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。<br>标准正态分布<br>torch.randn(</em>sizes, out=None) → Tensor<br>返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。<br>torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵<br>torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵<br>torch.Tensor是一种包含单一数据类型元素的多维矩阵，定义了7种CPU tensor和8种GPU tensor类型。<br>random.shuffle(a)：用于将一个列表中的元素打乱。shuffle() 是不能直接访问的，需要导入 random 模块，然后通过 random 静态对象调用该方法。<br>backward()是pytorch中提供的函数，配套有require_grad：<br>1.所有的tensor都有.requires_grad属性,可以设置这个属性.x = tensor.ones(2,4,requires_grad=True)<br>2.如果想改变这个属性，就调用tensor.requires_grad_()方法：　　 x.requires_grad_(False)</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Task2 文本预处理；语言模型；循环神经网络基础</title>
    <url>/p/2020/02/13/b2a59db2/</url>
    <content><![CDATA[<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><p>文本预处理步骤：<br>1.读取文本  2.分词  3.构建字典 建立索引<br>Vocab字典构建步骤：<br>1.统计词频，去重筛选掉低频词<br>2.根据需求添加特殊的token<br>3.建立字典，将每个token建立映射到唯一的索引<br>4.建立索引到token的映射</p>
<p>建立词典：<br>词典的主要作用是将每一个词映射到一个唯一的索引号，主要构建了一个idx_to_token列表来存储所有的词，一个token_to_idx来存储所有词的索引。<br>在实现的的流程上是：<br>对语料进行分词，生成一个token列表，里面包含了语料的分词结果<br>对分好的词统计词频，然后根据词频来构建词典（统计好的词频完成了去重的操作，同时也保留了词的频率，方便后续的操作）<br>其中有一些名词的作用是视频里提出来的<br>pad的作用是在采用批量样本训练时，对于长度不同的样本（句子），对于短的样本采用pad进行填充，使得每个样本的长度是一致的<br>bos( begin of sentence)和eos(end of sentence)是用来表示一句话的开始和结尾<br>unk(unknow)的作用是，处理遇到从未出现在预料库的词时都统一认为是unknow ,在代码中还可以将一些频率特别低的词也归为这一类</p>
<ol>
<li>建立字典，设置阈值</li>
<li>去重筛选词，特殊需求token</li>
<li>count_corpus统计词频，得到counter</li>
<li>增删，利用空列表<ul>
<li>pad:二维矩阵长度不一，短句子补token利用pad</li>
<li>bos:开始token</li>
<li>eos：结束token</li>
<li>unk：未登录词当作unk</li>
</ul>
</li>
<li>词到索引号<a id="more"></a>
</li>
</ol>
<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><h1 id="循环神经网络基础"><a href="#循环神经网络基础" class="headerlink" title="循环神经网络基础"></a>循环神经网络基础</h1>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》组队学习课程简介</title>
    <url>/p/2020/02/13/f33d322f/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://www.boyuai.com/elites/course/cZu18YmweLv10OeV" target="_blank" rel="noopener">https://www.boyuai.com/elites/course/cZu18YmweLv10OeV</a></p>
</blockquote>
<p>本课程面向希望更多的通过代码实践去学习深度学习原理的同学和在职人士。</p>
<p>《动手学深度学习》是2019年国内最受欢迎的人工智能学习教材之一，伯禹教育携手上海交通大学团队，以此书的知识架构为基础，沿用了其中的原理讲解文档，并将代码框架由MXNET迁移至PyTorch，还对这些优质的实践代码制作了讲解视频。其中部分PyTorch代码来自GitHub开源仓库：<a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch。" target="_blank" rel="noopener">https://github.com/ShusenTang/Dive-into-DL-PyTorch。</a></p>
<p>通过这门课程的学习，你将可以对深度学习中常见的方法以及相关的应用有一个从原理到实践的全面了解。</p>
<p>本课程主要针对代码进行讲解，理论基础较为薄弱的同学，建议配合《动手学深度学习》书籍或本平台上《机器学习》相关知识点学习。</p>
<p>《动手学深度学习》官方网址：<a href="http://zh.gluon.ai/" target="_blank" rel="noopener">http://zh.gluon.ai/</a> ——面向中文读者的能运行、可讨论的深度学习教科书。</p>
<a id="more"></a>
<hr>
<p>通俗来说，<strong>机器学习</strong>是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。<strong>深度学习</strong>是指机器学习中的一类函数，它们的形式通常为多层神经网络。近年来，仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>在描述深度学习的特点之前，我们先回顾并概括一下机器学习和深度学习的关系。机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出，而本书要重点探讨的深度学习是具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。</p>
<p>深度学习可以逐级表示越来越抽象的概念或模式。以图像为例，它的输入是一堆原始像素值。深度学习模型中，图像可以逐级表示为特定位置和角度的边缘、由边缘组合得出的花纹、由多种花纹进一步汇合得到的特定部位的模式等。最终，模型能够较容易根据更高级的表示完成给定的任务，如识别图像中的物体。值得一提的是，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。</p>
<p>因此，深度学习的一个外在特点是端到端的训练。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。比如说，计算机视觉科学家之前曾一度将特征抽取与机器学习模型的构建分开处理，像是Canny边缘探测 [20] 和SIFT特征提取 [21] 曾占据统治性地位达10年以上，但这也就是人类能找到的最好方法了。当深度学习进入这个领域后，这些特征提取方法就被性能更强的自动优化的逐级过滤器替代了。</p>
<p>相似地，在自然语言处理领域，词袋模型多年来都被认为是不二之选 [22]。词袋模型是将一个句子映射到一个词频向量的模型，但这样的做法完全忽视了单词的排列顺序或者句中的标点符号。不幸的是，我们也没有能力来手工抽取更好的特征。但是自动化的算法反而可以从所有可能的特征中搜寻最好的那个，这也带来了极大的进步。例如，语义相关的词嵌入能够在向量空间中完成如下推理：“柏林 - 德国 + 中国 = 北京”。可以看出，这些都是端到端训练整个系统带来的效果。</p>
<p>除端到端的训练以外，我们也正在经历从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型。当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。这也使我们可以得到更精确的模型，尽管需要牺牲一些可解释性。</p>
<p>相对其它经典的机器学习方法而言，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用，以及勇于尝试没有被证明过的方法。这种在处理统计问题上的新经验主义吸引了大量人才的涌入，使得大量实际问题有了更好的解决方案。尽管大部分情况下需要为深度学习修改甚至重新发明已经存在数十年的工具，但是这绝对是一件非常有意义并令人兴奋的事。</p>
]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Task3 过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶</title>
    <url>/p/2020/02/13/bd42c6ba/</url>
    <content><![CDATA[<h1 id="过拟合、欠拟合及其解决方案"><a href="#过拟合、欠拟合及其解决方案" class="headerlink" title="过拟合、欠拟合及其解决方案"></a>过拟合、欠拟合及其解决方案</h1><h2 id="模型选择、过拟合和欠拟合"><a href="#模型选择、过拟合和欠拟合" class="headerlink" title="模型选择、过拟合和欠拟合"></a>模型选择、过拟合和欠拟合</h2><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>
<p>机器学习模型应关注降低泛化误差。<br><a id="more"></a></p>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><h4 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h4><p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p>
<h4 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h4><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。</p>
<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p>接下来，我们将探究模型训练中经常出现的两类典型问题：</p>
<ul>
<li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li>
<li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。 在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li>
</ul>
<h4 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h4><p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征和对应的标量标签组成的训练数据集，多项式函数拟合的目标是找一个阶多项式函数<br>$$ \hat{y} = b + \sum_{k=1}^K x^k w_k $$<br>来近似$y$ 。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>
<p>给定训练数据集，模型复杂度和误差之间的关系：</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b2a59db2/q5jc27wxoj.png" alt></p>
<h4 id="训练数据集大小"><a href="#训练数据集大小" class="headerlink" title="训练数据集大小"></a>训练数据集大小</h4><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<h3 id="多项式函数拟合实验"><a href="#多项式函数拟合实验" class="headerlink" title="多项式函数拟合实验"></a>多项式函数拟合实验</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__) <span class="comment"># 1.3.0</span></span><br></pre></td></tr></table></figure>
<h4 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>], <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>) </span><br><span class="line">labels = (true_w[<span class="number">0</span>] * poly_features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * poly_features[:, <span class="number">1</span>]</span><br><span class="line">          + true_w[<span class="number">2</span>] * poly_features[:, <span class="number">2</span>] + true_b)</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line"></span><br><span class="line">features[:<span class="number">2</span>], poly_features[:<span class="number">2</span>], labels[:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># (tensor([[-0.8589],</span></span><br><span class="line"><span class="comment">#          [-0.2534]]), tensor([[-0.8589,  0.7377, -0.6335],</span></span><br><span class="line"><span class="comment">#          [-0.2534,  0.0642, -0.0163]]), tensor([-2.0794,  4.4039]))</span></span><br></pre></td></tr></table></figure>
<h4 id="定义、训练和测试模型"><a href="#定义、训练和测试模型" class="headerlink" title="定义、训练和测试模型"></a>定义、训练和测试模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semilogy</span><span class="params">(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             legend=None, figsize=<span class="params">(<span class="number">3.5</span>, <span class="number">2.5</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize)</span></span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals, y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=<span class="string">':'</span>)</span><br><span class="line">        d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">num_epochs, loss = <span class="number">100</span>, torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(train_features, test_features, train_labels, test_labels)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化网络模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置批量大小</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])    </span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)      <span class="comment"># 设置数据集</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>) <span class="comment"># 设置获取数据方式</span></span><br><span class="line">    </span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)                      <span class="comment"># 设置优化函数，使用的是随机梯度下降优化</span></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:                                                 <span class="comment"># 取一个批量的数据</span></span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>))                                     <span class="comment"># 输入到网络中计算输出，并和标签比较求得损失函数</span></span><br><span class="line">            optimizer.zero_grad()                                               <span class="comment"># 梯度清零，防止梯度累加干扰优化</span></span><br><span class="line">            l.backward()                                                        <span class="comment"># 求梯度</span></span><br><span class="line">            optimizer.step()                                                    <span class="comment"># 迭代优化函数，进行参数优化</span></span><br><span class="line">        train_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        test_labels = test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())         <span class="comment"># 将训练损失保存到train_ls中</span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())            <span class="comment"># 将测试损失保存到test_ls中</span></span><br><span class="line">    print(<span class="string">'final epoch: train loss'</span>, train_ls[<span class="number">-1</span>], <span class="string">'test loss'</span>, test_ls[<span class="number">-1</span>])    </span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">             range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'weight:'</span>, net.weight.data,</span><br><span class="line">          <span class="string">'\nbias:'</span>, net.bias.data)</span><br></pre></td></tr></table></figure>
<h4 id="三阶多项式函数拟合（正常）"><a href="#三阶多项式函数拟合（正常）" class="headerlink" title="三阶多项式函数拟合（正常）"></a>三阶多项式函数拟合（正常）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># final epoch: train loss 8887.298828125 test loss 1145.94287109375</span></span><br><span class="line"><span class="comment"># weight: tensor([[-8.5120, 19.0351, 12.8616]]) </span></span><br><span class="line"><span class="comment"># bias: tensor([-5.4607])</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b2a59db2/q5jc27wxoj.png" alt></p>
<h4 id="线性函数拟合（欠拟合）"><a href="#线性函数拟合（欠拟合）" class="headerlink" title="线性函数拟合（欠拟合）"></a>线性函数拟合（欠拟合）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># final epoch: train loss 781.689453125 test loss 329.79852294921875</span></span><br><span class="line"><span class="comment"># weight: tensor([[26.8753]]) </span></span><br><span class="line"><span class="comment"># bias: tensor([6.1426])</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b2a59db2/q5jf5al2tv.png" alt></p>
<h4 id="训练样本不足（过拟合）"><a href="#训练样本不足（过拟合）" class="headerlink" title="训练样本不足（过拟合）"></a>训练样本不足（过拟合）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(poly_features[<span class="number">0</span>:<span class="number">2</span>, :], poly_features[n_train:, :], labels[<span class="number">0</span>:<span class="number">2</span>], labels[n_train:])</span><br><span class="line"><span class="comment"># final epoch: train loss 6.23520565032959 test loss 409.9844665527344</span></span><br><span class="line"><span class="comment"># weight: tensor([[ 0.9729, -0.9612,  0.7259]]) </span></span><br><span class="line"><span class="comment"># bias: tensor([1.6334])</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b2a59db2/q5jf5bd11u.png" alt></p>
<h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>权重衰减等价于$L_2$范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</p>
<h3 id="L2-范数正则化（regularization）"><a href="#L2-范数正则化（regularization）" class="headerlink" title="L2 范数正则化（regularization）"></a>L2 范数正则化（regularization）</h3><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例</p>
<p>$$ \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2 $$</p>
<p>其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p>
<p>$$ \ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2, $$</p>
<p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。 有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为<br>$$<br>\begin{aligned} w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\ w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}<br>$$</p>
<p>可见，范数正则化令权重和先自乘小于1的数，再减去不含惩罚项的梯度。因此，范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。</p>
<h3 id="高维线性回归实验从零开始的实现"><a href="#高维线性回归实验从零开始的实现" class="headerlink" title="高维线性回归实验从零开始的实现"></a>高维线性回归实验从零开始的实现</h3><p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合$p$。设数据样本特征的维度为$x_1, x_2, \ldots, x_p$。对于训练数据集和测试数据集中特征为的任一样本，我们使用如下的线性函数来生成该样本的标签：<br>$$<br>y = 0.05 + \sum_{i = 1}^p 0.01x_i + \epsilon<br>$$<br>其中噪声项$\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__) <span class="comment"># 1.3.0</span></span><br></pre></td></tr></table></figure></p>
<h4 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><p>与前面观察过拟合和欠拟合现象的时候相似，在这里不再解释。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, num_inputs = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span></span><br><span class="line">true_w, true_b = torch.ones(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">features = torch.randn((n_train + n_test, num_inputs))</span><br><span class="line">labels = torch.matmul(features, true_w) + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class="line">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br><span class="line"><span class="comment"># 定义参数初始化函数，初始化模型参数并且附上梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></span><br><span class="line">    w = torch.randn((num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure></p>
<h4 id="定义L2范数惩罚项"><a href="#定义L2范数惩罚项" class="headerlink" title="定义L2范数惩罚项"></a>定义L2范数惩罚项</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).sum() / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line"></span><br><span class="line">dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span><span class="params">(lambd)</span>:</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l = l.sum()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, w.norm().item())</span><br></pre></td></tr></table></figure>
<h4 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(lambd=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># L2 norm of w: 11.6444091796875</span></span><br></pre></td></tr></table></figure>
<h4 id="使用权重衰减"><a href="#使用权重衰减" class="headerlink" title="使用权重衰减"></a>使用权重衰减</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(lambd=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># L2 norm of w: 0.04063604772090912</span></span><br></pre></td></tr></table></figure>
<h4 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br><span class="line"></span><br><span class="line">fit_and_plot_pytorch(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># L2 norm of w: 13.361410140991211</span></span><br><span class="line"></span><br><span class="line">fit_and_plot_pytorch(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># L2 norm of w: 0.051789578050374985</span></span><br></pre></td></tr></table></figure>
<h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为<br>$$<br>h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)<br>$$<br>这里是$\phi$激活函数，$x_1, \ldots, x_4$是输入，隐藏单元的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别$p$为和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p>
<p>$$<br>h_i’ = \frac{\xi_i}{1-p} h_i<br>$$<br>由于$E(\xi_i) = 1-p$，因此<br>$$<br>E(h_i’) = \frac{E(\xi_i)}{1-p}h_i = h_i<br>$$</p>
<p>即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b2a59db2/q5jd69in3m.png" alt></p>
<h3 id="丢弃法从零开始的实现"><a href="#丢弃法从零开始的实现" class="headerlink" title="丢弃法从零开始的实现"></a>丢弃法从零开始的实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__) <span class="comment"># 1.3.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    X = X.float()</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    mask = (torch.rand(X.shape) &lt; keep_prob).float()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">16</span>).view(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line">dropout(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span></span><br><span class="line">dropout(X, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># tensor([[ 0.,  0.,  0.,  6.,  8., 10.,  0., 14.],</span></span><br><span class="line"><span class="comment">#         [ 0.,  0., 20.,  0.,  0.,  0., 28.,  0.]])</span></span><br><span class="line">dropout(X, <span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, is_training=True)</span>:</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval() <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 自定义的模型</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">'is_training'</span> <span class="keyword">in</span> net.__code__.co_varnames): <span class="comment"># 如果有is_training这个参数</span></span><br><span class="line">                <span class="comment"># 将is_training设置成False</span></span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>) == y).float().sum().item() </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item() </span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">100.0</span>, <span class="number">256</span>  <span class="comment"># 这里的学习率设置的很大，原因与之前相同。</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span><br><span class="line">d2l.train_ch3(</span><br><span class="line">    net,</span><br><span class="line">    train_iter,</span><br><span class="line">    test_iter,</span><br><span class="line">    loss,</span><br><span class="line">    num_epochs,</span><br><span class="line">    batch_size,</span><br><span class="line">    params,</span><br><span class="line">    lr)</span><br><span class="line"><span class="comment"># epoch 1, loss 0.0046, train acc 0.549, test acc 0.704</span></span><br><span class="line"><span class="comment"># epoch 2, loss 0.0023, train acc 0.785, test acc 0.737</span></span><br><span class="line"><span class="comment"># epoch 3, loss 0.0019, train acc 0.825, test acc 0.834</span></span><br><span class="line"><span class="comment"># epoch 4, loss 0.0017, train acc 0.842, test acc 0.763</span></span><br><span class="line"><span class="comment"># epoch 5, loss 0.0016, train acc 0.848, test acc 0.813</span></span><br></pre></td></tr></table></figure>
<h3 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># epoch 1, loss 0.0046, train acc 0.553, test acc 0.736</span></span><br><span class="line"><span class="comment"># epoch 2, loss 0.0023, train acc 0.785, test acc 0.803</span></span><br><span class="line"><span class="comment"># epoch 3, loss 0.0019, train acc 0.818, test acc 0.756</span></span><br><span class="line"><span class="comment"># epoch 4, loss 0.0018, train acc 0.835, test acc 0.829</span></span><br><span class="line"><span class="comment"># epoch 5, loss 0.0016, train acc 0.848, test acc 0.851</span></span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>欠拟合现象：模型无法达到一个较低的误差</li>
<li>过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大</li>
</ul>
<h2 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h2><ol>
<li><p>为什么优化器中只对权重参数设置衰减，而不对偏置参数设置衰减呢？<br>对偏置增加正则也是可以的，但是对偏置增加正则不会明显的产生很好的效果。而且偏置并不会像权重一样对数据非常敏感，所以不用担心偏置会学习到数据中的噪声。而且大的偏置也会使得我们的网络更加灵活，所以一般不对偏置做正则化。</p>
</li>
<li><p>L2范数惩罚项通过惩罚绝对值较大的参数的方法来应对过拟合的。这里面的惩罚绝对值较大的参数是什么意思？<br>L2处理    权重会先自乘小于1的系数，再减去不含惩罚项的梯度；系数相等的情况下，绝对值较大的参数损失较大，故而惩罚较大。</p>
</li>
<li><p>按照最开始的说法，训练集，测试集（用来测试训练成果），验证集（用来训练超参数或者选择模型），但K折交叉验证为什么只有k-1个训练和1个验证，没有测试集？“。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。”那么这里的k次训练误差是哪里来的呢？<br>正常的训练过程是分为训练数据和测试数据，但是如果只使用训练数据得到的模型效果不一定是最好的，所以在训练集中划分出验证集，训练集和验证集的划分有很多中，但是最常用的就是k折交叉验证，就是把训练数据分成k份，用其中的 k-1份训练模型，用剩下的一份来验证模型的泛化能力，循环操作，选择最佳的超参数组合，之后再用全部数据训练得到一个模型，再用测试数据来看模型的最终效果。</p>
</li>
<li><p>有一个疑问。dropout（x,0.5）16个数按照0.5的概率丢弃，不应该是丢弃8个数字吗？是老师的口误吗？<br>丢弃率是指某个单元被丢弃（或者说被置为零）的概率。如丢弃率=0.5，表明每个单元都有50%的概率被置零，但各个单元之间是相互独立的。如，16个数（或者说16个单元）按照0.5概率丢弃，会出现16个数都被丢弃（或者16个数都被保留）的情况，概率为0.5^16；当然还有很多种被保留或丢弃的情况组合，最终的统计平均或者说期望是8个。</p>
</li>
<li><p>dropout的时候为什么要进行1-p的拉伸？<br>假设dropout概率为p，那么每个节点被保留的概率是1-p<br>以p=0.3为例，假如某一个全连接层有10个输入节点<br>训练时启用dropout，因此理想情况是7个节点能够向后传递信息。<br>而实际使用时不用dropout，因此10个节点能向后传递信息。<br>因此输出节点接收到的信号强度是不一样的，为了平衡训练和预测时的这种量级差异。<br>可以选择在训练的时候，对每个输入节点进行 除(1-p) 的操作，来进行“拉伸”</p>
</li>
<li><p>权重衰减可以有效处理过拟合的情况，这应该符合奥卡姆剃刀原理的吧？？   权重衰减可避免突出参数的负面干扰，而且经过数据验证也可看到这样确实有更好的拟合效果；但这样的结果是否与我们一直接触的大众数据有关呢，面对一些非常规的数据，比如说地震监测数据等，这种情况需要更显著地区段跳跃，那么这种处理方式还有效吗？？？<br>符合，引入正则项实际上是学习器的一种归纳偏好，即：选用尽可能简单的模型，避免过拟合，因为这样能够有更好的泛化性能。 这是一个增强泛化性能的通用的处理方式，当然如果你的模型如果本身准确率就不高，不会产生过拟合，那这种做法当然效果不好</p>
</li>
</ol>
<h1 id="梯度消失、梯度爆炸以及Kaggle房价预测"><a href="#梯度消失、梯度爆炸以及Kaggle房价预测" class="headerlink" title="梯度消失、梯度爆炸以及Kaggle房价预测"></a>梯度消失、梯度爆炸以及Kaggle房价预测</h1><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p>
<p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p>
<p>假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$X$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。</p>
<h3 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h3><p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p>
<p>回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<p>Image Name</p>
<h3 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h3><p>随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用<code>torch.nn.init.normal_()</code>使模型net的权重参数采用正态分布的随机初始化方式。不过，PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考源代码），因此一般不用我们考虑。</p>
<h3 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h3><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化。 假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布<br>$$<br>U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).<br>$$<br>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>
<h2 id="考虑到环境因素的其他问题"><a href="#考虑到环境因素的其他问题" class="headerlink" title="考虑到环境因素的其他问题"></a>考虑到环境因素的其他问题</h2><h3 id="协变量偏移"><a href="#协变量偏移" class="headerlink" title="协变量偏移"></a>协变量偏移</h3><p>这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布$P（y∣x）$不会改变。虽然这个问题容易理解，但在实践中也容易忽视。</p>
<p>想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。</p>
<p>cat    cat    dog    dog<br>Image Name    Image Name    Image Name    Image Name<br>测试数据：</p>
<p>cat    cat    dog    dog<br>Image Name    Image Name    Image Name    Image Name<br>显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。</p>
<p>统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。</p>
<h3 id="标签偏移"><a href="#标签偏移" class="headerlink" title="标签偏移"></a>标签偏移</h3><p>当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。</p>
<p>病因（要预测的诊断结果）导致 症状（观察到的结果）。</p>
<p>训练数据集，数据很少只包含流感p(y)的样本。</p>
<p>而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。</p>
<h3 id="概念偏移"><a href="#概念偏移" class="headerlink" title="概念偏移"></a>概念偏移</h3><p>另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。</p>
<p>Image Name</p>
<p>美国软饮料名称的概念转变<br>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p>
<h2 id="Kaggle房价预测"><a href="#Kaggle房价预测" class="headerlink" title="Kaggle房价预测"></a>Kaggle房价预测</h2><p>作为深度学习基础篇章的总结，我们将对本章内容学以致用。下面，让我们动手实战一个Kaggle比赛：房价预测。本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"><span class="comment"># 1.3.0</span></span><br></pre></td></tr></table></figure></p>
<h3 id="获取和读取数据集"><a href="#获取和读取数据集" class="headerlink" title="获取和读取数据集"></a>获取和读取数据集</h3><p>比赛数据分为训练数据集和测试数据集。两个数据集都包括每栋房子的特征，如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值“na”。只有训练数据集包括了每栋房子的价格，也就是标签。我们可以访问比赛网页，点击“Data”标签，并下载这些数据集。</p>
<p>我们将通过pandas库读入并处理数据。在导入本节需要的包前请确保已安装pandas库。 假设解压后的数据位于/home/kesci/input/houseprices2807/目录，它包括两个csv文件。下面使用pandas读取这两个文件。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/test.csv"</span>)</span><br><span class="line">train_data = pd.read_csv(<span class="string">"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/train.csv"</span>)</span><br></pre></td></tr></table></figure></p>
<p>训练数据集包括1460个样本、80个特征和1个标签。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.shape <span class="comment"># (1460, 81)</span></span><br></pre></td></tr></table></figure></p>
<p>测试数据集包括1459个样本和80个特征。我们需要将测试数据集中每个样本的标签预测出来。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data.shape <span class="comment"># (1459, 80)</span></span><br></pre></td></tr></table></figure></p>
<p>让我们来查看前4个样本的前4个特征、后2个特征和标签（SalePrice）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.iloc[<span class="number">0</span>:<span class="number">4</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-3</span>, <span class="number">-2</span>, <span class="number">-1</span>]]</span><br><span class="line"><span class="comment"># Id	MSSubClass	MSZoning	LotFrontage	SaleType	SaleCondition	SalePrice</span></span><br><span class="line"><span class="comment"># 0	1	60	RL	65.0	WD	Normal	208500</span></span><br><span class="line"><span class="comment"># 1	2	20	RL	80.0	WD	Normal	181500</span></span><br><span class="line"><span class="comment"># 2	3	60	RL	68.0	WD	Normal	223500</span></span><br><span class="line"><span class="comment"># 3	4	70	RL	60.0	WD	Abnorml	140000</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到第一个特征是Id，它能帮助模型记住每个训练样本，但难以推广到测试样本，所以我们不使用它来训练。我们将所有的训练数据和测试数据的79个特征按样本连结。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:<span class="number">-1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br></pre></td></tr></table></figure></p>
<h3 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h3><p>我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为，标准差为。那么，我们可以将该特征的每个值先减去再除以得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">'object'</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"><span class="comment"># 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>接下来将离散数值转成指示特征。举个例子，假设特征MSZoning里面有两个不同的离散值RL和RM，那么这一步转换将去掉MSZoning特征，并新加两个特征MSZoning_RL和MSZoning_RM，其值为0或1。如果一个样本原来在MSZoning里的值为RL，那么有MSZoning_RL=1且MSZoning_RM=0。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br><span class="line">all_features.shape <span class="comment"># (2919, 331)</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到这一步转换将特征数从79增加到了331。</p>
<p>最后，通过values属性得到NumPy格式的数据，并转成Tensor方便后面的训练。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)</span><br><span class="line">train_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">(feature_num)</span>:</span></span><br><span class="line">    net = nn.Linear(feature_num, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<p>下面定义比赛用来评价模型的对数均方根误差。给定预测值和对应的真实标签，它的定义为</p>
<p>对数均方根误差的实现如下。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_rmse</span><span class="params">(net, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 将小于1的值设成1，使得取对数时数值更稳定</span></span><br><span class="line">        clipped_preds = torch.max(net(features), torch.tensor(<span class="number">1.0</span>))</span><br><span class="line">        rmse = torch.sqrt(<span class="number">2</span> * loss(clipped_preds.log(), labels.log()).mean())</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br></pre></td></tr></table></figure></p>
<p>下面的训练函数跟本章中前几节的不同在于使用了Adam优化算法。相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。我们将在之后的“优化算法”一章里详细介绍它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, train_features, train_labels, test_features, test_labels,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs, learning_rate, weight_decay, batch_size)</span>:</span></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 这里使用了Adam优化算法</span></span><br><span class="line">    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay) </span><br><span class="line">    net = net.float()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X.float()), y.float())</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br></pre></td></tr></table></figure>
<h3 id="K折交叉验证-1"><a href="#K折交叉验证-1" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>我们在模型选择、欠拟合和过拟合中介绍了折交叉验证。它将被用来选择模型设计并调节超参数。下面实现了一个函数，它返回第i折交叉验证时所需要的训练和验证数据。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_k_fold_data</span><span class="params">(k, i, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># 返回第i折交叉验证时所需要的训练和验证数据</span></span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">        idx = slice(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat((X_train, X_part), dim=<span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat((y_train, y_part), dim=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure></p>
<p>在折交叉验证中我们训练次并返回训练和验证的平均误差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_fold</span><span class="params">(k, X_train, y_train, num_epochs,</span></span></span><br><span class="line"><span class="function"><span class="params">           learning_rate, weight_decay, batch_size)</span>:</span></span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net(X_train.shape[<span class="number">1</span>])</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[<span class="number">-1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'rmse'</span>,</span><br><span class="line">                         range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), valid_ls,</span><br><span class="line">                         [<span class="string">'train'</span>, <span class="string">'valid'</span>])</span><br><span class="line">        print(<span class="string">'fold %d, train rmse %f, valid rmse %f'</span> % (i, train_ls[<span class="number">-1</span>], valid_ls[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure>
<h3 id="模型选择-1"><a href="#模型选择-1" class="headerlink" title="模型选择"></a>模型选择</h3><p>我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。 有时候你会发现一组参数的训练误差可以达到很低，但是在折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察折交叉验证上的误差是否也相应降低。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">print(<span class="string">'%d-fold validation: avg train rmse %f, avg valid rmse %f'</span> % (k, train_l, valid_l))</span><br><span class="line"><span class="comment"># fold 0, train rmse 0.241365, valid rmse 0.223083</span></span><br><span class="line"><span class="comment"># fold 1, train rmse 0.229118, valid rmse 0.267488</span></span><br><span class="line"><span class="comment"># fold 2, train rmse 0.232072, valid rmse 0.237995</span></span><br><span class="line"><span class="comment"># fold 3, train rmse 0.238050, valid rmse 0.218671</span></span><br><span class="line"><span class="comment"># fold 4, train rmse 0.231004, valid rmse 0.259185</span></span><br><span class="line"><span class="comment"># 5-fold validation: avg train rmse 0.234322, avg valid rmse 0.241284</span></span><br></pre></td></tr></table></figure></p>
<h3 id="预测并在Kaggle中提交结果"><a href="#预测并在Kaggle中提交结果" class="headerlink" title="预测并在Kaggle中提交结果"></a>预测并在Kaggle中提交结果</h3><p>下面定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_pred</span><span class="params">(train_features, test_features, train_labels, test_data,</span></span></span><br><span class="line"><span class="function"><span class="params">                   num_epochs, lr, weight_decay, batch_size)</span>:</span></span><br><span class="line">    net = get_net(train_features.shape[<span class="number">1</span>])</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'rmse'</span>)</span><br><span class="line">    print(<span class="string">'train rmse %f'</span> % train_ls[<span class="number">-1</span>])</span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    test_data[<span class="string">'SalePrice'</span>] = pd.Series(preds.reshape(<span class="number">1</span>, <span class="number">-1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">'Id'</span>], test_data[<span class="string">'SalePrice'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">'./submission.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># sample_submission_data = pd.read_csv("../input/house-prices-advanced-regression-techniques/sample_submission.csv")</span></span><br></pre></td></tr></table></figure>
<p>设计好模型并调好超参数之后，下一步就是对测试数据集上的房屋样本做价格预测。如果我们得到与交叉验证时差不多的训练误差，那么这个结果很可能是理想的，可以在Kaggle上提交结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure>
<h2 id="笔记整理-1"><a href="#笔记整理-1" class="headerlink" title="笔记整理"></a>笔记整理</h2><ol>
<li><p>标签偏移指的是出现训练中不存在的标签，而圣诞礼物属于训练中存在的标签</p>
</li>
<li><p>请问反向传播是怎么回事？<br>将权重沿负梯度方向进行小步长的位移，可以逐渐的使loss函数下降，从而让模型具有好的效果。<br>反向传播其实就是从终点的loss往回回溯出每一个变量的梯度，从而好进行梯度下降的优化。<br>我看到梯度下降知识点的视频已放出，可以去学习一下，应该就对反向传播的意义有比较好的理解了。</p>
</li>
<li><p>还是不懂标签偏移量和协变量偏移是什么？<br>标签偏移是在P（x∣y），在y的条件下x的概率，可以假设为y不变的情况下x的概率，而现实是y导致x发生了变化，而y是变化的所以就发生了标签偏移，因为y是标签。<br>而协变量偏移P（y∣x），同理可以假设为在x不变的情况下y的概率，而现实是x发生了变化导致y发生了变化，所以就发生了协变量偏移，x为变量。</p>
</li>
<li><p>在反向传播中，每个隐藏单元的参数梯度值相等，这是为什么呢？<br>算出来相等那就确实相等，不是一定会相等</p>
</li>
</ol>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>解决hbase中region in transition的问题</title>
    <url>/p/2019/06/18/d99909a6/</url>
    <content><![CDATA[<p>由于RIT问题导致hbase的regionserver起不来。</p>
<a id="more"></a>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/d99909a6/e5357743b2cd6a51f8ea3461e0e14b3f.png" alt="region in transition情况"><br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/d99909a6/e3cbbf5c9a240ca109110bd3f8c9b8a6.png" alt="ambari中的hbase"></p>
<h1 id="删除hdfs中的表文件"><a href="#删除hdfs中的表文件" class="headerlink" title="删除hdfs中的表文件"></a>删除hdfs中的表文件</h1><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/d99909a6/318413a12eeeea61e4d8ed5ba4f81514.png" alt="删除hdfs中的表文件"><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su hdfs</span><br><span class="line">hdfs dfs -rm -r -f /apps/hbase/data/data/default/gdelt</span><br><span class="line">hdfs dfs -rm -r -f /apps/hbase/data/data/default/gdelt_gdelt_2dquickstart_z3_geom_dtg_v6</span><br></pre></td></tr></table></figure></p>
<h1 id="重启hbase后，修复hbase"><a href="#重启hbase后，修复hbase" class="headerlink" title="重启hbase后，修复hbase"></a>重启hbase后，修复hbase</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su hbase</span><br><span class="line">hbase hbck -repair</span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/d99909a6/441af90ee32ae54b0670a253b0d9780f.png" alt="修复hbase"></p>
<h1 id="链接zk，删除僵尸表"><a href="#链接zk，删除僵尸表" class="headerlink" title="链接zk，删除僵尸表"></a>链接zk，删除僵尸表</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/hdp/2.6.5.0-292/zookeeper/bin/</span><br><span class="line">./zkCli.sh</span><br><span class="line">ls /hbase/table</span><br><span class="line">rmr /hbase/table/gdelt</span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/d99909a6/5972c536549f882184b1b91efcb47ee9.png" alt="删除僵尸表"></p>
<h1 id="在zk中删除rit相关信息"><a href="#在zk中删除rit相关信息" class="headerlink" title="在zk中删除rit相关信息"></a>在zk中删除rit相关信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls /hbase/region-in-transition</span><br><span class="line">rmr /hbase/region-in-transition/6f6f4f8204ccaa0b71df08591de155ba</span><br></pre></td></tr></table></figure>
<h1 id="重启hbase"><a href="#重启hbase" class="headerlink" title="重启hbase"></a>重启hbase</h1>]]></content>
      <categories>
        <category>FAQ</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hdp</tag>
        <tag>ambari</tag>
        <tag>大数据</tag>
        <tag>rit</tag>
        <tag>hbase</tag>
        <tag>region in transition</tag>
      </tags>
  </entry>
  <entry>
    <title>Task4 机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer</title>
    <url>/p/2020/02/13/92342664/</url>
    <content><![CDATA[<h1 id="机器翻译及相关技术"><a href="#机器翻译及相关技术" class="headerlink" title="机器翻译及相关技术"></a>机器翻译及相关技术</h1><h2 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>数据预处理</p>
<ol>
<li>读取数据，处理数据中的编码问题，并将无效的字符串删除</li>
<li>分词，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)</li>
<li>建立词典，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西<ol>
<li>去重后词典，及其中单词对应的索引列表</li>
<li>还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典。</li>
<li>原始语料所有词对应的词典索引的列表</li>
</ol>
</li>
<li>对数据进行padding操作。</li>
<li>制作数据生成器，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。</li>
</ol>
<a id="more"></a>
<p>Seq2Seq模型的构建</p>
<ol>
<li>Seq2Seq模型由很多钟，但是整体框架都是基于先编码后解码的框架。也就是先对输入序列使用循环神经网络对他进行编码，编码成一个向量之后，再将编码得到的向量作为一个新的解码循环神经网络的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。</li>
<li>词嵌入，一般情况下输入到编码网络中的数据不是一个onehot向量而是经过了编码之后的向量，比如由word2vec技术，让编码后的向量由更加丰富的含义。</li>
<li>在进行编码和解码的过程中数据都是以时间步展开，也就是(Seq_len,)这种形式的数据进行处理的</li>
<li>对于编码与解码的循环神经网络，可以通过控制隐藏层的层数及每一层隐藏层神经元的数量来控制模型的复杂度</li>
<li>编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态,编码RNN输出的隐含状态认为是其对应的编码向量</li>
<li>解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是由编码器的输出的隐藏状态初始化的。</li>
</ol>
<p>损失函数</p>
<ol>
<li>解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择分数最大的那个词作为最终的输出。</li>
<li>在计算损失函数之前，要把padding去掉，因为padding的部分不参与计算</li>
</ol>
<p>测试</p>
<ol>
<li>解码器在测试的时候需要将模型的输出作为下一个时间步的输入</li>
<li>Beam Search搜索算法。<ol>
<li>假设预测的时候词典的大小为3，内容为a,b,c. beam size为2，解码的时候过程如下</li>
<li>生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。</li>
<li>生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb </li>
<li>后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。</li>
</ol>
</li>
</ol>
<p>想问问大家 做word_embedding 为什么会用到训练过程呀？一感觉就是一个词 转成 索引 再根据字典大小 转成词向量 ，字典大小应该是固定的，那不就可以直接得出，为何训练呢~ 类比前面学到的 把词转换成索引再转成one hot向量 （字符级时）</p>
<p>训练了才能获取嵌入词向量啊<br>通用任务是可以不训练，用别人训好的，但是要想取得更好的效果，最好还是在实际数据分布下训一个word-embedding“过拟合”一下。<br>举个形象点的例子，某一通用点的语料库上训的embedding可能在检测辱骂场景就很不好用，有些很关键的词分布是差异很大的，甚至没出现过。<br>假设使用one hot编码的话，那些词的表示是没有语义的，比如representation(男人) - representation(女人) = representation(男孩) - representation(女孩) 是得不到的。word embedding中最常见的word2vev使用语料库训练的时候，实际在进行两个事情，一个是根据前一个词猜后一个词，还有一个是某个词的上下文，也就是前后词猜中间这个。这样的方式得到表示会包含语义，但是也会有其他问题，所以word embedding也有很多不同的方法。</p>
<p>1.为何代码处source和target分别不同方式预处理数据（增加start/end），而不统一用True<br>    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)<br>    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)</p>
<p>2.如何设定max_len？<br>src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8) </p>
<p>3.在encoder和decoder会分别设置embedding层，如果导入已训练好的word embedding，希望参数不参加训练，该如何设置？</p>
<p>1.build_array中，你所说的True的parameter是is_source…所以在生成target数据的时候要设定为False。可以去看build_array函数的定义。<br>2.max_len的选取要看source和target数据集的句长统计. 可以选择所有句长平均值。<br>3.<br>self.embed = nn.Embedding(vocab_size, embedding_size)<br>self.embed.weight.data.copy_(torch.from_numpy(pretrained_embeddings))<br>self.embed.weight.requires_grad = False<br>这里pretrained_embeddings就是预训练好的词向量array</p>
<p>在进行word_embedding的时候应该要把参数padding_index设置为pad的token吧..这样pad对应的词向量就不会参与训练了？<br>是的，此时padding_idx所在的行初始化为0 ，并且不会反向传播</p>
<p>这里都默认大家知道lstm是什么了，然而我忘了。。。decoder的初始输入包含encoder最后一个hidden state以及一个memory cell<br>这个memory cell是什么？<br>memory cell就是LSTM的基本单元</p>
<h1 id="注意力机制与Seq2seq模型"><a href="#注意力机制与Seq2seq模型" class="headerlink" title="注意力机制与Seq2seq模型"></a>注意力机制与Seq2seq模型</h1><h2 id="笔记整理-1"><a href="#笔记整理-1" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>那个masked_softmax为什么要repeat？<br>X shape: (batch_size, seq_length, input_dim)<br>valid_length指一个sequence的有效长度，所以shape应该为(batch_size, seq_length)<br>当valid_length是一个1-D tensor的时候，表示同一个batch的每个sequence有效长度相同，因此要在同一batch内进行repeat</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="笔记整理-2"><a href="#笔记整理-2" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>我在transpose_qkv函数里输出原始的X的形状是[2,4,9]和注释里写的[batch_size,sql_len,hidden_size<em>num_heads]不一样，，这是为什么？<br>MultiHeadAttention层初始化的hidden_size其实是hidden_size </em> num_heads<br>例子里<code>num_heads=3, hidden_size=3</code>，总的<code>all_hidden_size=9</code></p>
]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Task5 卷积神经网络基础；leNet；卷积神经网络进阶</title>
    <url>/p/2020/02/13/aa23684a/</url>
    <content><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><h2 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>通过观察互相关运算可知，输入数据（这里以二维数组为例）的边缘部分相较于中间部分来说，使用的次数较少，对于一些存储重要信息的边缘数据来说，这无疑是重大的损失，这里可以通过填充来解决这一问题吗？？也就是在输入数据的边缘填充0，使得原来的边缘数据中心化？？？<br>  padding解决不了这个问题，因为padding的内容其实是不包含信息量的0。<br>  这个问题感觉是个不错的思考~但是又感觉很奇怪，平时是遇不到这个问题的。<br>  比如给你一张狗的图片，shape 224<em>224，你换成 222</em>222，你不可能不认识它了。<br>  比如即使你使用7<em>7的大卷积核，损失的也仅仅是边缘的3个像素而已。<br>  更何况现在都是3</em>3的卷积核，那1个像素是不会包含什么重要信息的。</p>
<p>  padding的用途之一就是弥补特征提取时可能造成的边缘信息缺失。我们人在观察图片内容的过程，其实也是就是一个特征提取的过程。人在拍摄照片时，习惯将重要的事物放在图片中央，所以，在用卷积操作提取图片特征时，中间运算次数多，两边少，这是能满足大多数图像重要信息提取的。但是，当你的图片中边缘有重要信息，而卷积核又取得相对较大时，为了更好的保留边缘信息，我们可以考虑使用SAME padding，保持特征图不缩小(具体计算公式见我给出的PPT)。楼上这位朋友解释说“padding解决不了这个问题，因为padding的内容其实是不包含信息量的0。”，这不太对。因为他忽略了虽然padding的0不含信息，但是原始图片(或是FM)的边缘是包含信息的啊，而且你认为边缘信息对你当前要处理的任务来说是重要的，所以，你在要SAME padding处理，以尽可能多提取边缘信息啊。</p>
<a id="more"></a>
<p>  老师提出问题<br>  conv2d.weight.data -= lr <em> conv2d.weight.grad<br>  conv2d.bias.data -= lr </em> conv2d.bias.grad<br>  上面两行代码，如果删掉.data会发生什么？如果遇到一些问题，如何解决？<br>  经测试：如果删掉.data，也就是代码变为：<br>  conv2d.weight -= lr <em> conv2d.weight.grad<br>  conv2d.bias -= lr </em> conv2d.bias.grad<br>  会报错：RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.<br>  这里给出一种解决方法：<br>  with torch.no_grad( ):<br>      conv2d.weight -= lr <em> conv2d.weight.grad<br>      conv2d.bias -= lr </em> conv2d.bias.grad</p>
<p>  为什么两个连续的3×3卷积核的感受野与一个5×5卷积核的感受野相同？<br>    比如一个7X7的图像，经过一次5<em>5的卷积核后形成一个3</em>3的特征图；<br>    同样一个7X7的图像，经过两次次3<em>3的卷积核后才形成形成一个3</em>3的特征图。</p>
<p>  例子中的Kernel都是固定值，但是再torch的nn.Conv2d()中只给出了kernel_size的值，这是为什么，是因为kernel value也是一个学习的变量吗？<br>    kernel_size 是需要手动调整的，但是在训练过程中并不需要学习。<br>    因此kernel_size准确来说是一个超参数，目前越来越趋向于使用小的卷积核，如1<em>1，3</em>3。</p>
<pre><code>kernel_size（即卷积核大小）这是需要人为设定的参数，该参数是不需要学习的，当然大小不同，卷积结果也是不同的。经过大量实验表明，大多选用1*1、3*3、5*5等尺寸较小且长宽为奇数的卷积核。
对于卷积核内的数值（可以认为是对应卷积作用区域下像素值的权重大小），这个数值是需要学习。当给定kernel_size时，函数即会初始化该卷积核内的数值。
</code></pre><p>  卷积层和池化的区别？<br>    最大的区别是，池化层没有需要学习的特征<br>    卷积层用来学习识别某一种特征<br>    池化层的主要作用是降维<br>    卷积层当设置步长&gt;1时也可以用来完成降维的工作</p>
<h1 id="leNet"><a href="#leNet" class="headerlink" title="leNet"></a>leNet</h1><h2 id="笔记整理-1"><a href="#笔记整理-1" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>在训练测试过程中，每次都对optimizer进行清零处理，为啥结果的acc还是呈现递增的趋势<br>  acc 是 accuracy，准确率，当然是越高越好啦。<br>  optimizer = optim.SGD(net.parameters(), lr=lr)，对优化器清零是清除的梯度，防止梯度累加，为下一波梯度计算留空间。而学习的参数并不清零，所以参数越来越优。<br>  和前面梯度优化的例子一样，清零不影响网络的记忆性（不能说是连续的，目前的还很难达到连续性智能）</p>
<p>1、在构建网络时，卷积核应该怎么设置呢？这块可以讲解下吗？还是说可以通过梯度传播调整<br>2、卷积的时候图像个数增加是因为引入多个卷积核吗？<br>3、构建网络时时是否每次卷积完毕必须引入池化层？还是说这个看网络设计者的调节</p>
<ol>
<li>很多同学都在问这块的问题，如何设计神经网络，其实不是这么课程重点需要关注的问题。<br> 实际使用时往往不需要你来设计，基本上都是用经典结构，最多是进行一些改造。<br> 那么到底如何设计呢？这块一两句话肯定说不清，可以按照发展顺序阅读经典论文，去寻求一些模型设计经验的线索。<br> LeNet -&gt; AlexNet -&gt; VGG -&gt; GoogleNet -&gt; ResNet -&gt; DenseNet 等等，后面就不说了，还有数不尽的论文等待着去学习。。。</li>
<li>这个问题提问我没太看明白</li>
<li>不是每个卷积层后面都要池化，经常是多个卷积层后面接一个池化层。<br> 池化是为了降维，我们最终希望提取到的是一些抽象的有代表性的特征，而不是很多很多感受野非常小的细节特征，例如纹理，颜色等。<br> 而且有的网络也会不使用池化层，而是使用步长&gt;1的卷积层来替代pool层完成降维的工作。</li>
</ol>
<p>池化层可以改变通道数？<br>池化层不是输入与输出的通道数相同，池化层的内核和padding可以改变特征的宽和高，但是通道数量不是不变的吗？<br>  标准的池化层不改变通道数，手动实现当然更改平均池化或者最大池化运算的通道数层，但效果差异不大，因为可以通过卷积层改变通道数。</p>
<p>这里选择的池化，放大放小的比例，通道数似乎没有明确的说明，原理是什么？？<br>如果是经验数据的话，这个可不可以成为学习的对象，通过调整卷积核和padding的大小能否选择出更好的网络？？<br>  当然可以通过调整看看会不会效果更好，这些都是人为设定的。</p>
<p>请问从pooling的6通道变成卷积层的16通道的详细变换过程是怎样的，依据是什么？<br>  通道的数量取决于卷积核的数量，只不过LeNet的作者选择了16个</p>
<p>在卷积、池化的过程中，会出现通到数增加的情况，这应该是为了尽可能地提取特征信息；在leNet网络里面，通道数的变化比例是最优的吗？？？  里面是否有理论支持，或者是一个经验数据呢？？？？？<br>  leNet网络里面的通道数变化比例在原始论文没有明确的理论支持，应该是一个经验数据</p>
<h1 id="卷积神经网络进阶"><a href="#卷积神经网络进阶" class="headerlink" title="卷积神经网络进阶"></a>卷积神经网络进阶</h1><h2 id="笔记整理-2"><a href="#笔记整理-2" class="headerlink" title="笔记整理"></a>笔记整理</h2><p>这里的ALNet引入了一个概念，就是按某一个概率p将某处权重置0，那么按这个网络前进后，还可以进行反向传播吗？？？   还有就是我们这里按一定规律取消部分神经元的作用，也就相当于采用另一种模型方式，那么我们这里的AlexNet就相当于一个模型集了，这理论上应该可以从多方面提取特征信息；然而就像人的大脑分成很多部分一样，如果我们不是单纯地使用一个简单的概率来规划神经元的使用，而是将不同神经元分成不同的区域或者进行更有效的规划，会不会让AI更连续呢？？？ 就像张钹院士在18年的一次报告中提到的，对于不满足“5个条件”的数据，也能很好预测<br>  可以，不更新失活单元的权重就行了。</p>
<p>多换一个数据集的话（主要是size的变化），NiN block是不是仅需要改变第一个Convolution的大小，而后续的1x1的Convolution保持不变？<br>  是的，你观察到了卷积和全连接的一个重要的差别，卷积其实不关心特征图的H和W。<br>  如果你从10分类shape224<em>224</em>3的彩色数据集，切换到一个2分类shape160<em>160</em>1的灰度图数据集。<br>  只需要替换第一个卷积的input_channel 和最后一个卷积或者全连接层的output_channel。<br>  而如果你是从彩色数据集换成了彩色数据集，第一个卷积甚至都不需要发生变化。</p>
<p>卷积其实不关心特征图的H和W“的原因是什么呢？<br>  卷积的工作流程是，以一定的卷积核大小，一定的步长，在特征图的不同位置，用同一个卷积核来进行互相关运算。<br>  这就好像是，一个卷积核是用来提取某一种局部特征的，它在图像中的不同位置来寻找是否有符合它所关心的特征的局部区域。<br>  这种工作机制导致了图像的尺寸（宽和高）并不影响卷积运算，只有通道数的变化才会影响。</p>
<p>inception网络第一个7<em>7卷积层后面为什么要加relu激活？<br>或者说b2中1</em>1卷积和3*3卷积为什么没加relu？还有inception定义中的p1 = F.relu(self.p1_1(x))，F是什么？<br>  F是torch.nn.functional，前面有<code>import torch.nn.functional as F</code><br>  激活的话</p>
<ul>
<li>1x1 conv除了改变channel数，还有增加非线性的作用，一般都会跟着一个激活层</li>
<li>其他感觉比较玄学设计，CV里可能跟ReLU可以做到单侧抑制增加稀疏有关吧</li>
</ul>
]]></content>
      <categories>
        <category>Datawhale</category>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>dl</tag>
        <tag>深度学习</tag>
        <tag>deep learning</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Day1 数据初识</title>
    <url>/p/2019/04/07/2612ecd/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>建立模型通过长文本数据正文(article)，预测文本对应的类别(class)</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>数据包含2个csv文件：</p>
<ol>
<li><p>train_set.csv 此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列：<br>第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。<br>注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！</p>
</li>
<li><p>test_set.csv：此数据用于测试。数据格式同train_set.csv，但不包含class。<br>注：test_set与train_test中文章id的编号是独立的。</p>
</li>
</ol>
<h2 id="评分标准"><a href="#评分标准" class="headerlink" title="评分标准"></a>评分标准</h2><p>评分算法：binary-classification<br>采用各个品类F1指标的算术平均值，它是Precision 和 Recall 的调和平均数。</p>
<p>其中，Pi是表示第i个种类对应的Precision， Ri是表示第i个种类对应Recall。</p>
<a id="more"></a>
<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data_train = pd.read_csv(<span class="string">'../dc-nlp/data/train_set.csv'</span>)</span><br><span class="line">data_test = pd.read_csv(<span class="string">'../dc-nlp/data/test_set.csv'</span>)</span><br><span class="line"></span><br><span class="line">data_train.drop(columns=<span class="string">'article'</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data_test.drop(columns=<span class="string">'article'</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">f_all = pd.concat(objs=[data_train, data_test], axis=<span class="number">0</span>, sort=<span class="literal">True</span>)</span><br><span class="line">y_train = (data_train[<span class="string">'class'</span>] - <span class="number">1</span>).values</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Datawhale</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>达观杯</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Day1 机器学习概述</title>
    <url>/p/2020/01/07/7dbd4cd6/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>机器学习介绍：机器学习是什么，怎么来的，理论基础是什么，为了解决什么问题。</li>
<li>机器学习分类：<ul>
<li>按学习方式分：有监督、无监督、半监督 </li>
<li>按任务类型分：回归、分类、聚类、降维 生成模型与判别模型</li>
</ul>
</li>
<li>机器学习方法三要素：<ul>
<li><strong>模型</strong> </li>
<li><strong>策略</strong>：损失函数 </li>
<li><strong>算法</strong>：梯度下降法、牛顿法、拟牛顿法</li>
<li>模型评估指标：R2、RMSE、accuracy、precision、recall、F1、ROC、AUC、Confusion Matrix </li>
<li>复杂度度量：偏差与方差、过拟合与欠拟合、结构风险与经验风险、泛化能力、正则化 </li>
<li>模型选择：正则化、交叉验证 </li>
<li>采样：样本不均衡 </li>
<li>特征处理：归一化、标准化、离散化、one-hot编码 </li>
<li>模型调优：网格搜索寻优、随机搜索寻优</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="机器学习综述"><a href="#机器学习综述" class="headerlink" title="机器学习综述"></a>机器学习综述</h2><blockquote>
<p>2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜。深度学习开始进行大众的视野中。深度学习其实是机器学习的一个分支,我们今天来看看机器学习是什么。机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径。 </p>
</blockquote>
<h3 id="机器学习的发展"><a href="#机器学习的发展" class="headerlink" title="机器学习的发展"></a>机器学习的发展</h3><p>其中，机器学习（Machine Learning）的各个阶段发展历程列表如下。</p>
<table><br><br><tr><th>时间段</th><th>机器学习理论</th><th>代表性成果</th></tr><br><br><tr><td rowspan="2">二十世纪五十年代初</td><td>人工智能研究处于推理期</td><td>A. Newell和H. Simon的“逻辑理论家”（Logic Theorist）程序证明了数学原理，以及此后的“通用问题求解”（General Problem Solving）程序。</td></tr><br><tr><td>已出现机器学习的相关研究</td><td>1952年，阿瑟·萨缪尔（Arthur Samuel）在IBM公司研制了一个西洋跳棋程序，这是人工智能下棋问题的由来。</td></tr><br><tr><td>二十世纪五十年代中后期</td><td>开始出现基于神经网络的“连接主义”（Connectionism）学习</td><td>F. Rosenblatt提出了感知机（Perceptron），但该感知机只能处理线性分类问题，处理不了“异或”逻辑。还有B. Widrow提出的Adaline。</td></tr><br><tr><td rowspan="4">二十世纪六七十年代</td><td>基于逻辑表示的“符号主义”（Symbolism）学习技术蓬勃发展</td><td>P. Winston的结构学习系统，R. S. Michalski的基于逻辑的归纳学习系统，以及E. B. Hunt的概念学习系统。</td></tr><br><tr><td>以决策理论为基础的学习技术</td><td>&nbsp;</td></tr><br><tr><td>强化学习技术</td><td>N. J. Nilson的“学习机器”。</td></tr><br><tr><td>统计学习理论的一些奠基性成果</td><td>支持向量，VC维，结构风险最小化原则。</td></tr><br><tr><td rowspan="4">二十世纪八十年代至九十年代中期</td><td>机械学习（死记硬背式学习）<br>示教学习（从指令中学习）<br>类比学习（通过观察和发现学习）<br>归纳学习（从样例中学习）</td><td>学习方式分类</td></tr><br><tr><td>从样例中学习的主流技术之一：（1）符号主义学习<br>（2）基于逻辑的学习</td><td>（1）决策树（decision tree）。<br>（2）归纳逻辑程序设计（Inductive Logic Programming, ILP）具有很强的知识表示能力，可以较容易地表达出复杂的数据关系，但会导致学习过程面临的假设空间太大，复杂度极高，因此，问题规模稍大就难以有效地进行学习。</td></tr><br><tr><td>从样例中学习的主流技术之二：基于神经网络的连接主义学习</td><td>1983年，J. J. Hopfield利用神经网络求解“流动推销员问题”这个NP难题。1986年，D. E. Rumelhart等人重新发明了BP算法，BP算法一直是被应用得最广泛的机器学习算法之一。</td></tr><br><tr><td>二十世纪八十年代是机器学习成为一个独立的学科领域，各种机器学习技术百花初绽的时期</td><td>连接主义学习的最大局限是“试错性”，学习过程涉及大量参数，而参数的设置缺乏理论指导，主要靠手工“调参”，参数调节失之毫厘，学习结果可能谬以千里。</td></tr><br><tr><td>二十世纪九十年代中期</td><td>统计学习（Statistical Learning）</td><td>支持向量机（Support Vector Machine，SVM），核方法（Kernel Methods）。</td></tr><br><tr><td>二十一世纪初至今</td><td>深度学习（Deep Learning）</td><td>深度学习兴起的原因有二：数据量大，机器计算能力强。</td></tr><br><br></table>

<h3 id="机器学习分类-6"><a href="#机器学习分类-6" class="headerlink" title="机器学习分类 ^6"></a>机器学习分类 <a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">^6</a></h3><ol>
<li><p>监督学习<br> 监督学习是指利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。在监督学习的过程中会提供对错指示，通过不断地重复训练，使其找到给定的训练数据集中的某种模式或规律，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入和输出，主要应用于分类和预测。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/Supervised-Learning-Algorithms.png" alt="Supervised Learning"></p>
</li>
<li><p>非监督学习<br> 与监督学习不同，在非监督学习中，无须对数据集进行标记，即没有输出。其需要从数据集中发现隐含的某种结构，从而获得样本数据的结构特征，判断哪些数据比较相似。因此，非监督学习目标不是告诉计算机怎么做，而是让它去学习怎样做事情。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/Unsupervised-Learning-Algorithms.png" alt="Unsupervised Learning"></p>
</li>
<li><p>半监督学习<br> 半监督学习是监督学习和非监督学习的结合，其在训练阶段使用的是未标记的数据和已标记的数据，不仅要学习属性之间的结构关系，也要输出分类模型进行预测。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/Semi-supervised-Learning-Algorithms.png" alt="Semi-Supervised Learning"></p>
</li>
<li><p>强化学习<br> 强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。</p>
</li>
</ol>
<h3 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h3><p>机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/machinelearningalgorithms.png" alt="机器学习的算法导图"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/A%20few%20useful%20things%20to%20know%20about%20machine%20learning.jpg" alt="机器学习的注意事项"></p>
<p><strong> 常见的机器学习算法 </strong></p>
<ol>
<li>Linear Algorithms<ol>
<li>Linear Regression</li>
<li>Lasso Regression </li>
<li>Ridge Regression</li>
<li>Logistic Regression</li>
</ol>
</li>
<li>Decision Tree<ol>
<li>ID3</li>
<li>C4.5</li>
<li>CART</li>
</ol>
</li>
<li>SVM</li>
<li>Naive Bayes Algorithms<ol>
<li>Naive Bayes</li>
<li>Gaussian Naive Bayes</li>
<li>Multinomial Naive Bayes</li>
<li>Bayesian Belief Network (BBN)</li>
<li>Bayesian Network (BN)</li>
</ol>
</li>
<li>kNN</li>
<li>Clustering Algorithms<ol>
<li>k-Means</li>
<li>k-Medians</li>
<li>Expectation Maximisation (EM)</li>
<li>Hierarchical Clustering</li>
</ol>
</li>
<li>K-Means</li>
<li>Random Forest</li>
<li>Dimensionality Reduction Algorithms</li>
<li>Gradient Boosting algorithms<ol>
<li>GBM</li>
<li>XGBoost</li>
<li>LightGBM</li>
<li>CatBoost</li>
</ol>
</li>
<li>Deep Learning Algorithms<ol>
<li>Convolutional Neural Network (CNN)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Long Short-Term Memory Networks (LSTMs)</li>
<li>Stacked Auto-Encoders</li>
<li>Deep Boltzmann Machine (DBM)</li>
<li>Deep Belief Networks (DBN)</li>
</ol>
</li>
</ol>
<h3 id="机器学习损失函数-8"><a href="#机器学习损失函数-8" class="headerlink" title="机器学习损失函数 ^8"></a>机器学习损失函数 <a href="https://www.cnblogs.com/lliuye/p/9549881.html" target="_blank" rel="noopener">^8</a></h3><ol>
<li><p>0-1损失函数（0-1 loss function）<br>$$<br> L(y,f(x)) =<br> \begin{cases}<br> 0, &amp; \text{y = f(x)}  \<br> 1, &amp; \text{y $\neq$ f(x)}<br> \end{cases}<br>$$<br>也就是说，当预测错误时，损失函数为1，当预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度。只要错误，就是1。</p>
</li>
<li><p>绝对值损失函数（absolute loss function）<br>$$<br> L(y,f(x))=|y-f(x)|<br>$$<br>该损失函数的意义和上面差不多，只不过是取了绝对值而不是求绝对值，差距不会被平方放大。</p>
</li>
<li><p>平方损失函数（quadratic loss function）<br>$$<br> L(y,f(x))=(y-f(x))^2<br>$$<br>是指预测值与实际值差的平方。</p>
</li>
<li><p>log对数损失函数（logarithmic loss function）<br>$$<br> L(y,f(x))=log(1+e^{-yf(x)})<br>$$<br>这个损失函数就比较难理解了。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。</p>
</li>
<li><p>指数损失函数<br>$$<br> L(y,f(x))=exp(-yf(x))<br>$$</p>
</li>
<li><p>Hinge损失函数<br>Hinge loss一般分类算法中的损失函数，尤其是SVM，其定义为：<br>$$<br> L(w,b)=max{0,1-yf(x)}<br>$$<br>其中$$ y=+1 $$或$$ y=−1 $$ ，$$ f(x)=wx+b $$，当为SVM的线性核时。</p>
</li>
</ol>
<h3 id="机器学习优化方法"><a href="#机器学习优化方法" class="headerlink" title="机器学习优化方法"></a>机器学习优化方法</h3><p>梯度下降是最常用的优化方法之一，它使用梯度的反方向$\nabla_\theta J(\theta)$更新参数$\theta$，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新。</p>
<ol>
<li>(全量)梯度下降<br>$$<br> \theta=\theta-\eta\nabla_\theta J(\theta)<br>$$</li>
<li>随机梯度下降<br>$$<br> \theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})<br>$$</li>
<li>小批量梯度下降<br>$$<br> \theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})<br>$$</li>
<li>引入动量的梯度下降<br>$$<br> \begin{cases}<br> v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \<br> \theta=\theta-v_t<br> \end{cases}<br>$$</li>
<li>自适应学习率的Adagrad算法<br>$$<br> \begin{cases}<br> g_t= \nabla_\theta J(\theta)  \<br> \theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t<br> \end{cases}<br>$$</li>
<li>牛顿法<br>$$<br> \theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)<br>$$<br>其中:<br>$t$: 迭代的轮数<br>$\eta$: 学习率<br>$G_t$: 前t次迭代的梯度和<br>$\varepsilon:$很小的数,防止除0错误<br>$H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计</li>
</ol>
<h3 id="机器学习的评价指标-8"><a href="#机器学习的评价指标-8" class="headerlink" title="机器学习的评价指标 ^8"></a>机器学习的评价指标 <a href="https://www.cnblogs.com/lliuye/p/9549881.html" target="_blank" rel="noopener">^8</a></h3><ol>
<li><p>均方误差 MSE(Mean Squared Error)<br>$$<br> MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2<br>$$<br>均方误差是指参数估计值与参数真值之差平方的期望值; MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。（ $ i $表示第$ i $个样本，$ N $ 表示样本总数）<br><strong>通常用来做回归问题的代价函数。</strong></p>
</li>
<li><p>平均绝对误差 MAE(Mean Absolute Error)<br>$$<br> MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|<br>$$<br>平均绝对误差是绝对误差的平均值 ，平均绝对误差能更好地反映预测值误差的实际情况。<br><strong>通常用来作为回归算法的性能指标。</strong></p>
</li>
<li><p>均方根误差 RMSE(Root Mean Squard Error)<br>$$<br> RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}<br>$$<br>均方根误差是均方误差的算术平方根，能够直观观测预测值与实际值的离散程度。<br><strong>通常用来作为回归算法的性能指标。</strong></p>
</li>
<li><p>Top-k准确率<br>$$<br> Top_k(y,pre_y)=\begin{cases}<br> 1, {y \in pre_y}  \<br> 0, {y \notin pre_y}<br> \end{cases}<br>$$</p>
</li>
<li><p>混淆矩阵 [9]<br><table><br><tr><th>混淆矩阵</th><br><th>Predicted as Positive</th><th>Predicted as Negative</th><br></tr><br><tr><td>Labeled as Positive</td><td>True Positive(TP)</td><td>False Negative(FN)</td></tr><br><tr><td>Labeled as Negative</td><td>False Positive(FP)</td><td>True Negative(TN)</td></tr><br></table></p>
<ul>
<li>真正例(True Positive, TP):真实类别为正例, 预测类别为正例</li>
<li>假负例(False Negative, FN): 真实类别为正例, 预测类别为负例</li>
<li>假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 </li>
<li>真负例(True Negative, TN): 真实类别为负例, 预测类别为负例</li>
<li>真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数<br>$$<br>  TPR=\frac{TP}{TP+FN}<br>$$</li>
<li><p>假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数<br>$$<br>  FNR=\frac{FN}{TP+FN}<br>$$</p>
</li>
<li><p>假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，<br>$$<br>  FPR=\frac{FP}{FP+TN}<br>$$</p>
</li>
<li>真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，<br>$$<br>  TNR=\frac{TN}{FP+TN}<br>$$</li>
<li>准确率(Accuracy)<br>$$<br>  ACC=\frac{TP+TN}{TP+FN+FP+TN}<br>$$</li>
<li>精准率<br>$$<br>  P=\frac{TP}{TP+FP}<br>$$</li>
<li>召回率<br>$$<br>  R=\frac{TP}{TP+FN}<br>$$</li>
<li>F1-Score<br>$$<br>  \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}<br>$$</li>
<li>ROC<br>ROC曲线的横轴为“假正例率”，纵轴为“真正例率”。以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳。</li>
<li><p>AUC(Area Under Curve)<br>AUC就是ROC曲线下的面积。真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/f11f3a292df5e0feaafde78c566034a85fdf7251.jpg" alt></p>
<p>用AUC判断分类器（预测模型）优劣的标准:</p>
<ul>
<li>AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.</li>
<li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="机器学习模型选择"><a href="#机器学习模型选择" class="headerlink" title="机器学习模型选择"></a>机器学习模型选择</h3><ol>
<li><p>交叉验证<br>所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。</p>
</li>
<li><p>k-折叠交叉验证</p>
<ul>
<li>假设训练集为S ，将训练集等分为k份:${S_1, S_2, …, S_k}$。</li>
<li>然后每次从集合中拿出k-1份进行训练</li>
<li>利用集合中剩下的那一份来进行测试并计算损失值</li>
<li>最后得到k次测试得到的损失值，并选择平均损失值最小的模型</li>
</ul>
</li>
<li><p>Bias与Variance，欠拟合与过拟合 <a href="https://zhuanlan.zhihu.com/p/30844838" target="_blank" rel="noopener">^12</a><br>如下图，针对同一组数据的三种模型，可以看出来中间的模型可以更好的表现数据，其中左边的模型一般称为欠拟合，右边的模型称为过拟合。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/v2-fa2af9b3bae2515771ad627f2af14a5b_r.jpg" alt><br><strong>欠拟合</strong>一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.<br><strong>过拟合</strong>一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/v2-e20cd1183ec930a3edc94b30274be29e_r.jpg" alt><br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/7dbd4cd6/v2-22287dec5b6205a5cd45cf6c24773aac_hd.jpg" alt><br>Bias描述的是模型与数据表现的真实情况的差别，Variance描述的是我们的假设与最好的假设之间的差别。</p>
</li>
<li><p>解决方法</p>
<ul>
<li>增加训练样本: 解决高Variance情况</li>
<li>减少特征维数: 解决高Variance情况</li>
<li>增加特征维数: 解决高Bias情况</li>
<li>增加模型复杂度: 解决高Bias情况</li>
<li>减小模型复杂度: 解决高Variance情况</li>
</ul>
</li>
</ol>
<h3 id="机器学习参数调优"><a href="#机器学习参数调优" class="headerlink" title="机器学习参数调优"></a>机器学习参数调优</h3><ol>
<li><p>网格搜索<br>一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果</p>
</li>
<li><p>随机搜索<br>与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。</p>
</li>
<li><p>贝叶斯优化算法<br>贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</p>
</li>
</ol>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Day2 学习TF-IDF理论并实践</title>
    <url>/p/2019/04/07/a4fc168b/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>学习TF-IDF理论并实践，使用TF-IDF表示文本</p>
<a id="more"></a>
<hr>
<h2 id="TF-IDF-Term-Frequency–Inverse-Document-Frequency"><a href="#TF-IDF-Term-Frequency–Inverse-Document-Frequency" class="headerlink" title="TF-IDF(Term Frequency–Inverse Document Frequency)"></a>TF-IDF(Term Frequency–Inverse Document Frequency)</h2><blockquote>
<p>主要思想是：如果词i在一篇文档j中出现的频率高，并且在其他文档中很少出现，则认为词i具有很好的区分能力，适合用来把文章j和其他文章区分开来。适合用来分类。</p>
</blockquote>
<p>TF-IDF实际上是：TF * IDF<br>$$ tfidf_{i,j}=tf_{i,j}\times idf_{i} $$</p>
<p>TF词频(Term Frequency):某一个给定的词语在该文件中出现的频率。<br>$$ tf_{i,j}=\frac{n_{i,j} }{\sum <em>{k}n</em>{k,j}} $$</p>
<p>IDF逆向文件频率(Inverse Document Frequency):是一个词语普遍重要性的度量。如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。如果某一类文档C中包含词条t的文档数为m，而其它类包含t的文档总数为k，显然所有包含t的文档数n=m+k，当m大的时候，n也大，按照IDF公式得到的IDF的值会小，就说明该词条t类别区分能力不强。<br>$$ idf_{i}=lg\frac{\left | D \right |}{\left | \left { j:t_{i}\in d_{j} \right } \right |} $$</p>
<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), min_df=<span class="number">3</span>, max_df=<span class="number">0.9</span>, sublinear_tf=<span class="literal">True</span>)</span><br><span class="line">vectorizer.fit(train_set[<span class="string">'word_seg'</span>])</span><br><span class="line">x_train = vectorizer.transform(train_set[<span class="string">'word_seg'</span>])</span><br><span class="line">x_val = vectorizer.transform(val_set[<span class="string">'word_seg'</span>])</span><br><span class="line"></span><br><span class="line">print(x_trains)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Datawhale</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>nlp</tag>
        <tag>machine learning</tag>
        <tag>tf-idf</tag>
      </tags>
  </entry>
  <entry>
    <title>Day3 学习Word2Vec理论并实践</title>
    <url>/p/2019/04/09/40163d71/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>学习word2vec词向量原理并实践，用来表示文本</p>
<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p><a href="https://www.bilibili.com/video/av41393758/?p=2" target="_blank" rel="noopener">CS224n 斯坦福深度自然语言处理课</a><br><a id="more"></a></p>
<hr>
<h2 id="One-Hot编码"><a href="#One-Hot编码" class="headerlink" title="One-Hot编码"></a>One-Hot编码</h2><blockquote>
<p>One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。</p>
</blockquote>
<p>One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing  </span><br><span class="line">   </span><br><span class="line">enc = preprocessing.OneHotEncoder()  </span><br><span class="line">enc.fit([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]])  <span class="comment">#这里一共有4个数据，3种特征</span></span><br><span class="line">   </span><br><span class="line">array = enc.transform([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>]]).toarray()  <span class="comment">#这里使用一个新的数据来测试</span></span><br><span class="line">   </span><br><span class="line"><span class="keyword">print</span> array   <span class="comment"># [[ 1  0  0  1  0  0  0  0  1]]</span></span><br></pre></td></tr></table></figure>
<p>将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><blockquote>
<p>Word2vec，是一群用来产生词向量(word embedding)的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。</p>
</blockquote>
<p>Predict between every word and its context word!</p>
<p>两种算法：Skip-grams(SG)和Continuous Bag of Words(CBOW)<br><img data-src="http://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/sg-and-cbow.jpg?Expires=1554825387&amp;OSSAccessKeyId=TMP.AQEikGQYqIYajs8ySbZHclaK0dte9NKTBtT652mkGdKgh--kqX9etfeD_AH4ADAtAhRE8PfThSeXBuFYPSaQvhEC81yUkgIVAOn1w4pDv6Atbb-YyivFf7W_L16C&amp;Signature=cNbuThZa5T0HaFd7PrWS8zB4lYQ%3D" alt="SG&amp;CBOW"></p>
<h3 id="SG-Skip-Gram"><a href="#SG-Skip-Gram" class="headerlink" title="SG(Skip-Gram)"></a>SG(Skip-Gram)</h3><blockquote>
<p>顾名思义，Skip-gram 就是“跳过某些符号”，是一个简单但却非常实用的模型。</p>
</blockquote>
<p>Skip-grams模型输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。<br>Skip-gram 模型解决的是n元模型中，因为窗口大小的限制，导致超出窗口范围的词语与当前词之间的关系不能被正确地反映到模型之中，如果单纯扩大窗口大小又会增加训练的复杂度。<br>例如，句子“中国足球踢得真是太烂了”有4个3元词组，分别是“中国足球踢得”、“足球踢得真是”、“踢得真是太烂”、“真是太烂了”，可是我们发现，这个句子的本意就是“中国足球太烂”可是上述 4个3元词组并不能反映出这个信息。Skip-gram 模型却允许某些词被跳过，因此可以组成“中国足球太烂”这个3元词组。 如果允许跳过2个词，即 2-Skip-gram。</p>
<h3 id="CBOW-Continuous-Bag-of-Words"><a href="#CBOW-Continuous-Bag-of-Words" class="headerlink" title="CBOW(Continuous Bag-of-Words)"></a>CBOW(Continuous Bag-of-Words)</h3><blockquote>
<p>Skip-Gram模型和CBOW的思路是反着来的，CBOW的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。</p>
</blockquote>
<p>上面两个模型里面包含三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中V是词汇表的大小。<br><img data-src="http://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/word2vec.png?Expires=1554825360&amp;OSSAccessKeyId=TMP.AQEikGQYqIYajs8ySbZHclaK0dte9NKTBtT652mkGdKgh--kqX9etfeD_AH4ADAtAhRE8PfThSeXBuFYPSaQvhEC81yUkgIVAOn1w4pDv6Atbb-YyivFf7W_L16C&amp;Signature=2%2FCxpGpPqybpwVfle%2BX0Ott%2BHnw%3D" alt="Word2Vec"></p>
<p>word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。</p>
<h3 id="Hierarchical-softmax"><a href="#Hierarchical-softmax" class="headerlink" title="Hierarchical softmax"></a>Hierarchical softmax</h3><p>word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。<br>第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。<br>和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。</p>
<ol>
<li>基于Hierarchical Softmax的模型梯度计算</li>
<li>基于Hierarchical Softmax的CBOW模型</li>
<li>基于Hierarchical Softmax的Skip-Gram模型<br>使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。</li>
</ol>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解。</p>
<ol>
<li>基于Negative Sampling的模型梯度计算</li>
<li>Negative Sampling负采样方法</li>
<li>基于Negative Sampling的CBOW模型</li>
<li>基于Negative Sampling的Skip-Gram模型</li>
</ol>
<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>这里用到gensim库，gensim是一个很好用的Python NLP的包，不光可以用于使用word2vec，还有很多其他的API可以用。它封装了google的C语言版的word2vec。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv,sys</span><br><span class="line">vector_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 拆分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence2list</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sentence.strip().split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">df_train = pd.read_csv(<span class="string">'../data/train_set1.csv'</span>,engine=<span class="string">'python'</span>) </span><br><span class="line">df_test = pd.read_csv(<span class="string">'../data/test_set1.csv'</span>,engine=<span class="string">'python'</span>)</span><br><span class="line">sentences_train = list(df_train.loc[:, <span class="string">'word_seg'</span>].apply(sentence2list))</span><br><span class="line">sentences_test = list(df_test.loc[:, <span class="string">'word_seg'</span>].apply(sentence2list))</span><br><span class="line">sentences = sentences_train + sentences_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 开始训练</span></span><br><span class="line">model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=<span class="number">8</span>, sg=<span class="number">0</span>, iter=<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>gensim.models.Word2Vec需要注意的参数有：</p>
<ul>
<li>sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。后面我们会有从文件读出的例子。</li>
<li>size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。</li>
<li>window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为c，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</li>
<li>sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。</li>
<li>hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。</li>
<li>negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。</li>
<li>cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xw,默认值也是1,不推荐修改默认值。</li>
<li>min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。</li>
<li>iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</li>
<li>alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为η，默认是0.025。</li>
<li>min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存训练结果</span></span><br><span class="line">wv = model.wv</span><br><span class="line">vocab_list = wv.index2word</span><br><span class="line">word_idx_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> idx, word <span class="keyword">in</span> enumerate(vocab_list):</span><br><span class="line">    word_idx_dict[word] = idx</span><br><span class="line">    </span><br><span class="line">vectors_arr = wv.vectors</span><br><span class="line">vectors_arr = np.concatenate((np.zeros(vector_size)[np.newaxis, :], vectors_arr), axis=<span class="number">0</span>)<span class="comment">#第0位置的vector为'unk'的vector</span></span><br><span class="line"></span><br><span class="line">f_wordidx = open(feature_path + <span class="string">'word_seg_word_idx_dict.pkl'</span>, <span class="string">'wb'</span>)</span><br><span class="line">f_vectors = open(feature_path + <span class="string">'word_seg_vectors_arr.pkl'</span>, <span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(word_idx_dict, f_wordidx)</span><br><span class="line">pickle.dump(vectors_arr, f_vectors)</span><br><span class="line">f_wordidx.close()</span><br><span class="line">f_vectors.close()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Datawhale</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>nlp</tag>
        <tag>machine learning</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>Day2 线性回归</title>
    <url>/p/2020/01/11/b49a15ed/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>模型建立：线性回归原理、线性回归模型</li>
<li>学习策略：线性回归损失函数、代价函数、目标函数</li>
<li>算法求解：梯度下降法、牛顿法、拟牛顿法等</li>
<li>线性回归的评估指标</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="理论部分-1"><a href="#理论部分-1" class="headerlink" title="理论部分"></a>理论部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task2_Linear_regression.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task2_Linear_regression.ipynb</a></p>
<ul>
<li>基于线性回归的房价预测问题</li>
<li>利用sklearn解决回归问题</li>
<li>sklearn.linear_model.LinearRegression</li>
</ul>
<a id="more"></a>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><blockquote>
<p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为$y = w’x+e$，$e$为误差服从均值为0的正态分布。<br>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。</p>
</blockquote>
<h3 id="线性回归的原理"><a href="#线性回归的原理" class="headerlink" title="线性回归的原理"></a>线性回归的原理</h3><p>进入一家房产网，可以看到房价、面积、厅室呈现以下数据：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b49a15ed/sogou20200111220714.png" alt="某房产网上二手房数据"><br>我们可以将价格和面积、厅室数量的关系习得为$f(x)=\theta_0+\theta_1x_1+\theta_2x_2$，使得$f(x)\approx y$，这就是一个直观的线性回归的样式。</p>
<blockquote>
<p><strong>小练习</strong>：这是国内一个房产网站上任意搜的数据，有兴趣可以找个网站观察一下，还可以获得哪些可能影响到房价的因素？可能会如何影响到实际房价呢？</p>
</blockquote>
<h4 id="线性回归的一般形式"><a href="#线性回归的一般形式" class="headerlink" title="线性回归的一般形式"></a>线性回归的一般形式</h4><p>有数据集${(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$,其中,$x_i = (x_{i1};x_{i2};x_{i3};…;x_{id}),y_i\in R$，其中$n$表示变量的数量，$d$表示每个变量的维度。可以用以下函数来描述$y$和$x$之间的关系：<br>$$\begin{align<em>}<br>f(x) &amp;= \theta_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_dx_d  \<br>&amp;= \sum_{i=0}^{d}\theta_ix_i \<br>\end{align</em>}$$<br>如何来确定$\theta$的值，使得$f(x)$尽可能接近$y$的值呢？均方误差是回归中常用的性能度量，即：<br>$$<br>J(\theta)=\frac{1}{2}\sum_{j=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2<br>$$<br>我们可以选择$\theta$，试图让均方误差最小化：</p>
<h4 id="极大似然估计（概率角度的诠释）"><a href="#极大似然估计（概率角度的诠释）" class="headerlink" title="极大似然估计（概率角度的诠释）"></a>极大似然估计（概率角度的诠释）</h4><p>下面我们用极大似然估计，来解释为什么要用均方误差作为性能度量。我们可以把目标值和变量写成如下等式：<br>$$<br>y^{(i)} = \theta^T x^{(i)}+\epsilon^{(i)}<br>$$<br>$\epsilon$表示我们未观测到的变量的印象，即随机噪音。我们假定$\epsilon$是独立同分布，服从高斯分布。（根据中心极限定理）<br>$$<br>p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)<br>$$<br>因此，<br>$$<br>p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}\right)<br>$$<br>我们建立极大似然函数，即描述数据遵从当前样本分布的概率分布函数。由于样本的数据集独立同分布，因此可以写成<br>$$<br>L(\theta) = p(\vec y | X;\theta) = \prod^n_{i=1}\frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}\right)<br>$$<br>选择$\theta$，使得似然函数最大化，这就是极大似然估计的思想。<br>为了方便计算，我们计算时通常对对数似然函数求最大值：<br>$$\begin{align<em>}<br>l(\theta) &amp;= log L(\theta) = log \prod^n_{i=1}\frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\theta^T x^{(i)})^2} {2\sigma^2}\right)  \<br>&amp;= \sum^n_{i=1}log\frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}\right) \<br>&amp;= nlog\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2}\sum^n_{i=1}((y^{(i)}-\theta^T x^{(i)})^2<br>\end{align</em>}$$<br>显然，最大化$l(\theta)$即最小化 $\frac{1}{2}\sum^n_{i=1}((y^{(i)}-\theta^T x^{(i)})^2$。<br>这一结果即均方误差，因此用这个值作为代价函数来优化模型在统计学的角度是合理的。</p>
<h3 id="线性回归损失函数、代价函数、目标函数"><a href="#线性回归损失函数、代价函数、目标函数" class="headerlink" title="线性回归损失函数、代价函数、目标函数"></a>线性回归损失函数、代价函数、目标函数</h3><ul>
<li><strong>损失函数(Loss Function)：</strong>度量单样本预测的错误程度，损失函数值越小，模型就越好。</li>
<li><strong>代价函数(Cost Function)：</strong>度量全部样本集的平均误差。</li>
<li><strong>目标函数(Object Function)：</strong>代价函数和正则化函数，最终要优化的函数。</li>
</ul>
<p>常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等；<br>常用的代价函数包括均方误差、均方根误差、平均绝对误差等。</p>
<blockquote>
<p><strong>思考题</strong>：既然代价函数已经可以度量样本集的平均误差，为什么还要设定目标函数？</p>
<blockquote>
<p><strong>回答</strong>：当模型复杂度增加时，有可能对训练集可以模拟的很好，但是预测测试集的效果不好，出现过拟合现象，这就出现了所谓的“结构化风险”。结构风险最小化即为了防止过拟合而提出来的策略，定义模型复杂度为$J(F)$，目标函数可表示为：<br>$$\underset{f\in F}{min}\, \frac{1}{n}\sum^{n}_{i=1}L(y_i,f(x_i))+\lambda J(F)$$</p>
</blockquote>
</blockquote>
<p>例如有以上6个房价和面积关系的数据点，可以看到，当设定$f(x)=\sum_{j=0}^{5}\theta_jx_j$时，可以完美拟合训练集数据，但是，真实情况下房价和面积不可能是这样的关系，出现了过拟合现象。当训练集本身存在噪声时，拟合曲线对未知影响因素的拟合往往不是最好的。 通常，随着模型复杂度的增加，训练误差会减少；但测试误差会先增加后减小。我们的最终目的时试测试误差达到最小，这就是我们为什么需要选取适合的目标函数的原因。</p>
<h3 id="线性回归的优化方法"><a href="#线性回归的优化方法" class="headerlink" title="线性回归的优化方法"></a>线性回归的优化方法</h3><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>设定初始参数$\theta$,不断迭代，使得$J(\theta)$最小化：<br>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial{J(\theta)}}{\partial\theta}<br>$$</p>
<p>$$\begin{align<em>}<br>\frac{\partial{J(\theta)}}{\partial\theta}<br>&amp;= \frac{\partial}{\partial\theta_j}\frac{1}{2}\sum_{i=1}^{n}(f_\theta(x)^{(i)}-y^{(i)})^2 \<br>&amp;= 2</em>\frac{1}{2}\sum_{i=1}^{n}(f_\theta(x)^{(i)}-y^{(i)})<em>\frac{\partial}{\partial\theta_j}(f_\theta(x)^{(i)}-y^{(i)}) \<br>&amp;= \sum_{i=1}^{n}(f_\theta(x)^{(i)}-y^{(i)})</em>\frac{\partial}{\partial\theta_j}(\sum_{j=0}^{d}\theta_jx_j^{(i)}-y^{(i)}))\<br>&amp;= \sum_{i=1}^{n}(f_\theta(x)^{(i)}-y^{(i)})x_j^{(i)} \<br>\end{align*}$$<br>即：<br>$$<br>\theta_j = \theta_j + \alpha\sum_{i=1}^{n}(y^{(i)}-f_\theta(x)^{(i)})x_j^{(i)}<br>$$<br>注：下标j表示第$j$个参数，上标$i$表示第$i$个数据点。<br>将所有的参数以向量形式表示，可得：<br>$$<br>\theta = \theta + \alpha\sum_{i=1}^{n}(y^{(i)}-f_\theta(x)^{(i)})x^{(i)}<br>$$<br>由于这个方法中，参数在每一个数据点上同时进行了移动，因此称为批梯度下降法，对应的，我们可以每一次让参数只针对一个数据点进行移动，即：<br>$$<br>\theta = \theta + \alpha(y^{(i)}-f_\theta(x)^{(i)})x^{(i)}<br>$$<br>这个算法成为随机梯度下降法，随机梯度下降法的好处是，当数据点很多时，运行效率更高；缺点是，因为每次只针对一个样本更新参数，未必找到最快路径达到最优值，甚至有时候会出现参数在最小值附近徘徊而不是立即收敛。但当数据量很大的时候，随机梯度下降法经常优于批梯度下降法。</p>
<p>在直观上，我们可以这样理解，看下图，一开始的时候我们随机站在一个点，把他看成一座山，每一步，我们都以下降最多的路线来下山，那么，在这个过程中我们到达山底（最优点）是最快的，而上面的a，它决定了我们“向下山走”时每一步的大小，过小的话收敛太慢，过大的话可能错过最小值，扯到蛋…）。这是一种很自然的算法，每一步总是寻找使J下降最“陡”的方向（就像找最快下山的路一样）。<a href="https://www.zhihu.com/question/264189719" target="_blank" rel="noopener">^2</a><br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b49a15ed/v2-77ab0b5fab734c998db19771ed81a0dc_hd.jpg" alt="高维梯度下降"><br>当然了，我们直观上理解了之后，接下来肯定是从数学的角度，我们可以这样想，先想在低维的时候，比如二维，我们要找到最小值，其实可以是这样的方法，具体化到1元函数中时，梯度方向首先是沿着曲线的切线的，然后取切线向上增长的方向为梯度方向，2元或者多元函数中，梯度向量为函数值f对每个变量的导数，该向量的方向就是梯度的方向，当然向量的大小也就是梯度的大小。现在假设我们要求函数的最值，采用梯度下降法，<a href="https://www.zhihu.com/question/264189719" target="_blank" rel="noopener">^2</a>结合如图所示：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b49a15ed/v2-982a9e8f26297deff656990e049383b8_hd.jpg" alt="二维梯度下降"></p>
<p>当J为凸函数时，梯度下降法相当于让参数$\theta$不断向J的最小值位置移动。</p>
<p>梯度下降法的缺陷：如果函数为非凸函数，有可能找到的并非全局最优值，而是局部最优值。</p>
<h5 id="梯度下降是用来做什么的-1"><a href="#梯度下降是用来做什么的-1" class="headerlink" title="梯度下降是用来做什么的? ^1"></a>梯度下降是用来做什么的? <a href="https://www.cnblogs.com/lc1217/p/7085034.html" target="_blank" rel="noopener">^1</a></h5><p>在机器学习算法中,有时候需要对原始的模型构建损失函数,然后通过优化算法对损失函数进行优化，以便寻找到最优的参数，使得损失函数的值最小。而在求解机器学习参数的优化算法中，使用较多的就是基于梯度下降的优化算法(Gradient Descent, GD)。</p>
<h5 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点 ^1"></a>优缺点 <a href="https://www.cnblogs.com/lc1217/p/7085034.html" target="_blank" rel="noopener">^1</a></h5><p><strong>优点</strong>：效率。在梯度下降法的求解过程中，只需求解损失函数的一阶导数，计算的代价比较小，可以在很多大规模数据集上应用。<br><strong>缺点</strong>：求解的是局部最优值，即由于方向选择的问题，得到的结果不一定是全局最优步长选择，过小使得函数收敛速度慢，过大又容易找不到最优解。</p>
<h5 id="梯度下降的变形形式-1"><a href="#梯度下降的变形形式-1" class="headerlink" title="梯度下降的变形形式 ^1"></a>梯度下降的变形形式 <a href="https://www.cnblogs.com/lc1217/p/7085034.html" target="_blank" rel="noopener">^1</a></h5><p>根据处理的训练数据的不同，主要有以下三种形式：</p>
<ol>
<li><strong>批量梯度下降法BGD(Batch Gradient Descent)</strong><br>针对的是整个数据集，通过对所有的样本的计算来求解梯度的方向。<br>优点：全局最优解；易于并行实现；<br>缺点：当样本数据很多时，计算量开销大，计算速度慢</li>
<li><strong>小批量梯度下降法MBGD（mini-batch Gradient Descent）</strong><br>把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性<br>优点：减少了计算的开销量，降低了随机性</li>
<li><strong>随机梯度下降法SGD（stochastic gradient descent）</strong><br>每个数据都计算算一下损失函数，然后求梯度更新参数。<br>优点：计算速度快<br>缺点：收敛性能不好</li>
</ol>
<blockquote>
<p>总结：SGD可以看作是MBGD的一个特例，及batch_size=1的情况。在深度学习及机器学习中，基本上都是使用的MBGD算法。</p>
</blockquote>
<h4 id="最小二乘法矩阵求解"><a href="#最小二乘法矩阵求解" class="headerlink" title="最小二乘法矩阵求解"></a>最小二乘法矩阵求解</h4><p>令<br>$$<br>X = \left[ \begin{array} {cccc}<br>(x^{(1)})^T\<br>(x^{(2)})^T\<br>\ldots \<br>(x^{(n)})^T<br>\end{array} \right]<br>$$<br>其中，<br>$$<br>x^{(i)} = \left[ \begin{array} {cccc}<br>x_1^{(i)}\<br>x_2^{(i)}\<br>\ldots \<br>x_d^{(i)}<br>\end{array} \right]<br>$$<br>由于<br>$$<br>Y = \left[ \begin{array} {cccc}<br>y^{(1)}\<br>y^{(2)}\<br>\ldots \<br>y^{(n)}<br>\end{array} \right]<br>$$<br>$h_\theta(x)$可以写作<br>$$<br>h_\theta(x)=X\theta<br>$$<br>对于向量来说，有<br>$$<br>z^Tz = \sum_i z_i^2<br>$$<br>因此可以把损失函数写作<br>$$<br>J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)<br>$$<br>为最小化$J(\theta)$,对$\theta$求导可得：<br>$$\begin{align<em>}<br>\frac{\partial{J(\theta)}}{\partial\theta}<br>&amp;= \frac{\partial}{\partial\theta} \frac{1}{2}(X\theta-Y)^T(X\theta-Y) \<br>&amp;= \frac{1}{2}\frac{\partial}{\partial\theta} (\theta^TX^TX\theta - Y^TX\theta-\theta^T X^TY - Y^TY) \<br>\end{align</em>}$$<br>中间两项互为转置，由于求得的值是个标量，矩阵与转置相同，因此可以写成<br>$$\begin{align<em>}<br>\frac{\partial{J(\theta)}}{\partial\theta}<br>&amp;= \frac{1}{2}\frac{\partial}{\partial\theta} (\theta^TX^TX\theta - 2\theta^T X^TY - Y^TY) \<br>\end{align</em>}$$<br>令偏导数等于零，由于最后一项和$\theta$无关，偏导数为0。<br>因此，<br>$$\frac{\partial{J(\theta)}}{\partial\theta}  = \frac{1}{2}\frac{\partial}{\partial\theta} \theta^TX^TX\theta - \frac{\partial}{\partial\theta} \theta^T X^TY<br>$$<br>利用矩阵求导性质，<br>$$<br>\frac{\partial \vec x^T\alpha}{\partial \vec x} =\alpha<br>$$<br>$$<br>和<br>$$<br>$$\frac{\partial A^TB}{\partial \vec x} = \frac{\partial A^T}{\partial \vec x}B + \frac{\partial B^T}{\partial \vec x}A$$<br>$$\begin{align<em>}<br>\frac{\partial}{\partial\theta} \theta^TX^TX\theta<br>&amp;= \frac{\partial}{\partial\theta}{(X\theta)^TX\theta}\<br>&amp;= \frac{\partial (X\theta)^T}{\partial\theta}X\theta + \frac{\partial (X\theta)^T}{\partial\theta}X\theta \<br>&amp;= 2X^TX\theta<br>\end{align</em>}$$<br>$$\frac{\partial{J(\theta)}}{\partial\theta} = X^TX\theta - X^TY<br>$$<br>令导数等于零，<br>$$X^TX\theta = X^TY$$<br>$$\theta = (X^TX)^{(-1)}X^TY<br>$$</p>
<blockquote>
<p>注：CS229视频中吴恩达的推导利用了矩阵迹的性质，可自行参考学习。</p>
</blockquote>
<h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>牛顿法（英语：Newton’s method）又称为牛顿-拉弗森方法（英语：Newton-Raphson method），它是一种在实数域和复数域上近似求解方程的方法。方法使用函数$\displaystyle f(x)$的泰勒级数的前面几项来寻找方程$\displaystyle f(y)=0$的根。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/b49a15ed/69557176.jpg" alt="牛顿法"></p>
<p>通过图例可知(参考吴恩达CS229),<br>$$f(\theta)’ = \frac{f(\theta)}{\Delta},\Delta = \theta_0 - \theta_1$$<br>$$可求得，\theta_1 = \theta_0 - \frac {f(\theta_0)}{f(\theta_0)’}$$<br>重复迭代，可以让逼近取到$f(\theta)$的最小值<br>当我们对损失函数$l(\theta)$进行优化的时候，实际上是想要取到$l’(\theta)$的最小值，因此迭代公式为：<br>$$<br>\theta :=\theta-\frac{l’(\theta)}{l’’(\theta)}<br>$$<br>$$<br>当\theta是向量值的时候，\theta :=\theta - H^{-1}\Delta_{\theta}l(\theta)<br>$$<br>其中，$\Delta_{\theta}l(\theta)$是$l(\theta)$对$\theta_i$的偏导数，$H$是$J(\theta)$的海森矩阵，<br>$$H_{ij} = \frac{\partial ^2l(\theta)}{\partial\theta_i\partial\theta_j}$$</p>
<blockquote>
<p><strong>思考题</strong>：请用泰勒展开法推导牛顿法公式。</p>
<blockquote>
<p><strong>回答</strong>：将$f(x)$用泰勒公式展开到第二阶，<br>$f(x) = f(x_0) + f’(x_0)(x - x_0)+\frac{1}{2}f’’(x_0)(x - x_0)^2$<br>对上式求导，并令导数等于0，求得x值<br>$$f’(x) = f’(x_0) + f’’(x_0)x -f’’(x_0)x_0 = 0$$<br>可以求得，<br>$$x = x_0 - \frac{f’(x_0)}{f’’(x_0)}$$<br>牛顿法的收敛速度非常快，但海森矩阵的计算较为复杂，尤其当参数的维度很多时，会耗费大量计算成本。我们可以用其他矩阵替代海森矩阵，用拟牛顿法进行估计，</p>
</blockquote>
</blockquote>
<h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>拟牛顿法的思路是用一个矩阵替代计算复杂的海森矩阵H，因此要找到符合H性质的矩阵。<br>要求得海森矩阵符合的条件，同样对泰勒公式求导$f’(x) = f’(x_0) + f’’(x_0)x -f’’(x_0)x_0$<br>令$x = x_1$，即迭代后的值，代入可得：<br>$$f’(x_1) = f’(x_0) + f’’(x_0)x_1 - f’’(x_0)x_0$$<br>更一般的，<br>$$f’(x_{k+1}) = f’(x_k) + f’’(x_k)x_{k+1} - f’’(x_k)x_k$$<br>$$f’(x_{k+1}) - f’(x_k)  = f’’(x_k)(x_{k+1}- x_k)= H(x_{k+1}- x_k)$$<br>$x_k$为第k个迭代值<br>即找到矩阵G，使得它符合上式。 常用的拟牛顿法的算法包括DFP，BFGS等，作为选学内容，有兴趣者可自行查询材料学习。</p>
<h3 id="线性回归的评价指标"><a href="#线性回归的评价指标" class="headerlink" title="线性回归的评价指标"></a>线性回归的评价指标</h3><ul>
<li><strong>均方误差(MSE)</strong>:$\frac{1}{m}\sum^{m}_{i=1}(y^{(i)} - \hat y^{(i)})^2$</li>
<li><strong>均方根误差(RMSE)</strong>：$\sqrt{MSE} = \sqrt{\frac{1}{m}\sum^{m}_{i=1}(y^{(i)} - \hat y^{(i)})^2}$</li>
<li><strong>平均绝对误差(MAE)</strong>：$\frac{1}{m}\sum^{m}_{i=1} | (y^{(i)} - \hat y^{(i)} | $</li>
</ul>
<p>但以上评价指标都无法消除量纲不一致而导致的误差值差别大的问题，最常用的指标是$R^2$,可以避免量纲不一致问题<br>$$<br>R^2: = 1-\frac{\sum^{m}<em>{i=1}(y^{(i)} - \hat y^{(i)})^2}{\sum^{m}</em>{i=1}(\bar y - \hat y^{(i)})^2} =1-\frac{\frac{1}{m}\sum^{m}<em>{i=1}(y^{(i)} - \hat y^{(i)})^2}{\frac{1}{m}\sum^{m}</em>{i=1}(\bar y - \hat y^{(i)})^2} = 1-\frac{MSE}{VAR}<br>$$<br>我们可以把$R^2$理解为，回归模型可以成功解释的数据方差部分在数据固有方差中所占的比例，$R^2$越接近1，表示可解释力度越大，模型拟合的效果越好。</p>
<h3 id="sklearn-linear-model参数详解-3"><a href="#sklearn-linear-model参数详解-3" class="headerlink" title="sklearn.linear_model参数详解 ^3"></a>sklearn.linear_model参数详解 <a href="https://blog.csdn.net/weixin_39175124/article/details/79465558" target="_blank" rel="noopener">^3</a></h3><ul>
<li><strong>fit_intercept</strong> : 默认为True,是否计算该模型的截距。如果使用中心化的数据，可以考虑设置为False,不考虑截距。注意这里是考虑，一般还是要考虑截距</li>
<li><strong>normalize</strong>: 默认为false. 当fit_intercept设置为false的时候，这个参数会被自动忽略。如果为True,回归器会标准化输入参数：减去平均值，并且除以相应的二范数。当然啦，在这里还是建议将标准化的工作放在训练模型之前。通过设置sklearn.preprocessing.StandardScaler来实现，而在此处设置为false</li>
<li><strong>copy_X</strong>: 默认为True, 否则X会被改写</li>
<li><strong>n_jobs</strong>: int 默认为1. 当-1时默认使用全部CPUs ??(这个参数有待尝试)</li>
</ul>
<p>可用属性：</p>
<ul>
<li><strong>coef_</strong>:训练后的输入端模型系数，如果label有两个，即y值有两列。那么是一个2D的array</li>
<li><strong>intercept_</strong>: 截距</li>
</ul>
<p>可用的methods:</p>
<ul>
<li><strong>fit(X,y,sample_weight=None)</strong>: X: array, 稀疏矩阵 [n_samples,n_features] y: array [n_samples, n_targets] sample_weight**: 权重 array [n_samples] 在版本0.17后添加了sample_weight</li>
<li><strong>get_params(deep=True)</strong>： 返回对regressor 的设置值</li>
<li><strong>predict(X)</strong>: 预测 基于 R^2值</li>
<li><strong>score</strong>： 评估</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>1.生成数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1234</span>) <span class="comment">#生成随机数</span></span><br><span class="line">x = np.random.rand(<span class="number">500</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7*x1 + 10.8*x2，可自行设置值进行尝试</span></span><br><span class="line">y = x.dot(np.array([<span class="number">4.2</span>,<span class="number">5.7</span>,<span class="number">10.8</span>]))</span><br></pre></td></tr></table></figure></p>
<p>2.先尝试调用sklearn的线性回归模型训练数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">lr = LinearRegression(fit_intercept=<span class="literal">True</span>) <span class="comment"># 调用模型</span></span><br><span class="line"></span><br><span class="line">lr.fit(x,y) <span class="comment"># 训练模型</span></span><br><span class="line">print(<span class="string">"估计的参数值为：%s"</span> %(lr.coef_)) <span class="comment"># 估计的参数值为：[ 4.2 5.7 10.8]</span></span><br><span class="line"><span class="comment"># 计算R平方</span></span><br><span class="line">print(<span class="string">'R2:%s'</span> %(lr.score(x,y))) <span class="comment"># R2:1.0</span></span><br><span class="line"><span class="comment"># 任意设定变量，预测目标值</span></span><br><span class="line">x_test = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">y_hat = lr.predict(x_test)</span><br><span class="line">print(<span class="string">"预测值为: %s"</span> %(y_hat))  <span class="comment"># 预测值为: [85.2]</span></span><br></pre></td></tr></table></figure></p>
<p>3.最小二乘法的矩阵求解<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR_LS</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = <span class="literal">None</span>      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="comment"># 最小二乘法矩阵求解</span></span><br><span class="line">        temp0 = np.dot(X.T,X)</span><br><span class="line">        temp = np.dot(np.linalg.inv(temp0),X.T)</span><br><span class="line">        self.w = np.dot(temp,y)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 用已经拟合的参数值预测新自变量</span></span><br><span class="line">        y_pred = np.dot(X,self.w)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    lr_ls = LR_LS()</span><br><span class="line">    lr_ls.fit(x,y)</span><br><span class="line">    print(<span class="string">"估计的参数值：%s"</span> %(lr_ls.w)) <span class="comment"># 估计的参数值：[ 4.2 5.7 10.8]</span></span><br><span class="line">    x_test = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"预测值为: %s"</span> %(lr_ls.predict(x_test))) <span class="comment">#预测值为: [85.2]</span></span><br></pre></td></tr></table></figure></p>
<p>4.梯度下降法<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LR_GD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = <span class="literal">None</span>     </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,X,y,alpha=<span class="number">0.02</span>,loss = <span class="number">1e-10</span>)</span>:</span> <span class="comment"># 设定步长为0.002,判断是否收敛的条件为1e-10</span></span><br><span class="line">        y = y.reshape(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#重塑y值的维度以便矩阵运算</span></span><br><span class="line">        [m,d] = np.shape(X) <span class="comment">#自变量的维度</span></span><br><span class="line">        self.w = np.zeros((d)) <span class="comment">#将参数的初始值定为0</span></span><br><span class="line">        tol = <span class="number">1e5</span></span><br><span class="line">        <span class="keyword">while</span> tol &gt; loss:</span><br><span class="line">            temp = X.dot(self.w)-y</span><br><span class="line">            self.w = self.w<span class="number">-1</span>/m*alpha*(temp.dot(X))</span><br><span class="line">            tol = np.abs(np.sum(temp))</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 用已经拟合的参数值预测新自变量</span></span><br><span class="line">        y_pred = X.dot(self.w)</span><br><span class="line">        <span class="keyword">return</span> y_pred  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    lr_gd = LR_GD()</span><br><span class="line">    lr_gd.fit(x,y)</span><br><span class="line">    print(<span class="string">"估计的参数值为：%s"</span> %(lr_gd.w)) <span class="comment"># 估计的参数值为：[ 4.20000001 5.70000003 10.79999997]</span></span><br><span class="line">    x_test = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">    print(<span class="string">"预测值为：%s"</span> %(lr_gd.predict(x_test))) <span class="comment"># 预测值为：[85.19999995]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>Day3 逻辑回归</title>
    <url>/p/2020/01/13/4786a2a0/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>逻辑回归与线性回归的联系与区别</li>
<li>模型建立：逻辑回归原理、逻辑回归模型</li>
<li>学习策略：逻辑回归损失函数、推导及优化</li>
<li>算法求解：批量梯度下降</li>
<li>正则化与模型评估指标</li>
<li>逻辑回归的优缺点</li>
<li>样本不均衡问题</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="练习部分"><a href="#练习部分" class="headerlink" title="练习部分"></a>练习部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task3_logistic_regression.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task3_logistic_regression.ipynb</a></p>
<ul>
<li>利用sklearn解决分类问题</li>
<li>sklearn.linear_model.LogisticRegression</li>
<li>利用梯度下降法将相同的数据分类，画图和sklearn的结果相比较</li>
<li>利用牛顿法实现结果，画图和sklearn的结果相比较，并比较牛顿法和梯度下降法迭代收敛的次数</li>
</ul>
<a id="more"></a>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><blockquote>
<p>logistic回归又称logistic回归分析，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。例如，探讨引发疾病的危险因素，并根据危险因素预测疾病发生的概率等。以胃癌病情分析为例，选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群必定具有不同的体征与生活方式等。因此因变量就为是否胃癌，值为“是”或“否”，自变量就可以包括很多了，如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。然后通过logistic回归分析，可以得到自变量的权重，从而可以大致了解到底哪些因素是胃癌的危险因素。同时根据该权值可以根据危险因素预测一个人患癌症的可能性。</p>
</blockquote>
<h3 id="逻辑回归与线性回归的联系与区别"><a href="#逻辑回归与线性回归的联系与区别" class="headerlink" title="逻辑回归与线性回归的联系与区别"></a>逻辑回归与线性回归的联系与区别</h3><p>线性回归解决的是连续变量问题，那么在分类任务中可以用线性回归吗？例如判断是良性肿瘤还是恶性肿瘤，判断是垃圾邮件还是正常邮件，等等……</p>
<p>答案是也可以，但是效果不好，见下图：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/20190830143654227.png" alt></p>
<p>图显示了是否购买玩具和年龄之间的关系，可以用线性回归拟合成一条直线，将购买标注为1，不购买标注为0，拟合后取当0.5值为阈值来划分类别。<br>$$\hat y = \begin{cases}<br>1, f(x)&gt;0.5\<br>0, f(x)&lt;0.5<br>\end{cases}$$<br>可以看到，在途中，年龄的区分点约为19岁。<br>但当数据点不平衡时，很容易影响到阈值，见以下图：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/20190830143716479.png" alt></p>
<p>可以看到，0值样本的年龄段往高年龄端偏移后，真实的阈值依然是19岁左右，但拟合出来的曲线的阈值往后边偏移了。可以想想，负样本越多，年龄大的人越多，偏移越严重。<br>可是这符合实际情况吗？实际情况是60岁的老人和80岁的老人都不会购买玩具，增加几位80岁的老人，并不会影响20岁以下人群购买玩具的概率。但因为拟合曲线原本的值域为$(-\infty \  \infty)$而转换后的值域为$[0,1]$，阈值对变量偏移很敏感。</p>
<h3 id="逻辑回归的原理"><a href="#逻辑回归的原理" class="headerlink" title="逻辑回归的原理"></a>逻辑回归的原理</h3><p>因此理想的替代函数应当预测分类为0或1的概率，当为1的概率大于0.5时，判断为1，当为1的概率小于0.5时，判断为0。因概率的值域为$[0,1]$，这样的设定比线性回归合理很多。<br>常用的替代函数为Sigmoid函数，即：<br>$$<br>h(z) = \frac{1}{1+e^{-z}}<br>$$<br>其中，$z = \theta^T x$<br>我们可以看到，当z大于0时，函数大于0.5；当函数等于0时，函数等于0.5；函数小于0时，函数小于0.5。如果用函数表示目标分到某一类的概率，我们可以采用以下“单位阶跃函数”来判断数据的类别：<br>$$h(z) = \left{<br>\begin{aligned}<br>0，  z<0 \\ 0.5， z="0" 1，>0<br>\end{aligned}<br>\right.$$<br>若Z大于0，则判断为正例；若小于0，判断为反例；若等于0，可任意判别。由于Sigmoid函数单调且可导，函数在（0，1）之间程Z字型，可以很好的模拟二分类情况，因此很适合我们要解决的问题。</0></p>
<p>接下来我们来推导逻辑回归函数的优化</p>
<h3 id="逻辑回归损失函数推导及优化"><a href="#逻辑回归损失函数推导及优化" class="headerlink" title="逻辑回归损失函数推导及优化"></a>逻辑回归损失函数推导及优化</h3><p>$$<br>P(y=1|x;\theta) = h_\theta (x) \<br>P(y=0|x;\theta) = 1-h_\theta (x)<br>$$<br>可以写作一般公式，<br>$$P(y|x;\theta)= h(x)^y (1-h(x))^{(1-y)}$$<br>极大似然函数为，<br>$$<br>L(\theta) = \prod^{m}<em>{i=1}h</em>\theta (x^{(i)})^{y^{(i)}} (1-h_\theta (x^{(i)})^{(1-y^{(i)})}<br>$$<br>对数极大似然函数为，<br>$$l(\theta) = log L(\theta) = \sum^{m}<em>{i=1} y^{(i)}log h</em>\theta (x^{(i)}) + (1-y^{(i)})log (1-h_\theta (x^{(i)}))<br>$$<br>损失函数为,<br>$$<br>J(\theta) = -\frac{1}{m}l(\theta) = -\frac{1}{m}\sum^{m}<em>{i=1} y^{(i)}h</em>\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)}))<br>$$<br>损失函数表示了预测值和真实值之间的差异程度，预测值和真实值越接近，则损失函数越小。</p>
<blockquote>
<p><strong>思考题</strong>：为什么不直接用和线性回归一样的平方损失函数？</p>
<blockquote>
<p><strong>回答</strong>：如果和线性回归一样的平方损失函数，则损失函数的形式为$\sum^m_{i=1}(y^{(i)}-\frac{1}{1+e^{-\theta^T x}})^2$，此为非凸函数，求解复杂，而且很容易求得局部最优解为非全局最优解。</p>
</blockquote>
</blockquote>
<p>我们用梯度下降法求解，</p>
<p>$$\theta:=\theta-\alpha\Delta_\theta J(\theta) = \theta + \frac{\alpha}{m}\Delta_\theta l(\theta)$$<br>由于$g’<em>\theta(z) = g</em>\theta (z)(1-g_\theta(z))$</p>
<blockquote>
<p><strong>小练习</strong>：试证明，当$g(z) = \frac{1}{1+e^{-z}}$ ,$g’(z) = g(z)(1-g(z))$</p>
</blockquote>
<blockquote>
<blockquote>
<p><strong>证明</strong>：<br>$$\begin{align<em>}<br>g’(z)<br>= \frac{-1}{(1+e^{-z})^2}(-e^{-z}) \<br>= \frac{e^{-z}}{(1+e^{-z})^2} \<br>= \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} \<br>= g(z)(1-g(z))<br>\end{align</em>}$$<br>因此可以求得，<br>$$\begin{align<em>}<br>\frac{\partial J(\theta)}{\partial \theta_i}<br>= \frac{1}{m}\sum^m_{i=1} y^{(i)}\frac{1}{h_\theta (x^{(i)})} h_\theta (x^{(i)})(1-h_\theta (x^{(i)}))x^{(i)} + (1-y^{(i)})\frac{1}{1-h_\theta (x^{(i)})}h_\theta (x^{(i)})(h_\theta (x^{(i)})-1)\<br>= \frac{1}{m}\sum^m_{i=1}y^{(i)}(1-h_\theta (x^{(i)}))x^{(i)}+(y^{(i)}-1)h_\theta (x^{(i)})x^{(i)} \<br>= \frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)}<br>\end{align</em>}$$<br>可以看到，形式和线性回归很相似，这是因为他们都属于广义线性模型（GLM，可参考吴恩达-CS229 第四课）。</p>
</blockquote>
</blockquote>
<h4 id="逻辑回归的分布式实现"><a href="#逻辑回归的分布式实现" class="headerlink" title="逻辑回归的分布式实现"></a>逻辑回归的分布式实现</h4><p>由于单机处理能力的限制，在对大规模样本训练时，往往需要将求解过程并行化。我们知道在求解过程中，行和列都存在批向量处理的情况，我们可以按行并行和按列并行。</p>
<h5 id="按行并行"><a href="#按行并行" class="headerlink" title="按行并行"></a>按行并行</h5><p>看$\frac{\partial J(\theta)}{\partial \theta_i} =\frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)}$，相当于将上个迭代步骤结果，将每个样本进行运算后求和，那么只需将样本拆分到不同机器上运算，最后在求和即可。</p>
<h5 id="按列并行"><a href="#按列并行" class="headerlink" title="按列并行"></a>按列并行</h5><p>$\frac{\partial J(\theta)}{\partial \theta_i} =\frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)}$是对参数$\theta_i$进行的运算，那么只需将特征分配到不同的机器上，分别计算梯度后，再归并到完整的特征向量进行梯度更新即可。</p>
<h3 id="正则化与模型评估指标"><a href="#正则化与模型评估指标" class="headerlink" title="正则化与模型评估指标"></a>正则化与模型评估指标</h3><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>我们可以在损失函数后面，加上正则化函数，即$\theta$的惩罚项，来抑制过拟合问题。</p>
<h5 id="L1正则"><a href="#L1正则" class="headerlink" title="L1正则"></a>L1正则</h5><p>$$<br>J(\theta) =\frac{1}{m}\sum^{m}<em>{i=1} y^{(i)}h</em>\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)})) + \frac{\lambda}{m<br>}\sum^m_{i=1}|\theta_i|<br>$$<br>$$<br>\Delta_{\theta_i} l(\theta) = \frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)} + \frac{\lambda}{m}sgn(\theta_i)<br>$$<br>梯度下降法的迭代函数变为,<br>$$\theta:=\theta-K’(\theta)-\frac{\lambda}{m}sgn(\theta)$$<br>$K(\theta)$为原来的损失函数，由于最后一项的符号由$\theta$决定，可以看到，当$\theta$大于零时，更新后的$\theta$变小；当$\theta$小于零时，更新后的$\theta$变大。因此，L1正则化调整后的结果会更加稀疏（结果向量中有更多的0值）。（见图示，相当于在等高线上找令菱形最小的点。）</p>
<h5 id="L2正则"><a href="#L2正则" class="headerlink" title="L2正则"></a>L2正则</h5><p>$$<br>J(\theta) =\frac{1}{m}\sum^{m}<em>{i=1} y^{(i)}h</em>\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)})) + \frac{\lambda}{2m}\sum^m_{i=1}\theta_i^2<br>$$<br>$$<br>\Delta_{\theta_i} l(\theta) = \frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)} + \frac{\lambda}{m}\theta_i<br>$$<br>梯度下降法的迭代函数变为,<br>$$\theta:=\theta-K’(\theta)-\frac{2\lambda}{m}\theta$$<br>$K(\theta)$为原来的损失函数，最有一项的$\lambda$决定了对参数的惩罚力度，惩罚力度越大，最后的结果向量的参数普遍较小且分散，避免了个别参数对整个函数起较大的影响。（见图示，相当于在等高线上找令圆形最小的点）</p>
<h5 id="逻辑回归的评价指标"><a href="#逻辑回归的评价指标" class="headerlink" title="逻辑回归的评价指标"></a>逻辑回归的评价指标</h5><p>由于逻辑回归模型属于分类模型，不能用线性回归的评价指标。 二元分类的评价指标基本都适用于逻辑回归。 观察以下混淆矩阵，<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/4b1eek0aoz.jpeg" alt></p>
<p>我们可以用查准率和查全率来评价预测结果：</p>
<ul>
<li><strong>查准率</strong> $P =\frac{TP}{TP+FP}$</li>
<li><strong>查全率</strong> $R =\frac{TP}{TP+FN}$</li>
</ul>
<p>我们可以用P-R曲线表示查准率和查全率之间的关系：<br>查准率和查全率经常相互矛盾，一般查准率高时查全率低，查全率高时查准率低。我们经常针对具体的应用场景决定更看重哪一个指标。</p>
<blockquote>
<p><strong>小练习</strong>：试举例说明，在什么场景下，更看重查准率；在什么情况下，更看重查全率，并说明原因。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/4b1eek0aoz.jpeg" alt></p>
</blockquote>
<p>P-R曲线越靠外，则预测效果越好，如果两条P-R曲线相交，则难说孰优孰率，可以计算曲线包住的面积。<br>我们可以用$F_\beta$表达对查准率/查全率的不同偏好，定义为：<br>$$F_\beta = \frac{(1+\beta^2) \cdot P \cdot R}{(\beta^2 \cdot P)+R}$$<br>$\beta$大于1时，查全率有更大影响；$\beta$小于1时，查准率有更大影响；$\beta$等于0时，即标准的F1度量。<br>但是，当正负样本分布发生变化时，P-R曲线会受到较大影响。试想负样本的数量扩大10倍，FP和TN都会成倍增加，会影响到查准率和查全率。这时候我们可以用ROC曲线来评价模型。</p>
<p>定义：</p>
<ul>
<li>$TPR = \frac{TP}{TP+FN}$</li>
<li>$FPR = \frac{FP}{TN+FP}$</li>
</ul>
<p>绘图的过程可以将样本的预测概率排序，将分类阈值从最小向最大移动，则每个阈值会得到一组TPR和FPR的位置点，将所有的点连结，则绘制成ROC曲线，同时，计算曲线下的面积，即AUC值。AUC值越大，则说明预测的效果越好。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/11525720-dd2545eaaaa7c2ba.png" alt></p>
<h5 id="AUC值的计算"><a href="#AUC值的计算" class="headerlink" title="AUC值的计算"></a>AUC值的计算</h5><p><strong>计算方法一</strong>：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/roc_plot.gif" alt></p>
<p>上图很形象地描述了RUC的描绘过程，那么，计算AUC值也很直观了，每一组点的坐标,可以写成$(x_i,y_i)$，那么，<br>$$AUC = \frac{1}{2}\sum^{m-1}<em>{i=1}(x</em>{i+1}-x_i) \cdot(y_i + y_{i+1})$$<br>上式乘以二分之一的原因是，阈值变更时，上图正例和反例单个出现的，但当数据量增加，每个阈值同时存在多个正例和反例时，上升的轨迹可能是斜线，因此要计算梯形面积。</p>
<p><strong>计算方法二</strong>：</p>
<p>任意给一个正样本和负样本，如果正样本的预测概率大于负样本，意味着以此正样本的预测概率为阈值，预测是成功的，则增加1分；如果两者相等，则增加0.5分。<br>$$<br>令 AUC_{ij} = \left{<br>\begin{aligned}<br>    1,   if\ pos_score_i &gt; neg_score_j \<br>    0.5,   if\ pos_score_i = neg_score_j \<br>    0,   if\ pos_score_i &lt; neg_score_j<br>\end{aligned}<br>\right.$$<br>单个正样本对所有负样本的得分占全部负样本的比例为：$AUC_i = \frac{1}{m^-}\sum<br>^{m^-}<em>{j=1}AUC</em>{ij}$</p>
<p>如果一个正样本在ROC曲线上的坐标为$(x,y)$，x即排序在其之前的负样本所占的比例，即假正例率。那么，整条曲线的ROC值即<br>$$<br>AUC = \frac{1}{m^+m^-}AUC_{ij} = \frac{1}{m^+m^-}\sum_{x^+\in D^+}\sum_{x^- \in D^-}(\mathbb{I}(f(x^+)&lt;f(x^-))  +\frac{1}{2}\mathbb{I}(f(x^+)=f(x^-))<br>$$<br>方 法一是将每个预测概率预测正确的比例做积分，方法二是将每个正样本的预测概率大于负样本预测概率的比例做积分。<br>我们可以把方法1理解为，将ROC曲线看作$y = f(x)$，求定积分$\int_0^1f(x)dx$，方法2理解为，将RUC曲线看作是$x = g(y)$，求$\int_0^1g(y)dy$。</p>
<h3 id="逻辑回归的优缺点"><a href="#逻辑回归的优缺点" class="headerlink" title="逻辑回归的优缺点"></a>逻辑回归的优缺点</h3><ul>
<li><strong>优点</strong>：从上面介绍已经可以额看到，逻辑回归的思想简洁，可以很好的解决二问题。</li>
<li><strong>缺点</strong>：观察下图<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/2019083014370448.png" alt></li>
</ul>
<ol>
<li>因为预测结果呈Z字型（或反Z字型），因此当数据集中在中间区域时，对概率的变化会很敏感，可能使得预测结果缺乏区分度。</li>
<li>由于逻辑回归依然是线性划分，对于非线性的数据集适应性弱。</li>
<li>当特征空间很大，性能欠佳。</li>
<li>只能处理两分类问题。</li>
</ol>
<h3 id="样本不均衡问题"><a href="#样本不均衡问题" class="headerlink" title="样本不均衡问题"></a>样本不均衡问题</h3><p>每200封邮件里有1封垃圾邮件，当碰到一封新邮件时，我们只需将其预测为正常邮件，可以达到99.5%的准确率。但是这样的学习器毫无价值，因为他无法判断出任意一封垃圾邮件。<br>我们预测时，实际上是用预测出的概率值与一个阈值进行比较，例如当y &gt; 0.5时，判断为正例。$\frac{y}{1-y}$表示了正例可能性和反例可能性的比值。阈值为0.5时，即$\frac{y}{1-y}&gt;1$时，预测为正例。<br>如果令$m_+$为样本正例数，$m_-$为样本负例数，随机的观测几率为$\frac{m_+}{m_-}$。只要分类器的预测概率高于观测几率，应判定为正例，即<br>$$<br>\frac{y}{1-y}&gt;\frac{m_+}{m_-}，预测为正例<br>$$<br>这时候，需要对预测值进行调整，使得$\frac {y’}{1-y’}=\frac{y}{1-y} \cdot\frac{m_+}{m_-}$，那么，0.5的阈值依然是合理的分类决策。<br>这就是类别不平衡的一个基本策略——“再缩放”。<br>“再缩放”的三类做法：</p>
<ul>
<li><strong>欠采样</strong>：去除一些反例使得正反例数目接近。</li>
<li><strong>过采样</strong>：增加一些正例使得正反例数目接近。</li>
<li><strong>阈值移动</strong>：基于原始集学习，当在预测是，将决策阈值改为符合样本正负样本比例的值。<br>可以想到，过采样因为增加了数据，时间开销大于欠采样法。但欠采样法由于随机丢弃反例，可能丢失一些重要信息。这时候可以将反例划分成若干个集合分别学习，从全局来看并非丢失重要信息。</li>
</ul>
<h3 id="sklearn参数-1"><a href="#sklearn参数-1" class="headerlink" title="sklearn参数 ^1"></a>sklearn参数 <a href="https://blog.csdn.net/qq_38683692/article/details/82533460" target="_blank" rel="noopener">^1</a></h3><table>
<thead>
<tr>
<th style="text-align:left">Logistics Regression参数名称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">函数调用形式</td>
<td>LogisticRegression(penalty=’l2’,dual=False,tol=1e-4,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver=’liblinear’,max_iter=100,multi_class=’ovr’,verbose=0,warm_start=False, n_jobs=1)</td>
</tr>
<tr>
<td style="text-align:left">penalty</td>
<td>字符串型，’l1’ or ‘l2’，默认：’l2’；正则化类型。</td>
</tr>
<tr>
<td style="text-align:left">dual</td>
<td>布尔型，默认：False。当样本数&gt;特征数时，令dual=False；用于liblinear解决器中L2正则化。</td>
</tr>
<tr>
<td style="text-align:left">tol</td>
<td>浮点型，默认：1e-4；迭代终止判断的误差范围。</td>
</tr>
<tr>
<td style="text-align:left">C</td>
<td>浮点型，默认：1.0；其值等于正则化强度的倒数，为正的浮点数。数值越小表示正则化越强。</td>
</tr>
<tr>
<td style="text-align:left">fit_intercept</td>
<td>布尔型，默认：True；指定是否应该向决策函数添加常量(即偏差或截距)。</td>
</tr>
<tr>
<td style="text-align:left">intercept_scaling</td>
<td>浮点型，默认为1；仅仅当solver是”liblinear”时有用。</td>
</tr>
<tr>
<td style="text-align:left">class_weight</td>
<td>默认为None；与”{class_label: weight}”形式中的类相关联的权重。如果不给，则所有的类的权重都应该是1。</td>
</tr>
<tr>
<td style="text-align:left">random_state</td>
<td>整型，默认None；当”solver”==”sag”或”liblinear”时使用。在变换数据时使用的伪随机数生成器的种子。如果是整数, random_state为随机数生成器使用的种子;若为RandomState实例，则random_state为随机数生成器;如果没有，随机数生成器就是’ np.random ‘使用的RandomState实例。</td>
</tr>
<tr>
<td style="text-align:left">solver</td>
<td>{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认: ‘liblinear’；用于优化问题的算法。<br>对于小数据集来说，”liblinear”是个不错的选择，而”sag”和’saga’对于大型数据集会更快。<br>对于多类问题，只有’newton-cg’， ‘sag’， ‘saga’和’lbfgs’可以处理多项损失;”liblinear”仅限于”one-versus-rest”分类。</td>
</tr>
<tr>
<td style="text-align:left">max_iter</td>
<td>最大迭代次数，整型，默认是100；</td>
</tr>
<tr>
<td style="text-align:left">multi_class</td>
<td>字符串型，{ovr’， ‘multinomial’}，默认:’ovr’；如果选择的选项是”ovr”，那么一个二进制问题适合于每个标签，否则损失最小化就是整个概率分布的多项式损失。对liblinear solver无效。</td>
</tr>
<tr>
<td style="text-align:left">verbose</td>
<td>整型，默认是0；对于liblinear和lbfgs solver，verbose可以设为任意正数。</td>
</tr>
<tr>
<td style="text-align:left">warm_start</td>
<td>布尔型，默认为False；当设置为True时，重用前一个调用的解决方案以适合初始化。否则，只擦除前一个解决方案。对liblinear解码器无效。</td>
</tr>
<tr>
<td style="text-align:left">n_jobs</td>
<td>整型，默认是1；如果multi_class=’ovr’ ，则为在类上并行时使用的CPU核数。无论是否指定了multi_class，当将’ solver ‘ ‘设置为’liblinear’时，将忽略此参数。如果给定值为-1，则使用所有核。</td>
</tr>
</tbody>
</table>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>1.先尝试调用sklearn的线性回归模型训练数据，尝试以下代码，画图查看分类的结果<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">df_X = pd.read_csv(<span class="string">'./logistic_x.txt'</span>, sep=<span class="string">'\ +'</span>,header=<span class="literal">None</span>, engine=<span class="string">'python'</span>) <span class="comment">#读取X值</span></span><br><span class="line">ys = pd.read_csv(<span class="string">'./logistic_y.txt'</span>, sep=<span class="string">'\ +'</span>,header=<span class="literal">None</span>, engine=<span class="string">'python'</span>) <span class="comment">#读取y值</span></span><br><span class="line">ys = ys.astype(int)</span><br><span class="line">df_X[<span class="string">'label'</span>] = ys[<span class="number">0</span>].values <span class="comment">#将X按照y值的结果一一打标签</span></span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line"><span class="comment">#在二维图中描绘X点所处位置，直观查看数据点的分布情况</span></span><br><span class="line">df_X.query(<span class="string">'label == 0'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'blue'</span>)</span><br><span class="line">df_X.query(<span class="string">'label == 1'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#提取用于学习的数据</span></span><br><span class="line">Xs = df_X[[<span class="number">0</span>, <span class="number">1</span>]].values</span><br><span class="line">Xs = np.hstack([np.ones((Xs.shape[<span class="number">0</span>], <span class="number">1</span>)), Xs]) </span><br><span class="line">ys = df_X[<span class="string">'label'</span>].values</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression(fit_intercept=<span class="literal">False</span>) <span class="comment">#因为前面已经将截距项的值合并到变量中，此处参数设置不需要截距项</span></span><br><span class="line">lr.fit(Xs, ys) <span class="comment">#拟合</span></span><br><span class="line">score = lr.score(Xs, ys) <span class="comment">#结果评价</span></span><br><span class="line">print(<span class="string">"Coefficient: %s"</span> % lr.coef_) <span class="comment"># Coefficient: [[-1.70090714  0.55446484  1.07222372]]</span></span><br><span class="line">print(<span class="string">"Score: %s"</span> % score) <span class="comment"># Score: 0.898989898989899</span></span><br><span class="line"></span><br><span class="line">ax = plt.axes()</span><br><span class="line"></span><br><span class="line">df_X.query(<span class="string">'label == 0'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'blue'</span>)</span><br><span class="line">df_X.query(<span class="string">'label == 1'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line">_xs = np.array([np.min(Xs[:,<span class="number">1</span>]), np.max(Xs[:,<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据以二维图形式描点，并用学习得出的参数结果作为阈值，划分数据区域</span></span><br><span class="line">_ys = (lr.coef_[<span class="number">0</span>][<span class="number">0</span>] + lr.coef_[<span class="number">0</span>][<span class="number">1</span>] * _xs) / (- lr.coef_[<span class="number">0</span>][<span class="number">2</span>])</span><br><span class="line">plt.plot(_xs, _ys, lw=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/QQ20200114124640.png" alt></p>
<p>2.用梯度下降法将相同的数据分类，画图和sklearn的结果相比较<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LGR_GD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = <span class="literal">None</span> </span><br><span class="line">        self.n_iters = <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,X,y,alpha=<span class="number">0.03</span>,loss = <span class="number">1e-10</span>)</span>:</span> <span class="comment"># 设定步长为0.002，判断是否收敛的条件为1e-10</span></span><br><span class="line">        y = y.reshape(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#重塑y值的维度以便矩阵运算</span></span><br><span class="line">        [m,d] = np.shape(X) <span class="comment">#自变量的维度</span></span><br><span class="line">        self.w = np.zeros((<span class="number">1</span>,d)) <span class="comment">#将参数的初始值定为0</span></span><br><span class="line">        tol = <span class="number">1e5</span></span><br><span class="line">        self.n_iters = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> tol &gt; loss: <span class="comment">#设置收敛条件</span></span><br><span class="line">            zs = X.dot(self.w.T) </span><br><span class="line">            h_f = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-zs)) </span><br><span class="line">            theta = self.w + alpha *np.mean(X*(y - h_f),axis=<span class="number">0</span>) <span class="comment">#计算迭代的参数值</span></span><br><span class="line">            tol = np.sum(np.abs(theta - self.w))</span><br><span class="line">            self.w = theta <span class="comment">#更新参数值</span></span><br><span class="line">            self.n_iters += <span class="number">1</span> <span class="comment">#更新迭代次数</span></span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 用已经拟合的参数值预测新自变量</span></span><br><span class="line">        y_pred = X.dot(self.w)</span><br><span class="line">        <span class="keyword">return</span> y_pred  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    lr_gd = LGR_GD()</span><br><span class="line">    lr_gd.fit(Xs,ys)</span><br><span class="line"></span><br><span class="line">    ax = plt.axes()</span><br><span class="line"></span><br><span class="line">    df_X.query(<span class="string">'label == 0'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'blue'</span>)</span><br><span class="line">    df_X.query(<span class="string">'label == 1'</span>).plot.scatter(x=<span class="number">0</span>, y=<span class="number">1</span>, ax=ax, color=<span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line">    _xs = np.array([np.min(Xs[:,<span class="number">1</span>]), np.max(Xs[:,<span class="number">1</span>])])</span><br><span class="line">    _ys = (lr_gd.w[<span class="number">0</span>][<span class="number">0</span>] + lr_gd.w[<span class="number">0</span>][<span class="number">1</span>] * _xs) / (- lr_gd.w[<span class="number">0</span>][<span class="number">2</span>])</span><br><span class="line">    plt.plot(_xs, _ys, lw=<span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4786a2a0/QQ20200114124757.png" alt></p>
<p>3.用牛顿法实现结果，画图和sklearn的结果相比较，并比较牛顿法和梯度下降法迭代收敛的次数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LGR_NT</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.n_iters = <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,X,y,loss = <span class="number">1e-10</span>)</span>:</span> <span class="comment"># 判断是否收敛的条件为1e-10</span></span><br><span class="line">        y = y.reshape(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#重塑y值的维度以便矩阵运算</span></span><br><span class="line">        [m,d] = np.shape(X) <span class="comment">#自变量的维度</span></span><br><span class="line">        self.w = np.zeros((<span class="number">1</span>,d)) <span class="comment">#将参数的初始值定为0</span></span><br><span class="line">        tol = <span class="number">1e5</span></span><br><span class="line">        n_iters =<span class="number">0</span></span><br><span class="line">        Hessian = np.zeros((d,d))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> tol &gt; loss:</span><br><span class="line">            zs = X.dot(self.w.T)</span><br><span class="line">            h_f = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-zs))</span><br><span class="line">            grad = np.mean(X*(y - h_f),axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(d):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(d):</span><br><span class="line">                    <span class="keyword">if</span> j&gt;=i:</span><br><span class="line">                        Hessian[i][j] = np.mean(h_f*(h_f<span class="number">-1</span>)*X[:,i]*X[:,j]) <span class="comment">#更新海森矩阵中的值</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        Hessian[i][j] = Hessian[j][i] <span class="comment">#按海森矩阵的性质，对称点可直接得出结果</span></span><br><span class="line">            theta = self.w - np.linalg.inv(Hessian).dot(grad)</span><br><span class="line">            tol = np.sum(np.abs(theta - self.w))</span><br><span class="line">            self.w = theta</span><br><span class="line">            n_iters += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        self.w = theta</span><br><span class="line">        self.n_iters = n_iters</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 用已经拟合的参数值预测新自变量</span></span><br><span class="line">        y_pred = X.dot(self.w)</span><br><span class="line">        <span class="keyword">return</span> y_pred  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    lgr_nt = LGR_NT()</span><br><span class="line">    lgr_nt.fit(Xs,ys)</span><br></pre></td></tr></table></figure></p>
<p>4.比较梯度下降法和牛顿法收敛速度<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"梯度下降法结果参数：%s;梯度下降法迭代次数：%s"</span> %(lgr_gd.w,lgr_gd.n_iters)) <span class="comment"># 梯队下降法结果参数：[[-2.62051144  0.7603715   1.17194673]]; 梯度下降法迭代次数: 32590</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"牛顿法结果参数：%s;牛顿法迭代次数：%s"</span> %(lgr_nt.w,lgr_nt.n_iters)) <span class="comment"># 牛顿法结果参数：[[-2.6205116   0.76037154  1.17194674]]; 牛顿法迭代次数: 47</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>LR</tag>
        <tag>logistic regression</tag>
        <tag>初级算法梳理</tag>
      </tags>
  </entry>
  <entry>
    <title>Day4 决策树</title>
    <url>/p/2020/01/15/5f13e255/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>特征选择：信息增益（熵、联合熵、条件熵）、信息增益比、基尼系数</li>
<li>决策树生成：ID3决策树、C4.5决策树、CART决策树（CART分类树、CART回归树）</li>
<li>决策树剪枝</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="练习部分"><a href="#练习部分" class="headerlink" title="练习部分"></a>练习部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb</a></p>
<p>利用sklearn解决分类问题和回归预测。</p>
<ul>
<li>sklearn.tree.DecisionTreeClassifier</li>
<li>sklearn.tree.DecisionTreeRegressor</li>
</ul>
<a id="more"></a>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树是一种树型结构的机器学习算法,它每个节点验证数据一个属性,根据该属性进行分割数据,将数据分布到不同的分支上,直到叶子节点,叶子结点上表示该样本的label。 每一条从根节点到叶子节点的路径表示分类[回归]的规则。 下面我们先来看看sklearn中决策树怎么用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris, load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类树</span></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Classifier Score:"</span>, clf.score(X_test, y_test)) <span class="comment"># Classifier Score: 1.0</span></span><br><span class="line"></span><br><span class="line">tree.plot_tree(clf.fit(X, y)) </span><br><span class="line">plt.show() <span class="comment"># pic1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回归树</span></span><br><span class="line">X, y = load_boston(return_X_y=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">clf = tree.DecisionTreeRegressor()</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Regression Score:"</span>, clf.score(X_test, y_test)) <span class="comment"># Regression Score: 0.4830393038746458</span></span><br><span class="line"></span><br><span class="line">tree.plot_tree(clf.fit(X, y)) </span><br><span class="line">plt.show() <span class="comment"># pic2</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/pic1.png" alt="pic1"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/pic2.png" alt="pic2"></p>
<h3 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h3><p>首先先来几个概念,我们后面介绍决策树原理的时候会提到,这里可以先扫一眼,用到的时候再回来看。</p>
<h4 id="熵和信息熵"><a href="#熵和信息熵" class="headerlink" title="熵和信息熵"></a>熵和信息熵</h4><p>熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。 可以看出,熵表示的是体系的不确定性大小。 熵越大, 物理的不确定性越大。 1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。 同理, 信息熵越小,数据的稳定性越好,我们更加相信此时数据得到的结论。 换言之, 我们现在目的肯定熵越小,机器学习得到的结果越准确。</p>
<p>信息熵表示随机变量不确定性的度量,设随机标量X是一个离散随机变量，其概率分布为:<br>$$P(X=x_i)=p_i, i=1,2,…,n$$<br>则随机变量X的熵定义为:<br>$$H(X)=-\sum_{i=1}^{n}p_ilog{p_i}$$<br>熵越大，随机变量的不确定性就越大，当$p_i=\frac{1}{n}$时， 随机变量的熵最大等于logn，故$0 \leq H(P) \leq logn$。</p>
<h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><p>条件熵就是在给定X的条件的情况下，随机标量Y的条件，记作$H(Y|X)$，可以结合贝叶斯公式进行理解，定义如下<br>$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$<br>这里$p_i=P(X=x_i),i=1,2,…,n$。 一般在基于数据的估计中，我们使用的基于极大似然估计出来的经验熵和经验条件熵。</p>
<h4 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h4><p>联合熵是相对两个及其以上的变量而言的, 两个变量X和Y的联合信息熵为:<br>$$ H(X,Y)=-\sum_x \sum_y P(x,y)log_2[P(x,y)] $$<br>其中: x和y是X和Y的特定值, 相应地, 是这些值一起出现的联合概率, 若为0, 则定义为0。</p>
<p>对于两个以上的变量$X_1,…,X_n$,一般形式位:<br>$$H(X_1,…,X_n)=-\sum_{x_1}\cdot \cdot \cdot\sum_{x_n}P(x1,…,x_n)log_2[P(x_1,…,x_n)]$$</p>
<p>性质:</p>
<ul>
<li>大于每个独立的熵<br>$$H(X,Y) \geq max(H(X),H(Y))$$<br>$$H(X_1,…,X_n) \geq max(H(X_1),…,H(X_n))$$</li>
<li>小于独立熵的和<br>$$H(X_1,…,X_n) \leq sum(H(X_1),…,H(X_n))$$</li>
<li>和条件熵的关系<br>$$H(Y|X)=H(X,Y)-H(X)$$</li>
<li>和互信息的关系<br>$$I(Y;X)=H(X)+H(Y)-H(X,Y)=H(Y)-(H(X,Y)-H(X))$$</li>
</ul>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>信息增益又叫互信息,它表示的是在的得知特征X的信息后,使得类Y的信息的不确定性(熵)减少的程度。<br>$$g(Y,X)=H(Y)-H(Y|X)$$</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>基尼指数又称基尼系数或者基尼不纯度,基尼系数是指国际上通用的、用以衡量一个国家或地区居民收入差距的常用指标。 在信息学中,例如分类问题, 假设有K个类,样本点属于第k类的概率是$p_k$,则该概率分布的基尼指数定义为:<br>$$Gini(p)=\sum_k^Kp_k(1-p_k)=1-\sum_k^Kp_k^2$$</p>
<h3 id="决策树解释"><a href="#决策树解释" class="headerlink" title="决策树解释"></a>决策树解释</h3><p>决策树是什么东西？就是我们平常所说的if-then条件，我们把它组合成树的结构。 决策树中有两种结点，叶子结点和非叶子结点。 其中非叶节点代表的条件，叶子结点表示的实例所属的类别。</p>
<p>我们如何生成这个决策树呢，最主要的一点就是选择那个特征作为当前树的分割结点，这就叫做特征选择，有了特征选择就有了决策树的生成，最后我们还有进行决策树剪枝(后面会提到为什么剪枝)。</p>
<p>看个统计学习方法上的例子:</p>
<p>现在我们有下面一张表的数据，想生成一个决策树模型，预测某个人是否符合贷款条件。<br>现在假如我们通过”某种方法”构造了一颗下面的决策树。 从下图可以看到特征对应的是非叶子结点，如果这个被分到这个叶节点，就预测为这个叶节点的类别。 从图中我们可以知道以下两点:</p>
<ul>
<li>每一个叶子节点都对应一条从根节点到叶节点的规则，这表示决策树是if-then规则的集合</li>
<li>如果一个实例到达了某个叶节点，一定表示它满足了从根节点到该叶子节点的所有条件，而后才得到类别，这不就是先满足条件再得到概率嘛，我们一般叫做条件概率分布。</li>
</ul>
<blockquote>
<p><strong>问题</strong>:为什么我们要选择是否有房子作为第一个构造特征呢？我们构造学习模型，会遵守经验风险最小化或者似然函数极大规则，选择损失函数，我们如何根据风险最小化，选择特征呢？</p>
</blockquote>
<h3 id="ID3-amp-C4-5"><a href="#ID3-amp-C4-5" class="headerlink" title="ID3&amp;C4.5"></a>ID3&amp;C4.5</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>给定训练数据集</p>
<p>$$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$$<br>其中，$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$特征向量，n是特征的个数，$y_i \in {1,2,…,K}$表示类别. N是样本个数. 基于这个数据生成决策树模型.</p>
<h4 id="决策树-1"><a href="#决策树-1" class="headerlink" title="决策树"></a>决策树</h4><p>常见的决策树模型有以下三种(CART决策树既可以做分类也可以做回归):</p>
<ol>
<li><strong>ID3</strong>: 使用信息增益准则选择特征, 相当于用极大似然法进行概率模型选择。 </li>
<li><strong>C4.5</strong>: 和ID3算法相似, 只是用信息增益比选择特征。 </li>
<li><strong>CART</strong>: 递归构建二叉决策树, 回归树:使用平方误差; 分类树:使用基尼指数。</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left">model</th>
<th style="text-align:left">feature select</th>
<th style="text-align:center">树的类型</th>
<th style="text-align:left">计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ID3</td>
<td style="text-align:left">{分类:信息增益}</td>
<td style="text-align:center">多叉树</td>
<td style="text-align:left">$g(D,A)=H(D)-H(D)-H(D\mid\mid A)$</td>
</tr>
<tr>
<td style="text-align:left">C4.5</td>
<td style="text-align:left">{分类:信息增益比}</td>
<td style="text-align:center">多叉树</td>
<td style="text-align:left">$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$</td>
</tr>
<tr>
<td style="text-align:left">CART</td>
<td style="text-align:left">{回归:平方误差;分类:基尼指数}</td>
<td style="text-align:center">二叉树</td>
<td style="text-align:left">$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$</td>
</tr>
</tbody>
</table>
<p>其中, $H_A(D)=H(D\mid A)$。从表格中，我们总结(ID3,C4.5)决策树算法伪代码:</p>
<blockquote>
<p><strong>输入</strong>：数据集D，特征集合A，阈值e<br><strong>输出</strong>：决策树T<br>1：如果D中所有实例输出同一类$C_k$, 则T作为单节点树，并将类$C_k$作为该节点的类标记，返回T；<br>2: 若$A=\varnothing$,则T为单节点树，将D中实例数最多的类$C_k$作为该节点的类标记，返回T；<br>3: 否则，根据<strong>信息增益</strong>(ID3)或者<strong>信息增益比</strong>(C4.5)计算特征A对D的值，选择当前最优的特征$A_g$；<br>4: 如果$A_g$的信息增益小于阈值e，则置T为单节点数，并将$D$中实例最多的类$C_k$作为当前的类标记，返回T；<br>5: 否则，根据$A_g$中的每一个不同的$a_i$,根据$A_g=a_i$将$D$分成若干个非空子集$D_i$，将$D_i$中样本数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$，返回$T$<br>6：对于第i个子节点，以$D_i$为数据集，以$A-{A_g}$为特征集，递归(重复1-5)构造决策树$T_i$,返回$T_i$。<br>7: 对决策树模型T进行剪枝。</p>
</blockquote>
<h4 id="过拟合和剪枝"><a href="#过拟合和剪枝" class="headerlink" title="过拟合和剪枝"></a>过拟合和剪枝</h4><p>决策树建立的过程中,只考虑经验损失最小化,没有考虑结构损失。 因此可能在训练集上表现良好,但是会出现过拟合问题。(我们构造决策树的时候，是完全的在训练数据上得到最优的模型。 这就是过拟合问题，训练误差很小，但是验证集上就不怎么好用。) 为了解决过拟合,我们从模型损失进行考虑:<br>$$模型损失=经验风险最小化+正则项=结构风险最小化$$</p>
<p>思路很简单,给损失函数加上正则项再进行优化。 正则项表示树节点的个数,因此有如下公式:<br>$$C_{\alpha}(T)=C(T)+\alpha|T|$$</p>
<p>进一步详细定义,解决问题:<br>重新定义损失函数，树的叶子节点个数|T|,t是树T的叶节点，该叶节点有$N_t$个样本，其中k类的样本点有$N_{tk}$个，k=1,2，…,K, $H_t(T)$是叶子节点t经验熵，$\alpha \leq 0$是参数,平衡经验损失和正则项，得到计算公式如下:<br>$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$<br>其中，经验熵为:<br>$$H_t(T)=-\sum_{k}\frac{N_{tk}}{H_t}log\frac{N_{tk}}{H_t}$$<br>这是有:<br>$$C_{\alpha}=C(T)+\alpha|T|$$<br>决策树剪枝优化过程考虑了在训练数据上的经验风险最小化和减小模型复杂度两个方向。 因为加了正则项，所有我们基于贪心的思想进行剪枝，因为当剪掉一个树节点，虽然经验风险增加了，但是模型复杂度降低了，我们基于两个方面的平衡进行剪枝，如果剪掉之后，总的风险变小，就进行剪枝。<br>算法:</p>
<blockquote>
<p><strong>输入</strong>: 算法产生的整个决策树，参数$\alpha$<br><strong>输出</strong>：修剪之后的树$T_{\alpha}$<br>1：计算每个节点的经验熵<br>2：递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果:$$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$$表示，剪掉之后，损失减小，就进行剪枝。<br>3：重复2，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。<br>4：动态规划剪枝。</p>
</blockquote>
<p>可以看出来上述算法是一个递归问题,存在很多重复项计算,这里我们使用dfs+备忘录进行加速计算,这种方法和动态规划类似。</p>
<p>算法:</p>
<blockquote>
<p><strong>输入</strong>: 算法产生的整个决策树，参数$\alpha$<br><strong>输出</strong>：修剪之后的树$T_{\alpha}$<br>1：dp[所有树的节点] = {0}; 保留所有几点的信息熵<br>2：计算每个cur_node节点的经验熵, {if dp[cur_node] 直接返回, 否则, 执行2}<br>3：递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果:$$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$$表示，剪掉之后，损失减小，就进行剪枝。<br>$$dp[cur_node] = C_{\alpha}(T_A)$$<br>4：重复2，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。</p>
</blockquote>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>分类与回归树(classification and regression tree, CART)与上述决策树的不同，</p>
<ul>
<li>既可以做分类又可以做回归。</li>
<li>是二叉树，内部节点特征取值，只有yes和no两个选项。同样地，先进行决策树构造，在基于验证集上进行CART决策树的剪枝，既然是回归和分类树，我们就分别介绍回归和分类两种情况。</li>
<li>分类: gini指数。</li>
<li>回归: 平方误差 定义数据格式:</li>
</ul>
<p>$$D={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$$<br>其中，$x_i$是向量，当回归问题时，$y_i$是连续变量; 分类问题时，$y_i$是离散变量。</p>
<h4 id="回归树-Regerssion-Tree"><a href="#回归树-Regerssion-Tree" class="headerlink" title="回归树(Regerssion Tree)"></a>回归树(Regerssion Tree)</h4><p>算法:<br>在训练数据上，根据某一个特征将每个区域划分为两个子区域并决定每个子区域的输出值，递归构建二叉树。</p>
<p>1.选择最优切分变量j和切分点s，求解<br>$$min_{j,s}[min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]$$<br>遍历变量j，对固定的切分变量j扫描所有的s，找到使得上式最小的对(j,s)。<br>2.使用选定的(j,s)划分区域并决定相应的输出值:<br>$$R_1(j,s)={x|x^{(j)} \leq s }, R_2(j,s)={x|x^{(j)}&gt; s },$$<br>$$c_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i, x \in R_m, m=1,2$$<br>3.继续对两个子区域调用1和2，知道满足条件<br>4.将输入空间划分为M个区域$R_1,R_2,…,R_m$,生成决策树:<br>$$f(x)=\sum_{m=1}^{M}c_mI(x \in R_m)$$</p>
<h4 id="分类树-classification-tree"><a href="#分类树-classification-tree" class="headerlink" title="分类树(classification tree)"></a>分类树(classification tree)</h4><p>基尼指数计算公式如下:<br>$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$<br>基于数据D，得到:<br>$$Gini(D)=1-\sum_{k=1}^{K}p_k^2$$<br>其中,$C_k$是D中所属第k类的样本子集，K是类的个数。<br>如果样本集合D根据特征A是否取某一可能取值a被被划分成$D_1$和$D_2$两个部分。<br>$$D_1={(x,y) \in D | A(x)=a }, D_2= D-D_1$$<br>在特征A的条件下，集合D的基尼指数定义为:<br>$$Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)$$<br>基尼指数和熵一样，同样表示集合D的不确定性，基尼指数(Gini(D,A))表示根据调教A=a后的集合D的不确定性，基尼指数越大，表示数据D的不确定性越大。</p>
<p>算法:</p>
<blockquote>
<p><strong>输入</strong>:训练数据D，停止计算的条件<br><strong>输出</strong>:CART决策树<br>1：计算所有特征A的每一个值a对应的条件基尼指数的值，选择最优的划分得到$D_1$和$D_2$。<br>2：递归对两个数据集$D_1$和$D_2$继续调用1，知道满足条件。<br>3：生成CART树的分类树。<br>4：预测的时候，根据决策树，x落到的叶子节点对应的类别表示这个预测x的类别。</p>
</blockquote>
<h4 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h4><p>从上面两个算法的不同可以看出，只是在label的设置和决策节点选择的方式不同，整个架构依然是决策树的常用的架构。 而且上面的树的构建过程，都是基于训练数据的经验风险最小化，没有使用带有正则项的结构风险最小化，这样的模型容易发生过拟合，为了不让模型过拟合，我们需要进行模型的剪枝。</p>
<p><strong>CART树的剪枝有很多难点和有意思的地方让我们开慢慢解开这层面纱</strong><br>CART剪枝算法有两步组成:</p>
<ol>
<li>从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列${T_0,T_1,…,T_n}$。</li>
<li>通过交叉验证的方法在独立的验证数据集上堆子序列进行测试，得到最优子树</li>
</ol>
<p><strong>损失函数</strong>:<br>$$C_{\alpha}(T)=C(T)+\alpha|T|$$<br>其中，T为任意子树，$C(T)$是在训练数据上的预测误差，|T|是树的叶子节点的个数,$\alpha \geq 0$是参数，$C_{\alpha}(T)$是参数$\alpha$是的子树T的整体的损失，参数$\alpha$是平衡训练数据拟合程度和模型复杂度的权重。<br>对于固定的$\alpha$,一定存在使损失函数$C_{\alpha}(T)$最小的子树，将其记作$T_{\alpha}$。 我们可以理解为每一个$\alpha$都对应一个最有子树和最小损失。</p>
<p>而且已经得到证明，可以用递归的方法对树进行剪枝。 将$\alpha$从小增大，$0=\alpha_0&lt;\alpha_1&lt;…&lt;\alpha_n&lt;+\infty$,产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,…,n$；剪枝得到的子树序列对应着区间$\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,…,n$的最有子树序列${T_0,T_1,…,T_n}$。</p>
<p>我们给出算法:</p>
<blockquote>
<p><strong>输入</strong>: CART算法生成的决策树T0.<br><strong>输出</strong>: 最优的决策树T{\alpha}<br>1：设k=0, T=$T_0$。<br>2：设 $\alpha=+\infty$。<br>3：自下而上地对各个内部节点t计算$C(T_t),|T_t|$以及$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$$$$\alpha=min(\alpha,g(t))$$这里，$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶子节点个数。<br>4：自上而下地访问内部节点t，如果有$g(t)=\alpha$,进行剪枝，并堆叶节点t以多数表决方法决定其类(分类，回归使用平均值)，得到树T。<br>5：设$k=k+1,\alpha=\alpha,T_k=T$。<br>6：如果T不是由根节点单独构成的树，则回到步骤4。<br>7：采用交叉验证法在子树序列${T_0,T_1,…,T_n}$中选取最优子树$T_{\alpha}$。</p>
</blockquote>
<p>接下面，我们不去看算法，来看书中给的一段文字的截图，这里截图是因为我要画图，进行比较解释，图中自由理论(哈哈):<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/cart.png" alt></p>
<p>看懂这个图之后，再看算法一气呵成，因为我们假设每一次得到的树都有可能是最优的，所以不能直接去最后一个树，要使用交叉验证选择最有的决策树结构。</p>
<h3 id="问题精选"><a href="#问题精选" class="headerlink" title="问题精选"></a>问题精选</h3><blockquote>
<p>决策树和条件概率分布的关系？</p>
<blockquote>
<p>决策树可以表示成给定条件下类的条件概率分布。 决策树中的每一条路径都对应是划分的一个条件概率分布。 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么使用信息增益，越大越能得到好的模型？</p>
<blockquote>
<p>上面提到过，信息熵表示数据的混乱的程度，信息增益是信息熵和条件信息熵的差值，表示的熵减少的程度，信息增益越大，代表根据我们的决策树规则得到的数据越趋向于规整，这就是我们划分类别的目的。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么从信息增益变到信息增益比，目的何在？</p>
<blockquote>
<p>信息增益根据特征之后的条件信息熵，这样的话偏向于特征取值较多的特征的问题，因此使用信息增益比对这个问题进行校正。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么要进行剪枝？</p>
<blockquote>
<p>在构造决策树的过程中，我们的两个停止条件是，子集只有一个类别和没有可以选择的特征，这是我们全部利用了数据中的所有可以使用的信息，但是我们知道数据是可能有误差了，而且数据不足，我们需要得到更鲁棒的模型，剪枝的意义就是是的深度减小，这样的就相当于减少规则的情况下，决策树的性能反而不差，使其更加鲁棒。</p>
</blockquote>
</blockquote>
<blockquote>
<p>ID3和C4.5算法可以处理实数特征吗？如果可以应该怎么处理？如果不可以请给出理由？</p>
<blockquote>
<p>ID3和C4.5使用划分节点的方法分别是信息增益和信息增益比，从这个公式中我们可以看到 这是处理类别特征的方法，实数特征能够计算信息增益吗？我们可以定义X是实数特征的信息增益是，<br>$$G(D|X:t)=H(D)-H(D|X:t)$$<br>其中<br>$$H(D|X:t)=H(D|x \leq t)p(x \leq t)+H(D|x&gt;t)p(x&gt;t)$$<br>$$G(D|X)=max_t=G(D|X:t)$$<br>对于每一个实数可以使用这种方式进行分割。 除此之外,我们还可以使用特征的分桶，将实数特征映射到有限个桶中，可以直接使用ID3和C4.5算法。</p>
</blockquote>
</blockquote>
<blockquote>
<p>基尼系数存在的问题?</p>
<blockquote>
<p>基尼指数偏向于多值属性;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。</p>
</blockquote>
</blockquote>
<h3 id="sklearn-决策树参数"><a href="#sklearn-决策树参数" class="headerlink" title="sklearn 决策树参数"></a>sklearn 决策树参数</h3><p>我们掌握理论之后，就去看看sklearn中决策树的实现。<br>DecisionTreeClassifier: sklearn中多分类决策树的接口。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Paramters</th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>criterion</strong> : str, 可选参数(default=”gini”)</td>
<td style="text-align:left">这个参数表示使用什么度量划分的质量。 gini: 表示使用基尼指数。<br>entropy: 表示使用的是信息增益。</td>
</tr>
<tr>
<td style="text-align:left"><strong>splitter</strong> : str, optional(default=”best”)</td>
<td style="text-align:left">选择分割节点的策略。 支持最优(best)和随机(random)两种方式。</td>
</tr>
<tr>
<td style="text-align:left"><strong>max_depth</strong> : int or None, optional(dafault=None)</td>
<td style="text-align:left">表示决策树的最大深度。 None: 表示不设置深度,可以任意扩展,<br> 直到叶子节点的个数小于min_samples_split个数。</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_samples_split</strong> : int, float, optional(default=2)</td>
<td style="text-align:left">表示最小分割样例数。<br> if int, 表示最小分割样例树,如果小于这个数字,不在进行分割。<br> if float, 表示的比例[0,1],最小的分割数字=ceil(min_samples_split * n_samples)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_samples_leaf</strong> : int, float, optional (default=1)</td>
<td style="text-align:left">表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回。<br> if int, min_samples_leaf就是最小样例数。<br> if float, 表示的比例[0,1],最小的样例数=ceil(min_samples_leaf * n_samples)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_weight_fraction_leaf</strong> : float, optional (default=0。)</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><strong>max_features</strong> : int, float, str or None, optional(default=None)</td>
<td style="text-align:left">进行最优分割时,特征的最大个数。<br> if int, max_features是每次分割的最大特征数<br> if float, int(max_features * n_features)作为最大特征数<br> if “auto”, 则max_features=sqrt(n_features)<br> if “sqrt”, 则max_features=sqrt(n_features)<br> if “log2”, 则max_features=log2(n_features)<br> if None, 则max_features=n_features</td>
</tr>
<tr>
<td style="text-align:left"><strong>random_state</strong> : int, RandomState instance or None, optional (default=None)</td>
<td style="text-align:left">随机化种子, if None,使用np.random随机产生</td>
</tr>
<tr>
<td style="text-align:left"><strong>max_leaf_nodes</strong> : int or None, optional (default=None)</td>
<td style="text-align:left">最大的叶子节点个数,如果大于这个值,需要进行继续划分。 None则表示没有限制。</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_impurity_decrease</strong> : float, optional (default=0。)</td>
<td style="text-align:left">分割之后基尼指数大于这个数,则进行分割。<br>N_t / N <em> (impurity - N_t_R / N_t </em> right_impurity - N_t_L / N_t * left_impurity)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_impurity_split</strong> : float, default=1e-7</td>
<td style="text-align:left">停止增长的阈值,小于这个值直接返回。<br> DecisionTreeRegressor: sklearn中回归树的接口。</td>
</tr>
<tr>
<td style="text-align:left"><strong>criterion</strong> : str, optional (default=”mse”)</td>
<td style="text-align:left">其他参数和分类树类似。<br> mse: mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node,<br> mae: mean absolute error, which minimizes the L1 loss using the median of each terminal node.</td>
</tr>
</tbody>
</table>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>使用cart树的分类和回归两个接口，接口参考sklearn。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numbers</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> ceil</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> issparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""自定的树结构,用来保存决策树.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Paramters:</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    col: int, default(-1)</span></span><br><span class="line"><span class="string">        当前使用的第几列数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    val: int or float or str, 分割节点</span></span><br><span class="line"><span class="string">        分割节点的值, </span></span><br><span class="line"><span class="string">        int or float : 使用大于进行比较 </span></span><br><span class="line"><span class="string">        str : 使用等于模式</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    LeftChild: DecisionTree</span></span><br><span class="line"><span class="string">        左子树, &lt;= val</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    RightChild: DecisionTree</span></span><br><span class="line"><span class="string">        右子树, &gt; val</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    results: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, col=<span class="number">-1</span>, val=None, LeftChild=None, RightChild=None, result=None)</span>:</span></span><br><span class="line">        self.col = col</span><br><span class="line">        self.val = val</span><br><span class="line">        self.LeftChild = LeftChild</span><br><span class="line">        self.RightChild = RightChild</span><br><span class="line">        self.result = result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTreeClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""使用基尼指数的分类决策树接口.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Paramters:</span></span><br><span class="line"><span class="string">    ---------</span></span><br><span class="line"><span class="string">    max_depth : int or None, optional(dafault=None)</span></span><br><span class="line"><span class="string">        表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,</span></span><br><span class="line"><span class="string">        直到叶子节点的个数小于min_samples_split个数.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_samples_split : int, optional(default=2)</span></span><br><span class="line"><span class="string">        表示最小分割样例数.</span></span><br><span class="line"><span class="string">        if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_samples_leaf : int, optional (default=1)</span></span><br><span class="line"><span class="string">        表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.</span></span><br><span class="line"><span class="string">        if int, min_samples_leaf就是最小样例数.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_impurity_decrease : float, optional (default=0.)</span></span><br><span class="line"><span class="string">        分割之后基尼指数大于这个数,则进行分割.</span></span><br><span class="line"><span class="string">        N_t / N * (impurity - N_t_R / N_t * right_impurity</span></span><br><span class="line"><span class="string">                        - N_t_L / N_t * left_impurity)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_impurity_split : float, default=1e-7</span></span><br><span class="line"><span class="string">        停止增长的阈值,小于这个值直接返回.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    classes_ : array of shape (n_classes,) or a list of such arrays</span></span><br><span class="line"><span class="string">        表示所有的类</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    feature_importances_ : ndarray of shape (n_features,)</span></span><br><span class="line"><span class="string">        特征重要性, 被选择最优特征的次数,进行降序.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    tree_ : Tree object</span></span><br><span class="line"><span class="string">        The underlying Tree object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_depth=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_samples_split=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_samples_leaf=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_impurity_decrease=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_impurity_split=<span class="number">1e-7</span>)</span>:</span>        </span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.min_samples_leaf = min_samples_leaf</span><br><span class="line">        self.min_impurity_decrease = min_impurity_decrease</span><br><span class="line">        self.min_impurity_split = min_impurity_split</span><br><span class="line">        self.classes_ = <span class="literal">None</span></span><br><span class="line">        self.max_features_ = <span class="literal">None</span></span><br><span class="line">        self.decision_tree = <span class="literal">None</span></span><br><span class="line">        self.all_feats = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, check_input=True)</span>:</span></span><br><span class="line">        <span class="string">"""使用X和y训练决策树的分类模型.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span></span><br><span class="line"><span class="string">            The target values (class labels) as integers or strings.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        check_input : bool, (default=True)</span></span><br><span class="line"><span class="string">            Allow to bypass several input checking.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        self : object</span></span><br><span class="line"><span class="string">            Fitted estimator.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(X, list):</span><br><span class="line">            X = self.__check_array(X)</span><br><span class="line">        <span class="keyword">if</span> isinstance(y, list):</span><br><span class="line">            y = self.__check_array(y)</span><br><span class="line">        <span class="keyword">if</span> X.shape[<span class="number">0</span>] != y.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"输入的数据X和y长度不匹配"</span>)</span><br><span class="line">        </span><br><span class="line">        self.classes_ = list(set(y))</span><br><span class="line">        <span class="keyword">if</span> isinstance(X, pd.DataFrame):</span><br><span class="line">            X = X.values</span><br><span class="line">        <span class="keyword">if</span> isinstance(y, pd.DataFrame):</span><br><span class="line">            y = y.values</span><br><span class="line">        </span><br><span class="line">        data_origin = np.c_[X, y]</span><br><span class="line"><span class="comment">#         print (data_origin)</span></span><br><span class="line">        self.all_feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line">        self.max_features_ = X.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        data = copy.deepcopy(data_origin)</span><br><span class="line">        self.decision_tree = self.__build_tree(data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__predict_one</span><span class="params">(self, input_x)</span>:</span></span><br><span class="line">        <span class="string">"""预测一个样例的返回结果.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        input_x : list or np.ndarray</span></span><br><span class="line"><span class="string">            需要预测输入数据</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        class : 对应的类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        tree = self.decision_tree</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(node, tree)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> tree.result != <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span> tree.result</span><br><span class="line">            value = node[tree.col]</span><br><span class="line">            tree = tree.LeftChild <span class="keyword">if</span> value &lt;= tree.val <span class="keyword">else</span> tree.RightChild</span><br><span class="line">            <span class="keyword">return</span> run(node, tree)</span><br><span class="line"></span><br><span class="line">        pre_y = run(input_x, tree)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pre_y</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test)</span>:</span></span><br><span class="line">        <span class="string">"""预测函数,</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        test: &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        result : np.array(list) </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test)):</span><br><span class="line">            result.append(self.__predict_one(test[i]))</span><br><span class="line">        <span class="keyword">return</span> np.array(result)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, vali_X, vali_y)</span>:</span></span><br><span class="line">        <span class="string">"""验证模型的特征,这里使用准确率.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        vali_X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        vali_y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span></span><br><span class="line"><span class="string">            The target values (class labels) as integers or strings.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        score : float, 预测的准确率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        vali_y = np.array(vali_y)</span><br><span class="line">        pre_y = self.predict(vali_X)</span><br><span class="line">        pre_score = <span class="number">1.0</span> * sum(vali_y == pre_y) / len(vali_y)</span><br><span class="line">        <span class="keyword">return</span> pre_score</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__build_tree</span><span class="params">(self, data, depth)</span>:</span></span><br><span class="line">        <span class="string">"""创建决策树的主要代码</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data : &#123;array-like&#125; of shape (n_samples, n_features) + &#123;label&#125;</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        depth: int, 树的深度</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        DecisionTree</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        """</span>        </span><br><span class="line">        labels = np.unique(data[:,<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># 只剩下唯一的类别时,停止,返回对应类别</span></span><br><span class="line">        <span class="keyword">if</span> len(labels) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=list(labels)[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历完所有特征时,只剩下label标签,就返回出现字数最多的类标签</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.all_feats:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 超过最大深度,则停止,使用出现最多的参数作为该叶子节点的类</span></span><br><span class="line">        <span class="keyword">if</span> self.max_depth <span class="keyword">and</span> depth &gt; self.max_depth:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果剩余的样本数大于等于给定的参数 min_samples_split,</span></span><br><span class="line">        <span class="comment"># 则不在进行分割, 直接返回类别中最多的类,该节点作为叶子节点</span></span><br><span class="line">        <span class="keyword">if</span> self.min_samples_split &gt;= data.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 叶子节点个数小于指定参数就进行返回,叶子节点中的出现最多的类</span></span><br><span class="line">        <span class="keyword">if</span> self.min_samples_leaf &gt;= data.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据基尼指数选择每个分割的最优特征</span></span><br><span class="line">        best_idx, best_val, min_gini = self.__getBestFeature(data)</span><br><span class="line"><span class="comment">#         print ("Current best Feature:", best_idx, best_val, min_gini)</span></span><br><span class="line">        <span class="comment"># 如果当前的gini指数小于指定阈值,直接返回</span></span><br><span class="line">        <span class="keyword">if</span> min_gini &lt; self.min_impurity_split:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line">        </span><br><span class="line">        leftData, rightData = self.__splitData(data, best_idx, best_val)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        leftDecisionTree = self.__build_tree(leftData, depth + <span class="number">1</span>)</span><br><span class="line">        rightDecisionTree = self.__build_tree(rightData, depth + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> DecisionTree(col=best_idx, val=best_val, LeftChild=leftDecisionTree, RightChild=rightDecisionTree)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getBestFeature</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">"""得到最优特征对应的列</span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data: np.ndarray</span></span><br><span class="line"><span class="string">            从data中选择最优特征</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bestInx, val, 最优特征的列的索引和使用的值.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        best_idx = <span class="number">-1</span></span><br><span class="line">        best_val = <span class="literal">None</span></span><br><span class="line">        min_gini = <span class="number">1.0</span>                </span><br><span class="line">        <span class="comment"># 遍历现在可以使用的特征列</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> self.all_feats:</span><br><span class="line">            x = data[:,f]</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> x:</span><br><span class="line">                l, r = self.__splitData(data, f, v)</span><br><span class="line">                l_gini = <span class="number">1.0</span> * len(l) / len(data) * self.gini(l[:,<span class="number">-1</span>])</span><br><span class="line">                r_gini = <span class="number">1.0</span> * len(r) / len(data) * self.gini(r[:,<span class="number">-1</span>])</span><br><span class="line">                c_gini = l_gini + r_gini</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> c_gini &lt; min_gini:</span><br><span class="line">                    best_idx = f</span><br><span class="line">                    best_val = v</span><br><span class="line">                    min_gini = c_gini</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># 删除使用过的特征</span></span><br><span class="line">        self.all_feats.remove(best_idx)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> best_idx, best_val, min_gini</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gini</span><span class="params">(self, labels)</span>:</span></span><br><span class="line">        <span class="string">"""计算基尼指数.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        labels: list or np.ndarray, 数据对应的类目集合.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        gini : float  Gini(p) = \sum_&#123;k=1&#125;^&#123;K&#125;p_k(1-p_k)=1-\sum_&#123;k=1&#125;^&#123;K&#125;p_k^2 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        label_set = np.array(labels)</span><br><span class="line">        gini = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> np.unique(label_set):</span><br><span class="line">            gini -= <span class="number">1.0</span> * (np.sum(label_set == l) / len(label_set)) ** <span class="number">2</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> gini</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__splitData</span><span class="params">(self, data, featColumn, val)</span>:</span></span><br><span class="line">        <span class="string">"""根据特征划分数据集分成左右两部分.</span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data: np.ndarray, 分割的数据</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        featColumn : int, 使用第几列的数据进行分割</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        val : int or float or str, 分割的值</span></span><br><span class="line"><span class="string">            int or float : 使用比较方式</span></span><br><span class="line"><span class="string">            str : 使用相等方式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        leftData, RightData</span></span><br><span class="line"><span class="string">            int or left: leftData &lt;= val &lt; rightData</span></span><br><span class="line"><span class="string">            str : leftData = val and rightData != val</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(val, str):</span><br><span class="line">            leftData = data[data[:, featColumn] == val]</span><br><span class="line">            rightData = data[data[:, featColumn] != val]</span><br><span class="line">        <span class="keyword">elif</span> isinstance(val, int) <span class="keyword">or</span> isinstance(val, float):</span><br><span class="line">            leftData = data[data[:, featColumn] &lt;= val]</span><br><span class="line">            rightData = data[data[:, featColumn] &gt; val]</span><br><span class="line">        <span class="keyword">return</span> leftData, rightData</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__check_array</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""检查数据类型</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Retures</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X: &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(X, list):</span><br><span class="line">            X = np.array(X)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(X, np.ndarray) <span class="keyword">and</span> <span class="keyword">not</span> isinstance(X, pd.DataFrame):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"输出数据不合法,目前只支持np.ndarray or pd.DataFrame"</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 分类树</span></span><br><span class="line">    X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Classifier Score:"</span>, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<p>Classifier Score: 0.9333333333333333</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Day4 学习LR&amp;SVM理论并实践</title>
    <url>/p/2019/04/09/66464f97/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>使用下面模型对数据进行分类（包括：模型构建&amp;调参&amp;性能评估），并截图F1评分的结果<br>1）逻辑回归(LR)模型，学习理论并用Task2的特征实践<br>2）支持向量机(SVM) 模型，学习理论并用Task2的特征实践<br><a id="more"></a></p>
<hr>
<h2 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归(Logistic Regression)"></a>逻辑回归(Logistic Regression)</h2><blockquote>
<p>logistic回归又称logistic回归分析，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。</p>
</blockquote>
<p>线性回归模型通常是处理因变量是连续变量的问题，如果因变量是定性变量，线性回归模型就不再适用了，需采用逻辑回归模型解决。<br>逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。<br>二分类问题的概率与自变量之间的关系图形往往是一个S型曲线，采用的Sigmoid函数实现。<br>Sigmoid函数为:<br>$$ f\left ( x \right )= \frac{1}{1+e^{-x}} $$<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-x))</span><br><span class="line"> </span><br><span class="line">x= np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">0.1</span>)</span><br><span class="line">h = Sigmoid(x)            <span class="comment">#Sigmoid函数</span></span><br><span class="line">plt.plot(x, h)</span><br><span class="line">plt.axvline(<span class="number">0.0</span>, color=<span class="string">'k'</span>)   <span class="comment">#坐标轴上加一条竖直的线（0位置）</span></span><br><span class="line">plt.axhspan(<span class="number">0.0</span>, <span class="number">1.0</span>, facecolor=<span class="string">'1.0'</span>, alpha=<span class="number">1.0</span>, ls=<span class="string">'dotted'</span>)  </span><br><span class="line">plt.axhline(y=<span class="number">0.5</span>, ls=<span class="string">'dotted'</span>, color=<span class="string">'k'</span>) </span><br><span class="line">plt.yticks([<span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>])  <span class="comment">#y轴标度</span></span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>)       <span class="comment">#y轴范围</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img data-src="Figure1.jpg" alt="Sigmoid"></p>
<p>一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：<br>$$ P\left ( y=1|x;\theta  \right )=g\left ( \theta ^{T}x \right )=\frac{1}{1+e^{-\theta ^{T}x}} $$<br>其中，$$ P\left ( y=1|x;\theta  \right ) $$它表示的就是将因变量预测成1（阳性）的概率，具体来说它所要表达的是在给定x条件下事件y发生的条件概率，而是该条件概率的参数。<br>！其他过程，后续补充吧！<br>LogisticRegression回归模型在Sklearn.linear_model子类下，调用sklearn逻辑回归算法步骤比较简单，即：</p>
<ol>
<li>导入模型。调用逻辑回归LogisticRegression()函数。</li>
<li>fit()训练。调用fit(x,y)的方法来训练模型，其中x为数据的属性，y为所属类型。</li>
<li>predict()预测。利用训练得到的模型对数据集进行预测，返回预测结果。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris   </span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression </span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入数据集</span></span><br><span class="line">iris = load_iris()         </span><br><span class="line">X = X = iris.data[:, :<span class="number">2</span>]   <span class="comment">#只取前两维特征</span></span><br><span class="line">Y = iris.target           </span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归模型</span></span><br><span class="line">lr = LogisticRegression(C=<span class="number">1e5</span>)  </span><br><span class="line">lr.fit(X,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># meshgrid函数生成两个网格矩阵</span></span><br><span class="line">h = <span class="number">.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># pcolormesh函数将xx,yy两个网格矩阵和对应的预测结果Z绘制在图片上</span></span><br><span class="line">Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制散点图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, cmap=plt.cm.Spectral)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal width'</span>)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.legend(loc=<span class="number">2</span>) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img data-src="Figure2.jpg" alt="LogisticRegression"></p>
<p>LogisticRegression函数的参数说明如下：</p>
<ul>
<li><strong>penalty</strong>：惩罚项，str类型，可选参数为l1和l2，默认为l2。用于指定惩罚项中使用的规范。newton-cg、sag和lbfgs求解算法只支持L2规范。L1G规范假设的是模型的参数满足拉普拉斯分布，L2假设的模型参数满足高斯分布，所谓的范式就是加上对参数的约束，使得模型更不会过拟合(overfit)，但是如果要说是不是加了约束就会好，这个没有人能回答，只能说，加约束的情况下，理论上应该可以获得泛化能力更强的结果。</li>
<li><strong>dual</strong>：对偶或原始方法，bool类型，默认为False。对偶方法只用在求解线性多核(liblinear)的L2惩罚项上。当样本数量&gt;样本特征的时候，dual通常设置为False。</li>
<li><strong>tol</strong>：停止求解的标准，float类型，默认为1e-4。就是求解到多少的时候，停止，认为已经求出最优解。<br>c：正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化。</li>
<li><strong>fit_intercept</strong>：是否存在截距或偏差，bool类型，默认为True。</li>
<li><strong>intercept_scaling</strong>：仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。float类型，默认为1。</li>
<li><strong>class_weight</strong>：用于标示分类模型中各种类型的权重，可以是一个字典或者’balanced’字符串，默认为不输入，也就是不考虑权重，即为None。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者自己输入各个类型的权重。举个例子，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9,1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。如果class_weight选择balanced，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))。n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]。<br>那么class_weight有什么作用呢？<br>在分类模型中，我们经常会遇到两类问题：<ul>
<li>第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。</li>
<li>第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。提高了某种分类的权重，相比不考虑权重，会有更多的样本分类划分到高权重的类别，从而可以解决上面两类问题。</li>
</ul>
</li>
<li><strong>random_state</strong>：随机数种子，int类型，可选参数，默认为无，仅在正则化优化算法为sag,liblinear时有用。</li>
<li><strong>solver</strong>：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为* liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： <ul>
<li>liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</li>
<li>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li>
<li>newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</li>
<li>sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</li>
<li>saga：线性收敛的随机优化算法的的变重。<br>总结： <ol>
<li>liblinear适用于小数据集，而sag和saga适用于大数据集因为速度更快。<br>对于多分类问题，只有newton-cg,sag,saga和lbfgs能够处理多项损失，而liblinear受限于一对剩余(OvR)。啥意思，就是用liblinear的时候，如果是多分类问题，得先把一种类别作为一个类别，剩余的所有类别作为另外一个类别。一次类推，遍历所有类别，进行分类。</li>
<li>newton-cg,sag和lbfgs这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear和saga通吃L1正则化和L2正则化。<br>同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到L2正则化。</li>
<li>从上面的描述，大家可能觉得，既然newton-cg, lbfgs和sag这么多限制，如果不是大样本，我们选择liblinear不就行了嘛！错，因为liblinear也有自己的弱点！我们知道，逻辑回归有二元逻辑回归和多元逻辑回归。对于多元逻辑回归常见的有one-vs-rest(OvR)和many-vs-many(MvM)两种。而MvM一般比OvR分类相对准确一些。郁闷的是liblinear只支持OvR，不支持MvM，这样如果我们需要相对精确的多元逻辑回归时，就不能选择liblinear了。也意味着如果我们需要相对精确的多元逻辑回归不能使用L1正则化了。</li>
</ol>
</li>
</ul>
</li>
<li><strong>max_iter</strong>：算法收敛最大迭代次数，int类型，默认为10。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数。</li>
<li><strong>multi_class</strong>：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。ovr即前面提到的one-vs-rest(OvR)，而multinomial即前面提到的many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。<br>OvR和MvM有什么不同<em>？</em> <ul>
<li>OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。</li>
<li>而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类。</li>
<li>可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。</li>
</ul>
</li>
<li><strong>verbose</strong>：日志冗长度，int类型。默认为0。就是不输出训练过程，1的时候偶尔输出结果，大于1，对于每个子模型都输出。</li>
<li><strong>warm_start</strong>：热启动参数，bool类型。默认为False。如果为True，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化）。</li>
<li><strong>n_jobs</strong>：并行数。int类型，默认为1。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序。<br>总结：</li>
<li>优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低。</li>
<li>缺点：容易欠拟合，分类精度可能不高。</li>
<li>其他： <ul>
<li>Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。</li>
<li>改进的一些最优化算法，比如sag。它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批量处理。</li>
<li>机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。</li>
<li>我们需要根据数据的情况，这是Sklearn的参数，以期达到更好的分类效果。</li>
</ul>
</li>
</ul>
<h2 id="支持向量机-Support-Vector-Machine"><a href="#支持向量机-Support-Vector-Machine" class="headerlink" title="支持向量机(Support Vector Machine)"></a>支持向量机(Support Vector Machine)</h2><blockquote>
<p>支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类（binary classification）的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面（maximum-margin hyperplane）。SVM可以通过核方法（kernel method）进行非线性分类，是常见的核学习（kernel learning）方法之一。</p>
</blockquote>
]]></content>
      <categories>
        <category>Datawhale</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>nlp</tag>
        <tag>machine learning</tag>
        <tag>lr</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>Day5 聚类</title>
    <url>/p/2020/01/19/c40f9711/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>相关概念<ul>
<li>无监督学习</li>
<li>聚类的定义</li>
</ul>
</li>
<li>常用距离公式<ul>
<li>曼哈顿距离</li>
<li>欧式距离</li>
<li>闵可夫斯基距离</li>
<li>切比雪夫距离</li>
<li>夹角余弦</li>
<li>汉明距离</li>
<li>杰卡德相似系数</li>
<li>杰卡德距离</li>
</ul>
</li>
<li>K-Means聚类：聚类过程和原理、算法流程、算法优化（k-means++、Mini Batch K-Means）</li>
<li>层次聚类：Agglomerative Clustering过程和原理</li>
<li>密度聚类：DBSCAN过程和原理</li>
<li>谱聚类：谱聚类原理（邻接矩阵、度矩阵、拉普拉斯矩阵、RatioCut、Ncut）和过程</li>
<li>高斯混合聚类：GMM过程和原理、EM算法原理、利用EM算法估计高斯混合聚类参数</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="练习部分"><a href="#练习部分" class="headerlink" title="练习部分"></a>练习部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task5_cluster_plus.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task5_cluster_plus.ipynb</a></p>
<ul>
<li>利用sklearn解决聚类问题。</li>
<li>sklearn.cluster.KMeans</li>
</ul>
<a id="more"></a>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul>
<li><strong>无监督学习</strong>： 无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的数据进行分类或分群。无监督学习的主要运用包含：聚类分析、关系规则、维度缩减。它是监督式学习和强化学习等策略之外的一种选择。 一个常见的无监督学习是数据聚类。在人工神经网络中，生成对抗网络、自组织映射和适应性共振理论则是最常用的非监督式学习。</li>
<li><strong>聚类</strong>： 聚类是一种无监督学习。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集，这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。</li>
</ul>
<p>通过简单的例子来直接查看K均值聚类的效果<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类前</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show() <span class="comment">#pic1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类后</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>).fit(X)</span><br><span class="line">label_pred = kmeans.labels_</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=label_pred)</span><br><span class="line">plt.show() <span class="comment">#pic2</span></span><br></pre></td></tr></table></figure></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/sogo20200119214622.png" alt="pic1"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/sogo20200119214642.png" alt="pic2"></p>
<h3 id="性能度量-4"><a href="#性能度量-4" class="headerlink" title="性能度量 ^4"></a>性能度量 <a href="https://www.zybuluo.com/rianusr/note/1199877" target="_blank" rel="noopener">^4</a></h3><p>在机器学习中我们都需要对任务进行评价以便于进行下一步的优化，聚类的性能度量主要有一下两种。</p>
<ul>
<li>外部指标：是指把算法得到的划分结果跟某个外部的“参考模型”（如专家给出的划分结果）比较</li>
<li>内部指标：是指直接考察聚类结果，不利用任何参考模型的指标。</li>
</ul>
<h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><p>在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。</p>
<h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。<br>$$d(x,y)=\sqrt{\Sigma_{k=1}^n (x_k-y_k)^2}$$</p>
<h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>曼哈顿距离也称为街区距离，计算公式如下：<br>$$d(x,y)=\Sigma_{k=1}^n \left|x_k-y_k\right|$$</p>
<h4 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h4><p>$$d(x,y) = \lim_{n\rightarrow \infty} (\Sigma_{k=1}^n (\left|x_k-y_k\right|)^r)^\dfrac{1}{r} = max_k (\left|x_k-y_k\right|)$$</p>
<h4 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h4><p>$$d(x,y)=(\Sigma_{k=1}^n (\left|x_k-y_k\right|)^r)^\dfrac{1}{r}$$<br>式中，r是一个可变参数，根据参数r取值的不同，闵可夫斯基距离可以表示一类距离</p>
<ul>
<li>r = 1时，为曼哈顿距离</li>
<li>r = 2时，为欧式距离</li>
<li>r →∞时，为切比雪夫距离</li>
</ul>
<p>闵可夫斯基距离包括欧式距离、曼哈顿距离、切比雪夫距离都假设数据各维属性的量纲和分布（期望、方差）相同，因此适用于度量独立同分布的数据对象。</p>
<h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><p>余弦相似度公式定义如下：<br>$$cos⁡(x,y)=\dfrac{xy}{\left|x\right|\left|y\right|} = \dfrac{\Sigma_{k=1}^n x_k y_k}{\sqrt{\Sigma_{k=1}^n x_k^2} \sqrt{\Sigma_{k=1}^n y_k^2}}$$<br>余弦相似度实际上是向量xx和yy夹角的余弦度量，可用来衡量两个向量方向的差异。如果余弦相似度为11，则xx和yy之间夹角为0°0°，两向量除模外可认为是相同的；如果预先相似度为00，则xx和yy之间夹角为90°90°，则认为两向量完全不同。在计算余弦距离时，将向量均规范化成具有长度11，因此不用考虑两个数据对象的量值。 余弦相似度常用来度量文本之间的相似性。文档可以用向量表示，向量的每个属性代表一个特定的词或术语在文档中出现的频率，尽管文档具有大量的属性，但每个文档向量都是稀疏的，具有相对较少的非零属性值。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/image_1chf96u471s12en1lji15vb1gvo9.png" alt="余弦距离"></p>
<h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>$$mahalanobis(x,y)=(x-y)\Sigma^{-1}(x-y)^T$$<br>式中，Σ−1Σ−1是数据协方差矩阵的逆。 前面的距离度量方法大都假设样本独立同分布、数据属性之间不相关。马氏距离考虑了数据属性之间的相关性，排除了属性间相关性的干扰，而且与量纲无关。若协方差矩阵是对角阵，则马氏距离变成了标准欧式距离；若协方差矩阵是单位矩阵，各个样本向量之间独立同分布，则变成欧式距离。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/image_1chd8cv6quhfu13kl7jme13df29.png" alt="马氏距离"></p>
<h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类亦称”基于原型的聚类” (prototype-based clustering)，此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常用.通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解.采用不同的原型表示、不同的求解方式，将产生不同的算法.</p>
<h4 id="K均值"><a href="#K均值" class="headerlink" title="K均值"></a>K均值</h4><p><strong>k均值聚类算法（k-means clustering algorithm）</strong>是一种迭代求解的聚类分析算法，其步骤是</p>
<blockquote>
<p>1: 创建 k 个点作为起始质心（通常是随机选择）<br>2: 当任意一个点的簇分配结果发生改变时（不改变时算法结束）<br>3: 　　对数据集中的每个数据点<br>4: 　　　　对每个质心<br>5: 　　　　　　计算质心与数据点之间的距离<br>6: 　　　　将数据点分配到距其最近的簇<br>7: 　　对每一个簇, 计算簇中所有点的均值并将均值作为质心<br>8: 聚类中心以及分配给它们的对象就代表一个聚类。</p>
</blockquote>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/v2-77e1b4e663ba45f0ced9839149e06b0d_r.jpg" alt="K均值算法"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    欧氏距离计算函数</span></span><br><span class="line"><span class="string">    :param vecA:</span></span><br><span class="line"><span class="string">    :param vecB:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :return: float </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dist = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    dist = np.sqrt(np.sum(np.square(vecA - vecB)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataMat, k)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    为给定数据集构建一个包含K个随机质心的集合,</span></span><br><span class="line"><span class="string">    随机质心必须要在整个数据集的边界之内,这可以通过找到数据集每一维的最小和最大值来完成</span></span><br><span class="line"><span class="string">    然后生成0到1.0之间的随机数并通过取值范围和最小值,以便确保随机点在数据的边界之内</span></span><br><span class="line"><span class="string">    :param np.dataMat:</span></span><br><span class="line"><span class="string">    :param k:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :return: np.dataMat</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取样本数与特征值</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment"># 初始化质心,创建(k,n)个以零填充的矩阵</span></span><br><span class="line">    centroids = np.mat(np.zeros((k, n)))</span><br><span class="line">    print(centroids)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 循环遍历特征值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        min_J= np.min(dataMat[:,i])</span><br><span class="line">        range_J = float(np.max(dataMat[:,i])-min_J)</span><br><span class="line">        </span><br><span class="line">        centroids[:,i] = np.mat(min_J+range_J * np.random.rand(k,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回质心</span></span><br><span class="line">    <span class="keyword">return</span> centroids.A</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataMat, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    创建K个质心,然后将每个店分配到最近的质心,再重新计算质心。</span></span><br><span class="line"><span class="string">    这个过程重复数次,直到数据点的簇分配结果不再改变为止</span></span><br><span class="line"><span class="string">    :param dataMat: 数据集</span></span><br><span class="line"><span class="string">    :param k: 簇的数目</span></span><br><span class="line"><span class="string">    :param distMeans: 计算距离</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取样本数和特征数</span></span><br><span class="line">    m, n = np.shape(dataMat)</span><br><span class="line">    <span class="comment"># 初始化一个矩阵来存储每个点的簇分配结果</span></span><br><span class="line">    <span class="comment"># clusterAssment包含两个列:一列记录簇索引值,第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果)</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 创建质心,随机K个质心</span></span><br><span class="line">    centroids = randCent(dataMat, k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 遍历所有数据找到距离每个点最近的质心,</span></span><br><span class="line">        <span class="comment"># 可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            minDist = float(<span class="string">"inf"</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                <span class="comment"># 计算数据点到质心的距离</span></span><br><span class="line">                <span class="comment"># 计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud</span></span><br><span class="line">                distJI = distMeas(centroids[j, :], dataMat[i, :])</span><br><span class="line">                <span class="comment"># 如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引)</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 如果任一点的簇分配结果发生改变,则更新clusterChanged标志</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex: </span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># 更新簇分配结果为最小质心的index(索引),minDist(最小距离)的平方</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># print(centroids)</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历所有质心并更新它们的取值</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">            <span class="comment">#取每个质心周围所有的点</span></span><br><span class="line">            point = dataMat[np.nonzero(clusterAssment[:,<span class="number">0</span>].A==i)[<span class="number">0</span>]]</span><br><span class="line">            <span class="comment"># 计算所有点的均值,axis=0表示沿矩阵的列方向进行均值计算</span></span><br><span class="line">            centroids[i,:] = np.mean(point,axis=<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 返回所有的类质心与点分配结果</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行Kmeans，假设有两聚类中心</span></span><br><span class="line">center,label_pred = kMeans(X, k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转化成易绘图的形式</span></span><br><span class="line">label = label_pred[:, <span class="number">0</span>].A.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果可视化</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=label)</span><br><span class="line">plt.scatter(center[<span class="number">0</span>, <span class="number">0</span>], center[<span class="number">0</span>, <span class="number">1</span>], marker=<span class="string">"*"</span>, s = <span class="number">100</span>)</span><br><span class="line">plt.scatter(center[<span class="number">1</span>, <span class="number">0</span>], center[<span class="number">1</span>, <span class="number">1</span>], marker=<span class="string">"*"</span>, s = <span class="number">100</span>)</span><br><span class="line">plt.show() <span class="comment"># pic3</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/sogo20200119215336.png" alt="pic3"></p>
<h4 id="LVQ-7"><a href="#LVQ-7" class="headerlink" title="LVQ ^7"></a>LVQ <a href="http://ddrv.cn/a/66611" target="_blank" rel="noopener">^7</a></h4><p><strong>学习向量量化(Learning Vector Quantization,简称LVQ)</strong>属于原型聚类，即试图找到一组原型向量来聚类，每个原型向量代表一个簇，将空间划分为若干个簇，从而对于任意的样本，可以将它划入到它距离最近的簇中，不同的是LVQ假设数据样本带有类别标记，因此可以利用这些类别标记来辅助聚类。</p>
<p>大致思想如下：</p>
<ol>
<li>统计样本的类别，假设一共有q类，初始化为原型向量的标记为{t1,t2,……,tq}。从样本中随机选取q个样本点位原型向量{p1, p2 ,……, pq}。初始化一个学习率a,a 取值范围(0,1)。</li>
<li>从样本集中随机选取一个样本(x, y)，计算该样本与q个原型向量的距离（欧几里得距离），找到最小的那个原型向量p，判断样本的标记y与原型向量的标记t是不是一致。若一致则更新为p’ = p + a<em>(x-p)，否则更新为p’ = p – a</em>(x – p)。</li>
<li>重复第2步直到满足停止条件。（如达到最大迭代次数）</li>
<li>返回q个原型向量。</li>
</ol>
<h4 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h4><p>高斯混合聚类：高斯混合聚类与k均值、LVQ用原型向量来刻画聚类结构不同，高斯混合聚类采用概率模型来表达聚类原型。相对于k均值聚类算法使用 k 个原型向量来表达聚类结构，高斯混合聚类使用 k 个高斯概率密度函数混合来表达聚类结构</p>
<p>$$P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )$$<br>于是迭代更新 k 个簇原型向量的工作转换为了迭代更新 k 个高斯概率密度函数的任务。每个高斯概率密度函数代表一个簇，当一个新的样本进来时，我们可以通过这 k 的函数的值来为新样本分类</p>
<p>高斯混合模型聚类算法EM步骤如下：</p>
<ol>
<li>猜测有几个类别，既有几个高斯分布。</li>
<li>针对每一个高斯分布，随机给其均值和方差进行赋值。</li>
<li>针对每一个样本，计算其在各个高斯分布下的概率。</li>
<li>针对每一个高斯分布，每一个样本对该高斯分布的贡献可以由其下的概率表示，如概率大则表示贡献大，反之亦然。这样把样本对该高斯分布的贡献作为权重来计算加权的均值和方差。之后替代其原本的均值和方差。</li>
<li>重复3~4直到每一个高斯分布的均值和方差收敛。</li>
</ol>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/v2-27b00c8cc948bf2ac129cddbfbddb93f_r.jpg" alt="高斯混合模型聚类算法"></p>
<h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><p>层次聚类(hierarchical clustering)基于簇间的相似度在不同层次上分析数据，从而形成树形的聚类结构，层次聚类一般有两种划分策略：自底向上的聚合（agglomerative）策略和自顶向下的分拆（divisive）策略。</p>
<h4 id="AGNES"><a href="#AGNES" class="headerlink" title="AGNES"></a>AGNES</h4><p>AGNES算法是自底向上的层次聚类算法。开始时将数据集中的每个样本初始化为一个簇，然后找到距离最近的两个簇，将他们合并，不断重复这个过程，直达到到预设的聚类数目为止。</p>
<p>簇间距离的计算可以有三种形式：</p>
<ul>
<li><ul>
<li><strong>最小距离</strong>：$d_{min}(C_i,C_j)=\min_{p\in C_i,q\in C_j}|p-q|.$</li>
</ul>
</li>
<li><ul>
<li><strong>最大距离</strong>：$d_{max}(C_i,C_j)=\max_{p\in C_i,q\in C_j}|p-q|.$</li>
</ul>
</li>
<li><ul>
<li><strong>平均距离</strong>：$d_{avg}(C_i,C_j)=\frac{1}{|C_i||C_j|}\sum_{p\in C_i}\sum_{q\in C_j}|p-q|.$</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>输入</strong>：样本集D={x1,x2,…,xm}D={x1,x2,…,xm}<br>   聚类簇距离度量函数dd；<br>   聚类簇数kk<br><strong>过程</strong>：<br>1： for j=1,2,…,mj=1,2,…,m do<br>2：     Cj={xj}Cj={xj}<br>3： end for<br>4： for i=1,2,…,mi=1,2,…,m do<br>5：  for i=1,2,…,mi=1,2,…,m do<br>6：   M(i,j)=d(Ci,Cj)M(i,j)=d(Ci,Cj);<br>7：   M(j,i)=M(i,j)M(j,i)=M(i,j);<br>8：  end for<br>9： end for<br>10： 设置当前聚类簇个数：q=mq=m;<br>11： while q&gt;kq&gt;k do<br>12：  找出距离最近的两个聚类簇Ci∗Ci∗和Cj∗Cj∗;<br>13：  合并Ci∗Ci∗和Cj∗Cj∗：Ci∗=Ci∗⋃Cj∗Ci∗=Ci∗⋃Cj∗；<br>14：  for j=j∗+1,j∗+2,..,qj=j∗+1,j∗+2,..,q do<br>15：   将聚类簇CjCj重新编号为CjCj<br>16：  end for<br>17：  删除距离矩阵MM的第j∗j∗行和第j∗j∗列;<br>18：  for j=1,2,…,q−1j=1,2,…,q−1 do<br>19：   M(i,j)=d(Ci,Cj)M(i,j)=d(Ci,Cj);<br>20：   M(j,i)=M(i,j)M(j,i)=M(i,j);<br>21：  end for<br>22：  q=q−1q=q−1<br>23： end while<br><strong>输出</strong>：簇划分:C={C1,C2,…,Ck}</p>
</blockquote>
<h4 id="自顶而下"><a href="#自顶而下" class="headerlink" title="自顶而下"></a>自顶而下</h4><p>自顶而下：把整个数据集视作一个簇，然后把一个簇分成几个簇，接着再分别把每一个簇分成更小的簇，如此反复下去，直到满足要求为止。</p>
<h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><p>密度聚类假设聚类结构通过样本分布的紧密程度。此算法是基于密度的角度来考察样本之间的连接性，并基于连接性不断扩展聚类簇最后获得最终的结果。通过判断样本在区域空间内是否大于某个阈值来决定是否将其放到与之相近的样本中。</p>
<h4 id="DBSCAN-6"><a href="#DBSCAN-6" class="headerlink" title="DBSCAN ^6"></a>DBSCAN <a href="https://blog.csdn.net/zhouxianen1987/article/details/68945844" target="_blank" rel="noopener">^6</a></h4><ul>
<li><strong>e-邻域</strong>:对xj∈D,其∈邻域包含样本集D中与xj的距离不大于e的样本,即N(xj)= {xi∈D | dist(xi,xj)≤e};</li>
<li><strong>核心对象(core object)</strong>: 若xj的E-邻域至少包含MinPts个样本，即|Ne(xj)|≥MinPts,则xj是-一个核心对象;</li>
<li><strong>密度直达(directly density- reachable)</strong>:若xj位于xi的e-邻域中,且xi是核心对象,则称x;由xi密度直达;</li>
<li><strong>密度可达(density. reachable)</strong>: 对xi与xj,若存在样本序列P1,P2,… ,Pn,其中p1=xi,Pn=xj且pi+1由pi密度直达,则称xj由xi密度可达;</li>
<li><strong>密度相连(density-conected)</strong>: 对xi与xj,若存在xk使得xi与xj均由xk密度可达,则称xi与xj密度相连.</li>
</ul>
<blockquote>
<p>首先将数据集D中的所有对象标记为未处理状态<br>1: for（数据集D中每个对象p） do<br>2:     if （p已经归入某个簇或标记为噪声） then<br>3:         continue;<br>4:     else<br>5:         检查对象p的Eps邻域 NEps(p) ；<br>6:         if (NEps(p)包含的对象数小于MinPts) then<br>7:             标记对象p为边界点或噪声点；<br>8:         else<br>9:             标记对象p为核心点，并建立新簇C, 并将p邻域内所有点加入C<br>10:            for (NEps(p)中所有尚未被处理的对象q)  do<br>11:                检查其Eps邻域NEps(q)，若NEps(q)包含至少MinPts个对象，则将NEps(q)中未归入任何一个簇的对象加入C；<br>12:            end for<br>13:        end if<br>14:     end if<br>15: end for</p>
</blockquote>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>相比 K-平均算法，DBSCAN 不需要预先声明聚类数量。</li>
<li>DBSCAN 可以找出任何形状的聚类，甚至能找出一个聚类，它包围但不连接另一个聚类，另外，由于 MinPts 参数，single-link effect （不同聚类以一点或极幼的线相连而被当成一个聚类）能有效地被避免。</li>
<li>DBSCAN 能分辨噪音（局外点）。</li>
<li>DBSCAN 只需两个参数，且对数据库内的点的次序几乎不敏感（两个聚类之间边缘的点有机会受次序的影响被分到不同的聚类，另外聚类的次序会受点的次序的影响）。</li>
<li>DBSCAN 被设计成能配合可加速范围访问的数据库结构，例如 R*树。</li>
<li>如果对资料有足够的了解，可以选择适当的参数以获得最佳的分类。</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>DBSCAN 不是完全决定性的：在两个聚类交界边缘的点会视乎它在数据库的次序决定加入哪个聚类，幸运地，这种情况并不常见，而且对整体的聚类结果影响不大——DBSCAN 对核心点和噪音都是决定性的。DBSCAN* 是一种变化了的算法，把交界点视为噪音，达到完全决定性的结果。</li>
<li>DBSCAN 聚类分析的质素受函数 regionQuery(P,ε) 里所使用的度量影响，最常用的度量是欧几里得距离，尤其在高维度资料中，由于受所谓“维数灾难”影响，很难找出一个合适的 ε ，但事实上所有使用欧几里得距离的算法都受维数灾难影响。</li>
<li>如果数据库里的点有不同的密度，而该差异很大，DBSCAN 将不能提供一个好的聚类结果，因为不能选择一个适用于所有聚类的 minPts-ε 参数组合。</li>
<li>如果没有对资料和比例的足够理解，将很难选择适合的 ε 参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">'''计算样本点之间的距离</span></span><br><span class="line"><span class="string">    :param data(mat):样本</span></span><br><span class="line"><span class="string">    :return:dis(mat):样本点之间的距离</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m, n = np.shape(data)</span><br><span class="line">    dis = np.mat(np.zeros((m, m)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, m):</span><br><span class="line">            <span class="comment"># 计算i和j之间的欧式距离</span></span><br><span class="line">            tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</span><br><span class="line">                tmp += (data[i, k] - data[j, k]) * (data[i, k] - data[j, k])</span><br><span class="line">            dis[i, j] = np.sqrt(tmp)</span><br><span class="line">            dis[j, i] = dis[i, j]</span><br><span class="line">    <span class="keyword">return</span> dis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_eps</span><span class="params">(distance_D, eps)</span>:</span></span><br><span class="line">    <span class="string">'''找到距离≤eps的样本的索引</span></span><br><span class="line"><span class="string">    :param distance_D(mat):样本i与其他样本之间的距离</span></span><br><span class="line"><span class="string">    :param eps(float):半径的大小</span></span><br><span class="line"><span class="string">    :return: ind(list):与样本i之间的距离≤eps的样本的索引</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    ind = []</span><br><span class="line">    n = np.shape(distance_D)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> distance_D[<span class="number">0</span>, j] &lt;= eps:</span><br><span class="line">            ind.append(j)</span><br><span class="line">    <span class="keyword">return</span> ind</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dbscan</span><span class="params">(data, eps, MinPts)</span>:</span></span><br><span class="line">    <span class="string">'''DBSCAN算法</span></span><br><span class="line"><span class="string">    :param data(mat):需要聚类的数据集</span></span><br><span class="line"><span class="string">    :param eps(float):半径</span></span><br><span class="line"><span class="string">    :param MinPts(int):半径内最少的数据点数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        types(mat):每个样本的类型：核心点、边界点、噪音点</span></span><br><span class="line"><span class="string">        sub_class(mat):每个样本所属的类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = np.shape(data)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 在types中，1为核心点，0为边界点，-1为噪音点</span></span><br><span class="line">    types = np.mat(np.zeros((<span class="number">1</span>, m)))</span><br><span class="line">    sub_class = np.mat(np.zeros((<span class="number">1</span>, m)))</span><br><span class="line">    <span class="comment"># 用于判断该点是否处理过，0表示未处理过</span></span><br><span class="line">    dealt = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 计算每个数据点之间的距离</span></span><br><span class="line">    dis = distance(data)</span><br><span class="line">    <span class="comment"># 用于标记类别</span></span><br><span class="line">    number = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每一个点进行处理</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># 找到未处理的点</span></span><br><span class="line">        <span class="keyword">if</span> dealt[i, <span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 找到第i个点到其他所有点的距离</span></span><br><span class="line">            D = dis[i,]</span><br><span class="line">            <span class="comment"># 找到半径eps内的所有点</span></span><br><span class="line">            ind = find_eps(D, eps)</span><br><span class="line">            <span class="comment"># 区分点的类型</span></span><br><span class="line">            <span class="comment"># 边界点</span></span><br><span class="line">            <span class="keyword">if</span> len(ind) &gt; <span class="number">1</span> <span class="keyword">and</span> len(ind) &lt; MinPts + <span class="number">1</span>:</span><br><span class="line">                types[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">                sub_class[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 噪音点</span></span><br><span class="line">            <span class="keyword">if</span> len(ind) == <span class="number">1</span>:</span><br><span class="line">                types[<span class="number">0</span>, i] = <span class="number">-1</span></span><br><span class="line">                sub_class[<span class="number">0</span>, i] = <span class="number">-1</span></span><br><span class="line">                dealt[i, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            <span class="comment"># 核心点</span></span><br><span class="line">            <span class="keyword">if</span> len(ind) &gt;= MinPts + <span class="number">1</span>:</span><br><span class="line">                types[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> ind:</span><br><span class="line">                    sub_class[<span class="number">0</span>, x] = number</span><br><span class="line">                <span class="comment"># 判断核心点是否密度可达</span></span><br><span class="line">                <span class="keyword">while</span> len(ind) &gt; <span class="number">0</span>:</span><br><span class="line">                    dealt[ind[<span class="number">0</span>], <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">                    D = dis[ind[<span class="number">0</span>],]</span><br><span class="line">                    tmp = ind[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">del</span> ind[<span class="number">0</span>]</span><br><span class="line">                    ind_1 = find_eps(D, eps)</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> len(ind_1) &gt; <span class="number">1</span>:  <span class="comment"># 处理非噪音点</span></span><br><span class="line">                        <span class="keyword">for</span> x1 <span class="keyword">in</span> ind_1:</span><br><span class="line">                            sub_class[<span class="number">0</span>, x1] = number</span><br><span class="line">                        <span class="keyword">if</span> len(ind_1) &gt;= MinPts + <span class="number">1</span>:</span><br><span class="line">                            types[<span class="number">0</span>, tmp] = <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            types[<span class="number">0</span>, tmp] = <span class="number">0</span></span><br><span class="line">                            </span><br><span class="line">                        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(ind_1)):</span><br><span class="line">                            <span class="keyword">if</span> dealt[ind_1[j], <span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                                dealt[ind_1[j], <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">                                ind.append(ind_1[j])</span><br><span class="line">                                sub_class[<span class="number">0</span>, ind_1[j]] = number</span><br><span class="line">                number += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 最后处理所有未分类的点为噪音点</span></span><br><span class="line">    ind_2 = ((sub_class == <span class="number">0</span>).nonzero())[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ind_2:</span><br><span class="line">        sub_class[<span class="number">0</span>, x] = <span class="number">-1</span></span><br><span class="line">        types[<span class="number">0</span>, x] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> types, sub_class</span><br><span class="line">In [ ]:</span><br><span class="line">types, P = dbscan(X, <span class="number">0.1</span>, <span class="number">4</span>)</span><br><span class="line">types, P</span><br></pre></td></tr></table></figure>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c40f9711/153aceb3cdac953277c6c840339ac023.jpg" alt="聚类方法概述"></p>
<table>
<thead>
<tr>
<th style="text-align:left">Method name（方法名称）</th>
<th style="text-align:left">Parameters（参数）</th>
<th style="text-align:left">Scalability（可扩展性）</th>
<th style="text-align:left">Usecase（使用场景）</th>
<th style="text-align:left">Geometry (metric used)（几何图形（公制使用））</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">K-Means（K-均值）</td>
<td style="text-align:left">number of clusters（聚类形成的簇的个数）</td>
<td style="text-align:left">非常大的 n_samples, 中等的 n_clusters 使用 MiniBatch 代码）</td>
<td style="text-align:left">通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇）</td>
<td style="text-align:left">Distances between points（点之间的距离）</td>
</tr>
<tr>
<td style="text-align:left">Affinity propagation</td>
<td style="text-align:left">damping（阻尼）, sample preference（样本偏好）</td>
<td style="text-align:left">Not scalable with n_samples（n_samples 不可扩展）</td>
<td style="text-align:left">Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td style="text-align:left">Graph distance (e.g. nearest-neighbor graph)（图距离（例如，最近邻图））</td>
</tr>
<tr>
<td style="text-align:left">Mean-shift</td>
<td style="text-align:left">bandwidth（带宽）</td>
<td style="text-align:left">Not scalable with n_samples （n_samples不可扩展）</td>
<td style="text-align:left">Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td style="text-align:left">Distances between points（点之间的距离）</td>
</tr>
<tr>
<td style="text-align:left">Spectral clustering</td>
<td style="text-align:left">number of clusters（簇的个数）</td>
<td style="text-align:left">中等的 n_samples, 小的 n_clusters</td>
<td style="text-align:left">Few clusters, even cluster size, non-flat geometry（几个簇，均匀的簇大小，非平面几何）</td>
<td style="text-align:left">Graph distance (e.g. nearest-neighbor graph)（图距离（例如最近邻图））</td>
</tr>
<tr>
<td style="text-align:left">Ward hierarchical clustering</td>
<td style="text-align:left">number of clusters（簇的个数）</td>
<td style="text-align:left">大的 n_samples 和 n_clusters</td>
<td style="text-align:left">Many clusters, possibly connectivity constraints（很多的簇，可能连接限制）</td>
<td style="text-align:left">Distances between points（点之间的距离）</td>
</tr>
<tr>
<td style="text-align:left">Agglomerative clustering</td>
<td style="text-align:left">number of clusters（簇的个数）, linkage type（链接类型）, distance（距离）</td>
<td style="text-align:left">大的 n_samples 和 n_clusters</td>
<td style="text-align:left">Many clusters, possibly connectivity constraints, non Euclidean distances（很多簇，可能连接限制，非欧氏距离）</td>
<td style="text-align:left">Any pairwise distance（任意成对距离）</td>
</tr>
<tr>
<td style="text-align:left">DBSCAN</td>
<td style="text-align:left">neighborhood size（neighborhood 的大小）</td>
<td style="text-align:left">非常大的 n_samples, 中等的 n_clusters</td>
<td style="text-align:left">Non-flat geometry, uneven cluster sizes（非平面几何，不均匀的簇大小）</td>
<td style="text-align:left">Distances between nearest points（最近点之间的距离）</td>
</tr>
<tr>
<td style="text-align:left">Gaussian mixtures（高斯混合）</td>
<td style="text-align:left">many（很多）</td>
<td style="text-align:left">Not scalable（不可扩展）</td>
<td style="text-align:left">Flat geometry, good for density estimation（平面几何，适用于密度估计）</td>
<td style="text-align:left">Mahalanobis distances to centers（ 与中心的马氏距离）</td>
</tr>
<tr>
<td style="text-align:left">Birch</td>
<td style="text-align:left">branching factor（分支因子）, threshold（阈值）, optional global clusterer（可选全局簇）.</td>
<td style="text-align:left">大的 n_clusters 和 n_samples</td>
<td style="text-align:left">Large dataset, outlier removal, data reduction.（大型数据集，异常值去除，数据简化）</td>
<td style="text-align:left">Euclidean distance between points（点之间的欧氏距离）</td>
</tr>
</tbody>
</table>
<ul>
<li>当簇具有特殊的形状，即非平面流体（译注：即该流体的高斯曲率非0），并且标准欧氏距离不是正确的度量标准（metric）时，非平面几何聚类(Non-flat geometry clustering)是非常有用的。这种情况出现在上图的两个顶行中。</li>
<li>用于聚类（clustering）的高斯混合模型（Gaussian mixture models），专用于混合模型描述在 文档的另一章节 。可以将 KMeans 视为具有每个分量的协方差(equal covariance per component)相等的高斯混合模型的特殊情况。</li>
</ul>
<h3 id="sklearn参数详解"><a href="#sklearn参数详解" class="headerlink" title="sklearn参数详解"></a>sklearn参数详解</h3><p><a href="https://sklearn.apachecn.org/docs/0.21.3/22.html" target="_blank" rel="noopener">https://sklearn.apachecn.org/docs/0.21.3/22.html</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>cluster</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>Chrome插件存档</title>
    <url>/p/2019/02/20/3cd32332/</url>
    <content><![CDATA[<p>Chrome plugin …<br><a id="more"></a></p>
<h2 id="常用插件"><a href="#常用插件" class="headerlink" title="常用插件"></a>常用插件</h2><h4 id="下载-Chrono下载管理器"><a href="#下载-Chrono下载管理器" class="headerlink" title="下载  Chrono下载管理器"></a>下载  Chrono下载管理器</h4><h4 id="恢复最近关闭-SimpleUndoClose"><a href="#恢复最近关闭-SimpleUndoClose" class="headerlink" title="恢复最近关闭 SimpleUndoClose"></a>恢复最近关闭 SimpleUndoClose</h4><h4 id="鼠标手势-crxMouse-Chrome™-手势"><a href="#鼠标手势-crxMouse-Chrome™-手势" class="headerlink" title="鼠标手势 crxMouse Chrome™ 手势"></a>鼠标手势 crxMouse Chrome™ 手势</h4><h4 id="截图-FireShot"><a href="#截图-FireShot" class="headerlink" title="截图 FireShot"></a>截图 FireShot</h4><h4 id="保存文档-Evernote-Web-Clipper"><a href="#保存文档-Evernote-Web-Clipper" class="headerlink" title="保存文档 Evernote Web Clipper"></a>保存文档 Evernote Web Clipper</h4><h4 id="一键收藏-OneTab"><a href="#一键收藏-OneTab" class="headerlink" title="一键收藏 OneTab"></a>一键收藏 OneTab</h4><h4 id="Tampermonkey"><a href="#Tampermonkey" class="headerlink" title="Tampermonkey"></a>Tampermonkey</h4><h2 id="开发相关插件"><a href="#开发相关插件" class="headerlink" title="开发相关插件"></a>开发相关插件</h2><h3 id="Axure"><a href="#Axure" class="headerlink" title="Axure"></a>Axure</h3><h4 id="Axhub-Icons"><a href="#Axhub-Icons" class="headerlink" title="Axhub Icons"></a>Axhub Icons</h4><blockquote>
<p>可一键复制Iconfont图标到Axure内的 Chrome扩展</p>
</blockquote>
<p>官方地址：<a href="https://axhub.im/icons/" target="_blank" rel="noopener">https://axhub.im/icons/</a></p>
<ul>
<li>可以一键复制iconfont图标到 Axure</li>
<li>可以在iconfont调整颜色或大小后再复制到Axure</li>
<li>可以复制到Axure后转换为svg，无损缩放及调整颜色</li>
<li>支持iconfont 200W+ 单色和多色图标</li>
</ul>
<h3 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h3><h4 id="Isometric-Contributions"><a href="#Isometric-Contributions" class="headerlink" title="Isometric Contributions"></a>Isometric Contributions</h4><p>Isometric Contributions是一款可以将冷冰冰的数据转化为生动图形的Chrome扩展程序。它可以将你每天的contributions数目（可以理解为提交GitHub的数目）转化为颜色不一的立体柱状图，并给出自己的统计数据。Contributions数目由少到多分别以颜色逐渐变重的立体柱状图显示，宛如搭建一座高低错落的城市一般。</p>
<h4 id="Octohint"><a href="#Octohint" class="headerlink" title="Octohint"></a>Octohint</h4><p>在 GitHub 上浏览代码是有一些痛点的：它只提供基本的语法高亮。比如想找到某个变量出现或者声明的位置，一般只能通过浏览器自带的查找工具（Command + F 或 Ctrl + F）去找。如果是一个大的文件，比如有几千行，要找的变量可能是在几百行前声明的，中途还有一系列修改。这种查找方式比较繁琐，会造成一些心智负担。</p>
<p>Octohint 是一个浏览器扩展，目标是尽量减少这种心智负担，让浏览代码更加方便。</p>
<h4 id="Octotree"><a href="#Octotree" class="headerlink" title="Octotree"></a>Octotree</h4><p>Octotree是一个 Chrome插件，用来显示 Github 项目的目录结构。<br>Octotree的特性：<br>1.类似 IDE 的非常方便的代码目录树<br>2.使用 PJAX 的超快代码浏览（很快！）<br>3.支持公有库和私有库</p>
<p>Octotree的缺点:<br>1.需要初始化的时间，像nodejs这种稍微大一点的代码库要花大概1分钟多有去爬吧</p>
<h4 id="Sourcegraph"><a href="#Sourcegraph" class="headerlink" title="Sourcegraph"></a>Sourcegraph</h4><p>Sourcegraph 被大众广为熟知正是因为它支持在 GitHub 上轻松浏览和搜索代码，功能相当于整合了Octo大部分插件，但是需要跳转至Sourcegraph的页面，两者融合使用更方便。<br>Sourcegraph 是一款能够根据语义来把 Web 上的开源代码编入索引的代码搜索浏览工具。你可以从代码仓库和安装包，甚至是函数里搜索代码，同时也可以直接点击被完全创建了链接的代码来阅读文档、跳转到变量定义或者马上找到可用的 Demo。总而言之，你可以在你的 Web 浏览器上完成这一切，而不需要配置任何编辑器。</p>
<h3 id="Vue"><a href="#Vue" class="headerlink" title="Vue"></a>Vue</h3><h4 id="Vue-js-devtools"><a href="#Vue-js-devtools" class="headerlink" title="Vue.js devtools"></a>Vue.js devtools</h4><h3 id="React"><a href="#React" class="headerlink" title="React"></a>React</h3><h4 id="React-Developer-Tools"><a href="#React-Developer-Tools" class="headerlink" title="React Developer Tools"></a>React Developer Tools</h4><h4 id="Redux-DevTools"><a href="#Redux-DevTools" class="headerlink" title="Redux DevTools"></a>Redux DevTools</h4>]]></content>
      <categories>
        <category>TODO</category>
        <category>plugin</category>
      </categories>
      <tags>
        <tag>chrome</tag>
        <tag>github</tag>
        <tag>axure</tag>
        <tag>vue</tag>
        <tag>react</tag>
        <tag>plugin</tag>
      </tags>
  </entry>
  <entry>
    <title>Day6 朴素贝叶斯</title>
    <url>/p/2020/01/19/50902270/</url>
    <content><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>相关概念<ul>
<li>生成模型</li>
<li>判别模型</li>
</ul>
</li>
<li>朴素贝叶斯基本原理<ul>
<li>条件概率公式</li>
<li>乘法公式</li>
<li>全概率公式</li>
<li>贝叶斯定理</li>
<li>特征条件独立假设</li>
<li>后验概率最大化</li>
<li>拉普拉斯平滑</li>
</ul>
</li>
<li>朴素贝叶斯的三种形式<ul>
<li>高斯型</li>
<li>多项式型</li>
<li>伯努利型</li>
</ul>
</li>
<li>极值问题情况下的每个类的分类概率</li>
<li>下溢问题如何解决</li>
<li>零概率问题如何解决</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="练习部分"><a href="#练习部分" class="headerlink" title="练习部分"></a>练习部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task6_bayes_plus.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task6_bayes_plus.ipynb</a></p>
<ul>
<li>利用sklearn解决聚类问题。</li>
<li>sklearn.naive_bayes.GaussianNB</li>
</ul>
<a id="more"></a>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><p><strong>生成模型</strong>：在概率统计理论中, 生成模型是指能够随机生成观测数据的模型，尤其是在给定某些隐含参数的条件下。它给观测值和标注数据序列指定一个联合概率分布。在机器学习中，生成模型可以用来直接对数据建模（例如根据某个变量的概率密度函数进行数据采样），也可以用来建立变量间的条件概率分布。条件概率分布可以由生成模型根据贝叶斯定理形成。常见的基于生成模型算法有高斯混合模型和其他混合模型、隐马尔可夫模型、随机上下文无关文法、朴素贝叶斯分类器、AODE分类器、潜在狄利克雷分配模型、受限玻尔兹曼机</p>
<blockquote>
<p><strong>举例</strong>：要确定一个瓜是好瓜还是坏瓜，用判别模型的方法是从历史数据中学习到模型，然后通过提取这个瓜的特征来预测出这只瓜是好瓜的概率，是坏瓜的概率。</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p><strong>判别模型</strong>: 在机器学习领域判别模型是一种对未知数据 y 与已知数据 x 之间关系进行建模的方法。判别模型是一种基于概率理论的方法。已知输入变量 x ，判别模型通过构建条件概率分布 P(y|x) 预测 y 。常见的基于判别模型算法有逻辑回归、线性回归、支持向量机、提升方法、条件随机场、人工神经网络、随机森林、感知器</p>
<blockquote>
<p><strong>举例</strong>：利用生成模型是根据好瓜的特征首先学习出一个好瓜的模型，然后根据坏瓜的特征学习得到一个坏瓜的模型，然后从需要预测的瓜中提取特征，放到生成好的好瓜的模型中看概率是多少，在放到生产的坏瓜模型中看概率是多少，哪个概率大就预测其为哪个。</p>
</blockquote>
<p>生成模型是所有变量的全概率模型，而判别模型是在给定观测变量值前提下目标变量条件概率模型。因此生成模型能够用于模拟（即生成）模型中任意变量的分布情况，而判别模型只能根据观测变量得到目标变量的采样。判别模型不对观测变量的分布建模，因此它不能够表达观测变量与目标变量之间更复杂的关系。因此，生成模型更适用于无监督的任务，如分类和聚类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class="number">0.2</span>)</span><br><span class="line">clf = GaussianNB().fit(X_train, y_train)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Classifier Score:"</span>, clf.score(X_test, y_test)) <span class="comment"># Classifier Score: 0.9666666666666667</span></span><br></pre></td></tr></table></figure>
<h3 id="先验概率、条件概率"><a href="#先验概率、条件概率" class="headerlink" title="先验概率、条件概率"></a>先验概率、条件概率</h3><ul>
<li><strong>条件概率</strong>: 就是事件A在事件B发生的条件下发生的概率。条件概率表示为$P（A|B）$，读作“A在B发生的条件下发生的概率”。</li>
<li><strong>先验概率</strong>: 在贝叶斯统计中，某一不确定量 p 的先验概率分布是在考虑”观测数据”前，能表达 p 不确定性的概率分布。它旨在描述这个不确定量的不确定程度，而不是这个不确定量的随机性。这个不确定量可以是一个参数，或者是一个隐含变量。</li>
<li><strong>后验概率</strong>: 在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。同样，后验概率分布是一个未知量（视为随机变量）基于试验和调查后得到的概率分布。“后验”在本文中代表考虑了被测试事件的相关证据。</li>
</ul>
<p>通过上述西瓜的数据集来看:</p>
<ul>
<li>条件概率，就是在条件为瓜的颜色是青绿的情况下，瓜是好瓜的概率</li>
<li>先验概率，就是常识、经验、统计学所透露出的“因”的概率，即瓜的颜色是青绿的概率。</li>
<li>后验概率，就是在知道“果”之后，去推测“因”的概率，也就是说，如果已经知道瓜是好瓜，那么瓜的颜色是青绿的概率是多少。后验和先验的关系就需要运用贝叶斯决策理论来求解。</li>
</ul>
<h3 id="贝叶斯决策理论"><a href="#贝叶斯决策理论" class="headerlink" title="贝叶斯决策理论"></a>贝叶斯决策理论</h3><p>贝叶斯决策论是概率框架下实施决策的基本方法，对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。</p>
<p>假设有N种可能标记， $λ_{ij}$是将类$c_j$误分类为$c_i$所产生的损失，基于后验概率$ P(c_i | x)$ 可以获得样本x分类为$c_i$所产生的期望损失 ，即在样本x上的条件风险：<br>$$R(c_i|\mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j|\mathbf{x})$$<br>我们的任务是寻找一个判定准则 $h:X→Y$以最小化总体风险<br>$$R(h)= \mathbb{E}<em>x [R(h(\mathbf(x)) | \mathbf(x))]$$<br>显然，对每个样本x，若h能最小化条件风险 $R(h((x))|(x))$,则总体风险R(h)也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需要在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即：<br>$$h^* (x) = argmin</em>{c\in y} R(c|\mathbf{x})$$<br>此时，h 称作贝叶斯最优分类器，与之对应的总体风险R(h )称为贝叶斯风险，1-R(h<em>)反映了分类器能达到的最好性能，即机器学习所产生的模型精度的上限。<br>具体来说，若目标是最小化分类错误率（对应0/1损失），则$λ_{ij}$可以用0/1损失改写，得到条件风险和最小化分类错误率的最优分类器分别为：<br>$$R(c|\mathbf{x}) = 1- P(c|\mathbf{x})$$<br>$$h^</em>(x) = argmax_{c\in \mathcal{Y}} P(c|\mathbf{x})$$<br>即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标识。</p>
<p>获得后验概率的两种方法：</p>
<ul>
<li><ul>
<li><strong>判别式模型</strong>:给定x，可以通过直接建模$P(c|x)$来预测c。</li>
</ul>
</li>
<li><ul>
<li><strong>生成模型</strong>:先对联合分布$p(x,c)$建模，然后再有此获得$P(c|x)$。</li>
</ul>
</li>
</ul>
<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>对生成模型来说，必然考虑：<br>$$P(c|x) = \frac{P(x,c)}{P(x)} = \frac{P(c) P(x|c)}{P(x)}$$</p>
<p>其中$P(c)$是“先验概率”；$P(x|c)$是样本x对于类标记c的类条件概率，或称为“似然”；$P(x)$是用于归一化的“证据”因子。上式即为贝叶斯公式。</p>
<p>可以将其看做$$P(类别|特征) = \frac{P(特征,类别)}{P(特征)} = \frac{P(类别) P(特征|类别)}{P(特征)}$$</p>
<p>对类条件概率$P(x|c)$来说，直接根据样本出现的频率来估计将会遇到严重的困难，所以引入了极大似然估计。</p>
<h4 id="极大似然估计-3"><a href="#极大似然估计-3" class="headerlink" title="极大似然估计^3"></a>极大似然估计<a href="https://www.jianshu.com/p/f1d3906e4a3e" target="_blank" rel="noopener">^3</a></h4><p>估计类条件概率有一种常用的策略就是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。假设P(x|c)具有某种确定的形式并且被参数$θ_c$ 唯一确定，则我们的任务就是利用训练结D估计参数 $θ_c$。为了明确期间，我们将P(x|c)记为$p(x|θc)$.</p>
<blockquote>
<p>举个通俗的例子：假设一个袋子装有白球与红球，比例未知，现在抽取10次（每次抽完都放回，保证事件独立性），假设抽到了7次白球和3次红球，在此数据样本条件下，可以采用最大似然估计法求解袋子中白球的比例（最大似然估计是一种“模型已定，参数未知”的方法）。当然，这种数据情况下很明显，白球的比例是70%，但如何通过理论的方法得到这个答案呢？</p>
</blockquote>
<p>一些复杂的条件下，是很难通过直观的方式获得答案的，这时候理论分析就尤为重要了，这也是学者们为何要提出最大似然估计的原因。我们可以定义从袋子中抽取白球和红球的概率如下：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/1.png" alt="x1为第一次采样，x2为第二次采样，f为模型, theta为模型参数"></p>
<p>其中$\theta$是未知的，因此，我们定义似然L为：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/2.png" alt="L为似然的符号"></p>
<p>两边取ln，取ln是为了将右边的乘号变为加号，方便求导。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/3.png" alt="两边取ln的结果，左边的通常称之为对数似然。"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/4.png" alt="这是平均对数似然"></p>
<p>最大似然估计的过程，就是找一个合适的theta，使得平均对数似然的值为最大。因此，可以得到以下公式：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/5.png" alt="最大似然估计的公式"></p>
<p>这里讨论的是2次采样的情况，当然也可以拓展到多次采样的情况：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/6.png" alt="最大似然估计的公式（n次采样）"></p>
<p>我们定义M为模型（也就是之前公式中的f），表示抽到白球的概率为theta，而抽到红球的概率为(1-theta)，因此10次抽取抽到白球7次的概率可以表示为：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/7.png" alt="10次抽取抽到白球7次的概率"></p>
<p>将其描述为平均似然可得：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/8.png" alt="10次抽取抽到白球7次的平均对数似然，抽球的情况比较简单，可以直接用平均似然来求解"></p>
<p>那么最大似然就是找到一个合适的theta，获得最大的平均似然。因此我们可以对平均似然的公式对theta求导，并另导数为0。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/50902270/9.png" alt="求导过程"></p>
<p>由此可得，当抽取白球的概率为0.7时，最可能产生10次抽取抽到白球7次的事件。</p>
<p>以上就用到了最大似然估计的思想</p>
<p>令$D_c$表示训练集$D$中第c类样本组成的集合，假设这些集合是独立同分布的，则对参数$θcθc$对于数据集$D_c$的似然是:</p>
<p>$$P(D_c|\theta_c) = \prod P(\mathbf{x}|\theta_c)$$<br>对$θ_c$进行激发似然估计买就是去寻找能最大化似然函数的参数值$θ_c$.直观上，极大似然估计是在试图在$θ_c$的所有可能的去职中，找到一个能使数据出现最大“可能性”的最大值。</p>
<p>上面的式子中的连乘操作容易造成下溢，通常使用对数似然：<br>$$L(\theta_c) = \log P(D_c| \theta_c) = \sum_{x\in D_c} \log P(x|\theta_c)$$<br>此时，参数$θ_c$的极大似然估计$\hat{\theta_c}$为<br>$$\hat{\theta_c} = argmax_{\theta_c} LL(\theta_c)$$<br>例如，在连续属性的情形下，假设概率密度函数<br>$$p(x|c) \sim \mathcal{N}(\mu_c , \sigma^2)$$<br>则参数$μ_c$和$σ_2$的极大似然估计为：<br>$$\hat{\mu_c} = \frac{1}{|D_c|} \sum_{x\in D_c} x$$<br>$$\hat{\sigma_c}^2 = \frac{1}{|D_c|} \sum_{x\in D_c} (x-\hat{\mu_c} )(x-\hat{\mu_c} ^T)$$<br>也就是说通过极大似然发得到的额正态分布均值就是样本均值，方差就是<br>$(x-\hat{\mu_c} )(x-\hat{\mu_c} ^T)$的均值。这显然是一个符合只觉得结果，在离散属性情形下，也可以通过类似的方法来估计类条件概率。</p>
<blockquote>
<p>需要注意的是这种方法虽然能够使类条件概率估计变得简单，但是估计结果准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。在显示生活中往往需要应用任务本身的经验知识，“猜测”则会导致误导性的结果。</p>
</blockquote>
<p>贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p>
<ol>
<li>写出似然函数；</li>
<li>对似然函数取对数，并整理；</li>
<li>求导数，令偏导数为0，得到似然方程组；</li>
<li>解似然方程组，得到所有参数即为所求。</li>
</ol>
<h4 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h4><p>基于贝叶斯公式来估计后验概率$P(c|x)$主要困难在于类条件概率$P(x|c)$是<strong>所有属性</strong>上的联合概率，难以从有限的训练样本直接估计而得。 基于有限训练样本直接计算联合概率，在计算上将会遭遇组合爆炸问题；在数据上将会遭遇样本稀疏问题；属性越多，问题越严重。</p>
<p>为了避开这个障碍，朴素贝叶斯分类器采用了<strong>“属性条件独立性假设”：对已知类别，假设所有属性相互独立</strong>。换言之，假设每个属性独立的对分类结果发生影响相互独立。</p>
<p>回答西瓜的例子就可以认为｛色泽 根蒂 敲声 纹理 脐部 触感｝这些属性对西瓜是好还是坏的结果所产生的影响相互独立。</p>
<p>基于条件独立性假设，对于多个属性的后验概率可以写成：<br>$$P(c|\mathbf{x}) = \frac{P(C)P(\mathbf{x}|c)}{P(\mathbf{x})} = \frac{P(c)}{P(\mathbf{x})}\prod_{i=1}^d P(x_i|c)$$</p>
<p>d为属性数目，$x_i$是$x$在第$i$个属性上取值。 对于所有的类别来说P(x)相同，基于极大似然的贝叶斯判定准则有朴素贝叶斯的表达式：<br>$$h_{nb}(\mathbf{x}) = \arg max_{c\in \mathcal{Y}}P(c)\prod_{i=1}^d P(x_i|c) \quad (1)$$</p>
<h3 id="极值问题情况下的每个类的分类概率"><a href="#极值问题情况下的每个类的分类概率" class="headerlink" title="极值问题情况下的每个类的分类概率"></a>极值问题情况下的每个类的分类概率</h3><h4 id="极值问题"><a href="#极值问题" class="headerlink" title="极值问题"></a>极值问题</h4><p>很多时候遇到求出各种目标函数（object function）的最值问题（最大值或者最小值）。关于函数最值问题，其实在高中的时候我们就已经了解不少，最经典的方法就是：直接求出极值点。这些极值点的梯度为0。若极值点唯一，则这个点就是代入函数得出的就是最值；若极值点不唯一，那么这些点中，必定存在最小值或者最大值（去除函数的左右的最端点），所以把极值代入函数，经对比后可得到结果。</p>
<p>请注意：并不一定所有函数的极值都可以通过设置导数为0的方式求 出。也就是说，有些问题中当我们设定导数为0时，未必能直接计算出满足导数为0的点（比如逻辑回归模型），这时候就需要利用数值计算相关的技术（最典型为梯度下降法，牛顿法……）。</p>
<h3 id="下溢问题如何解决"><a href="#下溢问题如何解决" class="headerlink" title="下溢问题如何解决"></a>下溢问题如何解决</h3><p>数值下溢问题：是指计算机浮点数计算的结果小于可以表示的最小数，因为计算机的能力有限，当数值小于一定数时，其无法精确保存，会造成数值的精度丢失，由上述公式可以看到，求概率时多个概率值相乘，得到的结果往往非常小；因此通常采用取对数的方式，将连乘转化为连加，以避免数值下溢。</p>
<h3 id="零概率问题如何解决？"><a href="#零概率问题如何解决？" class="headerlink" title="零概率问题如何解决？"></a>零概率问题如何解决？</h3><p>零概率问题，就是在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0.</p>
<p>在实际的模型训练过程中，可能会出现零概率问题（因为先验概率和反条件概率是根据训练样本算的，但训练样本数量不是无限的，所以可能出现有的情况在实际中存在，但在训练样本中没有，导致为0的概率值，影响后面后验概率的计算），即便可以继续增加训练数据量，但对于有些问题来说，数据怎么增多也是不够的。这时我们说模型是不平滑的，我们要使之平滑，一种方法就是将训练（学习）的方法换成贝叶斯估计。</p>
<blockquote>
<p>现在看一个示例，及$P(敲声=清脆|好瓜=是)=\frac{8}{0}=0$ 不论样本的其他属性如何，分类结果都会为“好瓜=否”，这样显然不太合理。</p>
</blockquote>
<p><strong>朴素贝叶斯算法的先天缺陷</strong>：其他属性携带的信息被训练集中某个分类下未出现的属性值“抹去”，造成预测出来的概率绝对为0。为了拟补这一缺陷，前辈们引入了拉普拉斯平滑的方法：对先验概率的分子(划分的计数)加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1：</p>
<p>//TODO</p>
<p>将这两个式子应用到上面的计算过程中，就可以弥补朴素贝叶斯算法的这一缺陷问题。</p>
<p>用西瓜的数据来看，当我们计算</p>
<p>P(好瓜=是)时，样本有17个，所以|D| = 17，N，好瓜标签可以分为｛是，否｝两类，所以N=2，（好瓜=是）的样本个数有8个，所以这里$|D_c|$=8。</p>
<p>综上，根据拉普拉斯平滑后有<br>//TODO</p>
<h3 id="sklearn参数详解"><a href="#sklearn参数详解" class="headerlink" title="sklearn参数详解"></a>sklearn参数详解</h3><p>高斯朴素贝叶斯算法是假设特征的可能性(即概率)为高斯分布。<br>class sklearn.naive_bayes.GaussianNB(priors=None)</p>
<p>参数：<br><strong>priors</strong>:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。<br><strong>var_smoothing</strong>：可选参数，所有特征的最大方差<br>属性：<br><strong>class_prior_</strong>:每个样本的概率<br><strong>class_count</strong>:每个类别的样本数量<br><strong>classes_</strong>:分类器已知的标签类型<br><strong>theta_</strong>:每个类别中每个特征的均值<br><strong>sigma_</strong>:每个类别中每个特征的方差<br><strong>epsilon_</strong>:方差的绝对加值方法<br>贝叶斯的方法和其他模型的方法一致。<br><strong>fit(X,Y)</strong>:在数据集(X,Y)上拟合模型。<br><strong>get_params()</strong>:获取模型参数。<br><strong>predict(X)</strong>:对数据集X进行预测。<br><strong>predict_log_proba(X)</strong>:对数据集X预测，得到每个类别的概率对数值。predict_proba(X):对数据集X预测，得到每个类别的概率。<br><strong>score(X,Y)</strong>:得到模型在数据集(X,Y)的得分情况。<br>根据李航老师的代码构建自己的朴素贝叶斯模型<br><a href="https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC04%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/4.NaiveBayes.ipynb" target="_blank" rel="noopener">https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC04%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/4.NaiveBayes.ipynb</a></p>
<p>这里采用GaussianNB 高斯朴素贝叶斯,概率密度函数为<br>$$P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )$$<br>数学期望：$\mu$<br>方差：$\sigma ^2=\frac{1}{n}\sum_i^n(x_i-\overline x)^2$</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数学期望</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(X)</span>:</span></span><br><span class="line">        <span class="string">"""计算均值</span></span><br><span class="line"><span class="string">        Param: X : list or np.ndarray</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            avg : float</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        avg = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        avg = np.mean(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> avg</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 标准差（方差）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stdev</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""计算标准差</span></span><br><span class="line"><span class="string">        Param: X : list or np.ndarray</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            res : float</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        res = np.std(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 概率密度函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian_probability</span><span class="params">(self, x, mean, stdev)</span>:</span></span><br><span class="line">        <span class="string">"""根据均值和标注差计算x符号该高斯分布的概率</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        x : 输入</span></span><br><span class="line"><span class="string">        mean : 均值</span></span><br><span class="line"><span class="string">        stdev : 标准差</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        res : float， x符合的概率值</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        res = <span class="number">1</span>/(np.sqrt(<span class="number">2</span>*np.pi)*stdev) * np.exp(- (x - mean)**<span class="number">2</span> / (<span class="number">2</span> * stdev**<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 处理X_train</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summarize</span><span class="params">(self, train_data)</span>:</span></span><br><span class="line">        <span class="string">"""计算每个类目下对应数据的均值和标准差</span></span><br><span class="line"><span class="string">        Param: train_data : list</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return : [mean, stdev]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        summaries = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_data[<span class="number">0</span>])):</span><br><span class="line">            data = [x[i] <span class="keyword">for</span> x <span class="keyword">in</span> train_data]</span><br><span class="line">            summaries.append((NaiveBayes.mean(data), self.stdev(data)))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> summaries</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分类别求出数学期望和标准差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        labels = list(set(y))</span><br><span class="line">        data = &#123;label: [] <span class="keyword">for</span> label <span class="keyword">in</span> labels&#125;</span><br><span class="line">        <span class="keyword">for</span> f, label <span class="keyword">in</span> zip(X, y):</span><br><span class="line">            data[label].append(f)</span><br><span class="line">        self.model = &#123;</span><br><span class="line">            label: self.summarize(value) <span class="keyword">for</span> label, value <span class="keyword">in</span> data.items()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'gaussianNB train done!'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_probabilities</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="string">"""计算数据在各个高斯分布下的概率</span></span><br><span class="line"><span class="string">        Paramter:</span></span><br><span class="line"><span class="string">        input_data : 输入数据</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">        probabilities : &#123;label : p&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># summaries:&#123;0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]&#125;</span></span><br><span class="line">        <span class="comment"># input_data:[1.1, 2.2]</span></span><br><span class="line">        probabilities = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> label, value <span class="keyword">in</span> self.model.items():</span><br><span class="line">            probabilities[label] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">                <span class="keyword">if</span> isinstance(value[i] ,float)==<span class="literal">False</span> :</span><br><span class="line">                    mean, stdev = value[i]</span><br><span class="line">                    probabilities[label] *= self.gaussian_probability(input_data[i<span class="number">-2</span>], mean, stdev)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> probabilities</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 类别</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_test)</span>:</span></span><br><span class="line">        <span class="comment"># &#123;0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26&#125;</span></span><br><span class="line">        label = sorted(self.calculate_probabilities(X_test).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">-1</span>])[<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line">    <span class="comment"># 计算得分</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X_test, y_test)</span>:</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> zip(X_test, y_test):</span><br><span class="line">            label = self.predict(X)</span><br><span class="line">            <span class="keyword">if</span> label == y:</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> right / float(len(X_test))</span><br><span class="line"></span><br><span class="line">model = NaiveBayes()</span><br><span class="line"></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># (array([4.7, 3.2, 1.6, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.3, 3. , 1.1, 0.1]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.8, 1.6, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.8, 3. , 1.4, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.7, 1.5, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([4.7, 3.2, 1.3, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.4, 2.9, 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.2, 3.4, 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.4, 1.5, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.6, 3.4, 1.4, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([4.6, 3.6, 1. , 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 3.5, 1.6, 0.6]),)</span></span><br><span class="line"><span class="comment"># (array([4.9, 3.1, 1.5, 0.1]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 3.8, 1.7, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([4.6, 3.1, 1.5, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.9, 3.6, 1.4, 0.1]),)</span></span><br><span class="line"><span class="comment"># (array([4.8, 3.4, 1.9, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.5, 1.4, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.8, 1.5, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.3, 3.7, 1.5, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.4, 3.9, 1.7, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 3.4, 1.6, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([5.4, 3.7, 1.5, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.5, 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.9, 3. , 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 4.4, 1.5, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([4.8, 3.4, 1.6, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.4, 3.2, 1.3, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 3.3, 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.6, 3.2, 1.4, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.4, 3.4, 1.5, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 3.8, 1.9, 0.4]),)</span></span><br><span class="line"><span class="comment"># (array([4.5, 2.3, 1.3, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 3.5, 1.3, 0.3]),)</span></span><br><span class="line"><span class="comment"># (array([4.4, 3. , 1.3, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.2, 3.5, 1.5, 0.2]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 2.7, 5.1, 1.6]),)</span></span><br><span class="line"><span class="comment"># (array([6.1, 2.8, 4.7, 1.2]),)</span></span><br><span class="line"><span class="comment"># (array([6.6, 2.9, 4.6, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 2.9, 4.3, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.8, 2.6, 4. , 1.2]),)</span></span><br><span class="line"><span class="comment"># (array([5.5, 2.5, 4. , 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.5, 2.4, 3.8, 1.1]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 2.5, 3.9, 1.1]),)</span></span><br><span class="line"><span class="comment"># (array([5.4, 3. , 4.5, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 3.2, 4.5, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 2.8, 4.1, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 3.4, 4.5, 1.6]),)</span></span><br><span class="line"><span class="comment"># (array([5.9, 3.2, 4.8, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.6, 3. , 4.4, 1.4]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 3. , 4.5, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.8, 2.7, 3.9, 1.2]),)</span></span><br><span class="line"><span class="comment"># (array([4.9, 2.4, 3.3, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([5.9, 3. , 4.2, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 2.9, 4.5, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 3. , 4.2, 1.2]),)</span></span><br><span class="line"><span class="comment"># (array([7. , 3.2, 4.7, 1.4]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 2.5, 4.9, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.5, 2.6, 4.4, 1.2]),)</span></span><br><span class="line"><span class="comment"># (array([6.1, 2.8, 4. , 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 2. , 3.5, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 2.3, 4.4, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.5, 2.3, 4. , 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 2.7, 4.2, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 2.9, 3.6, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 2.8, 4.5, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.2, 2.9, 4.3, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.1, 3. , 4.6, 1.4]),)</span></span><br><span class="line"><span class="comment"># (array([5.5, 2.4, 3.7, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.7, 3. , 5. , 1.7]),)</span></span><br><span class="line"><span class="comment"># (array([5.8, 2.7, 4.1, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.5, 2.8, 4.6, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 2.2, 4. , 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.9, 3.1, 4.9, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 3. , 4.1, 1.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.8, 2.8, 4.8, 1.4]),)</span></span><br><span class="line"><span class="comment"># (array([5. , 2.3, 3.3, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 3.3, 4.7, 1.6]),)</span></span><br><span class="line"><span class="comment"># (array([6.2, 2.2, 4.5, 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 2.6, 3.5, 1. ]),)</span></span><br><span class="line"><span class="comment"># (array([5.1, 2.5, 3. , 1.1]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 2.7, 4.9, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.7, 2.5, 5.8, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.2, 3.4, 5.4, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([5.8, 2.8, 5.1, 2.4]),)</span></span><br><span class="line"><span class="comment"># (array([4.9, 2.5, 4.5, 1.7]),)</span></span><br><span class="line"><span class="comment"># (array([6.7, 3.3, 5.7, 2.1]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 2.9, 5.6, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.9, 3.1, 5.1, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 3.1, 5.5, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.1, 3. , 4.9, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.5, 3. , 5.2, 2. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 3.4, 5.6, 2.4]),)</span></span><br><span class="line"><span class="comment"># (array([7.2, 3.2, 6. , 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([7.7, 3. , 6.1, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 2.7, 5.3, 1.9]),)</span></span><br><span class="line"><span class="comment"># (array([7.2, 3.6, 6.1, 2.5]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 3.2, 5.3, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 3. , 4.8, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([5.8, 2.7, 5.1, 1.9]),)</span></span><br><span class="line"><span class="comment"># (array([6.7, 3.3, 5.7, 2.5]),)</span></span><br><span class="line"><span class="comment"># (array([6.5, 3. , 5.5, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([5.7, 2.5, 5. , 2. ]),)</span></span><br><span class="line"><span class="comment"># (array([5.9, 3. , 5.1, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([6.4, 2.8, 5.6, 2.2]),)</span></span><br><span class="line"><span class="comment"># (array([7.7, 2.6, 6.9, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 2.5, 5. , 1.9]),)</span></span><br><span class="line"><span class="comment"># (array([6.8, 3.2, 5.9, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.5, 3.2, 5.1, 2. ]),)</span></span><br><span class="line"><span class="comment"># (array([6. , 2.2, 5. , 1.5]),)</span></span><br><span class="line"><span class="comment"># (array([7.7, 2.8, 6.7, 2. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.1, 2.6, 5.6, 1.4]),)</span></span><br><span class="line"><span class="comment"># (array([6.9, 3.1, 5.4, 2.1]),)</span></span><br><span class="line"><span class="comment"># (array([6.7, 3. , 5.2, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([6.2, 2.8, 4.8, 1.8]),)</span></span><br><span class="line"><span class="comment"># (array([7.2, 3. , 5.8, 1.6]),)</span></span><br><span class="line"><span class="comment"># (array([6.9, 3.2, 5.7, 2.3]),)</span></span><br><span class="line"><span class="comment"># (array([7.6, 3. , 6.6, 2.1]),)</span></span><br><span class="line"><span class="comment"># (array([5.6, 2.8, 4.9, 2. ]),)</span></span><br><span class="line"><span class="comment"># (array([6.3, 3.3, 6. , 2.5]),)</span></span><br><span class="line"></span><br><span class="line"><span class="string">'gaussianNB train done!'</span></span><br><span class="line"></span><br><span class="line">print(model.predict([<span class="number">4.4</span>,  <span class="number">3.2</span>,  <span class="number">1.3</span>,  <span class="number">0.2</span>])) <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line">model.score(X_test, y_test) <span class="comment"># 0.9666666666666667</span></span><br></pre></td></tr></table></figure>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>朴素贝叶斯模型有稳定的分类效率。</li>
<li>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。</li>
<li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点:"></a>缺点:</h4><ul>
<li>理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。</li>
<li>需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。</li>
<li>由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。</li>
<li>对输入数据的表达形式很敏感。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Datawhale</category>
        <category>初级算法梳理</category>
      </categories>
      <tags>
        <tag>datawhale</tag>
        <tag>ml</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>bayes</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>无重复字符的最长子串</title>
    <url>/p/2021/02/06/ebbfe8fe/</url>
    <content><![CDATA[<p>给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。<br><a id="more"></a></p>
]]></content>
      <categories>
        <category>Python</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>git-help</title>
    <url>/p/2020/01/27/2103582f/</url>
    <content><![CDATA[<p>git remote上传到远程代码库：第一次上传</p>
<p>1.初始化git版本库：<code>git init</code></p>
<p>2.添加文件到本地库：<code>git add .</code></p>
<p>3.提交文件到本地库：<code>git commit -m &quot;msg(提交日志)&quot;</code><br>重要提示：2、3可合并（<code>git commit -am &quot;&quot;</code>）</p>
<p>4.关联远程库：<code>git remote add origin(可修改) branch_Name(为空时默认为master) url</code><br>关联之后可以用<code>git remote -v</code>来检查是否关联成功</p>
<p>5.一般情况需要先pull一下：<code>git pull origin master</code><br>一般情况下含有共同文件时需要执行 <code>git merge origin/master --allow-unrelated-histories</code>，这之后解决一下冲突</p>
<p>6.push到远程库：<code>git push -u origin master</code></p>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>配置Hexo+NexT</title>
    <url>/p/2019/02/19/d77c6bdf/</url>
    <content><![CDATA[<p>使用Github发布Next主题的Hexo个人静态博客。<br><a id="more"></a></p>
<h1 id="hexo-init"><a href="#hexo-init" class="headerlink" title="hexo init"></a>hexo init</h1><h1 id="tags与categories页面"><a href="#tags与categories页面" class="headerlink" title="tags与categories页面"></a>tags与categories页面</h1><h1 id="新增博文"><a href="#新增博文" class="headerlink" title="新增博文"></a>新增博文</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new PAGENAME</span><br></pre></td></tr></table></figure>
<h1 id="使用数学公式"><a href="#使用数学公式" class="headerlink" title="使用数学公式"></a>使用数学公式</h1><ol>
<li><p>更换渲染引擎<br>Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 Next 主题中开启 MathJax 开关<br>进入到主题目录，找到 _config.yml 配置问题，把 math 默认的 false 修改为true，具体如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Math Equations Render Support</span></span><br><span class="line"><span class="attr">math:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default (true) will load mathjax / katex script on demand.</span></span><br><span class="line">  <span class="comment"># That is it only render those page which has `mathjax: true` in Front Matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>还需要在文章的Front-matter里打开mathjax开关</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: index.html</span><br><span class="line">date: 2018-07-05 12:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="使用NexT主题"><a href="#使用NexT主题" class="headerlink" title="使用NexT主题"></a>使用NexT主题</h1><h1 id="配置NexT插件"><a href="#配置NexT插件" class="headerlink" title="配置NexT插件"></a>配置NexT插件</h1><h2 id="Valine"><a href="#Valine" class="headerlink" title="Valine"></a>Valine</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Valine.</span></span><br><span class="line"><span class="comment"># You can get your appid and appkey from https://leancloud.cn</span></span><br><span class="line"><span class="comment"># more info please open https://valine.js.org</span></span><br><span class="line"><span class="attr">valine:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version.</span></span><br><span class="line">  <span class="attr">appid:</span>  <span class="comment"># your leancloud application appid</span></span><br><span class="line">  <span class="attr">appkey:</span>  <span class="comment"># your leancloud application appkey</span></span><br><span class="line">  <span class="attr">notify:</span> <span class="literal">false</span> <span class="comment"># mail notifier , https://github.com/xCss/Valine/wiki</span></span><br><span class="line">  <span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line">  <span class="attr">placeholder:</span> <span class="string">Just</span> <span class="string">go</span> <span class="string">go</span> <span class="comment"># comment box placeholder</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">mm</span> <span class="comment"># gravatar style</span></span><br><span class="line">  <span class="attr">guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># custom comment header</span></span><br><span class="line">  <span class="attr">pageSize:</span> <span class="number">10</span> <span class="comment"># pagination size</span></span><br><span class="line">  <span class="attr">visitor:</span> <span class="literal">false</span> <span class="comment"># leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br></pre></td></tr></table></figure>
<h2 id="Busuanzi"><a href="#Busuanzi" class="headerlink" title="Busuanzi"></a>Busuanzi</h2><h2 id="Abbrlink"><a href="#Abbrlink" class="headerlink" title="Abbrlink"></a>Abbrlink</h2><h2 id="google-site-verification"><a href="#google-site-verification" class="headerlink" title="google_site_verification"></a>google_site_verification</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Google Webmaster tools verification.</span></span><br><span class="line"><span class="attr">google_site_verification:</span> <span class="string">XXX</span></span><br></pre></td></tr></table></figure>
<h2 id="google-analytics"><a href="#google-analytics" class="headerlink" title="google_analytics"></a>google_analytics</h2><p>Google Analytics中创建用户，登录。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Google Analytics</span></span><br><span class="line"><span class="attr">google_analytics:</span></span><br><span class="line">  <span class="attr">tracking_id:</span> <span class="string">UA-XXXXXXXX-X</span></span><br><span class="line">  <span class="attr">localhost_ignored:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h2 id="baidu-site-verification"><a href="#baidu-site-verification" class="headerlink" title="baidu_site_verification"></a>baidu_site_verification</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Baidu Webmaster tools verification.</span></span><br><span class="line"><span class="attr">baidu_site_verification:</span> <span class="string">xxx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEO.</span></span><br><span class="line"><span class="attr">baidu_push:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="local-search"><a href="#local-search" class="headerlink" title="local_search"></a>local_search</h2><h2 id="highlight-theme"><a href="#highlight-theme" class="headerlink" title="highlight_theme"></a>highlight_theme</h2><h2 id="AddThis"><a href="#AddThis" class="headerlink" title="AddThis"></a>AddThis</h2><h2 id="fancybox"><a href="#fancybox" class="headerlink" title="fancybox"></a>fancybox</h2><p>开启后，可以具有图片点击放大或者视频点击弹出的效果。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">fancybox:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h2 id="creative-commons"><a href="#creative-commons" class="headerlink" title="creative_commons"></a>creative_commons</h2><p>文章末尾添加版权声明。<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">creative_commons:</span></span><br><span class="line">  <span class="attr">license:</span> <span class="string">by-nc-sa</span></span><br><span class="line">  <span class="attr">sidebar:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">post:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">language:</span></span><br></pre></td></tr></table></figure></p>
<p>NexT 使用文档：<a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">http://theme-next.iissnan.com/getting-started.html</a></p>
]]></content>
      <categories>
        <category>TODO</category>
        <category>blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>sketch插件存档</title>
    <url>/p/2020/01/27/23841d50/</url>
    <content><![CDATA[<p>Sketch plugin … </p>
<a id="more"></a>
<h2 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h2><h3 id="Kitchen"><a href="#Kitchen" class="headerlink" title="Kitchen"></a>Kitchen</h3><h3 id="Flavor"><a href="#Flavor" class="headerlink" title="Flavor"></a>Flavor</h3><h3 id="Rename-it"><a href="#Rename-it" class="headerlink" title="Rename-it"></a>Rename-it</h3><h3 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h3><h3 id="Segmentcircle"><a href="#Segmentcircle" class="headerlink" title="Segmentcircle"></a>Segmentcircle</h3><h3 id="Sketch-Measure"><a href="#Sketch-Measure" class="headerlink" title="Sketch Measure"></a>Sketch Measure</h3><h3 id="Preview-in-browser"><a href="#Preview-in-browser" class="headerlink" title="Preview in browser"></a>Preview in browser</h3><h3 id="Sketch-Guides"><a href="#Sketch-Guides" class="headerlink" title="Sketch Guides"></a>Sketch Guides</h3><h3 id="Material-Plugin"><a href="#Material-Plugin" class="headerlink" title="Material Plugin"></a>Material Plugin</h3><h2 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h2><h3 id="Ant-Design-Pro"><a href="#Ant-Design-Pro" class="headerlink" title="Ant.Design.Pro"></a>Ant.Design.Pro</h3><h3 id="Material"><a href="#Material" class="headerlink" title="Material"></a>Material</h3>]]></content>
      <categories>
        <category>TODO</category>
        <category>plugin</category>
      </categories>
      <tags>
        <tag>plugin</tag>
        <tag>sketch</tag>
        <tag>antd</tag>
        <tag>原型</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot 异步处理</title>
    <url>/p/2020/01/30/1326f080/</url>
    <content><![CDATA[<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/page_9.png" alt="同步处理与异步处理请求流程"></p>
<p>在Servlet 3.0之前，Servlet采用Thread-Per-Request的方式处理请求，即每一次Http请求都由某一个线程从头到尾负责处理，也就是<strong>同步处理请求</strong>。如果一个请求需要进行IO操作，比如访问数据库、调用第三方服务接口等，那么其所对应的线程将同步地等待IO操作完成，而IO操作是非常慢的，所以此时的线程并不能及时地释放回线程池以供后续使用，在并发量越来越大的情况下，这将带来严重的性能问题。</p>
<a id="more"></a>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/13169342-ec4911e012331c29.jpg" alt="同步处理请求流程"></p>
<p>而在Servlet3.0发布后，提供了一个新特性：<strong>异步处理请求</strong>。可以先释放容器分配给请求的线程与相关资源，减轻系统负担，释放了容器所分配线程的请求，其响应将被延后，可以在耗时处理完成（例如长时间的运算）时再对客户端进行响应。其请求流程为：</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/13169342-4b8e6d78ce24c15c.jpg" alt="异步处理请求流程"></p>
<p>通常开发过程中，一般上我们都是同步调用，即：程序按定义的顺序依次执行的过程，每一行代码执行过程必须等待上一行代码执行完毕后才执行。而异步调用指：程序在执行时，无需等待执行的返回值可继续执行后面的代码。显而易见，同步有依赖相关性，而异步没有，所以异步可并发执行，可提高执行效率，在相同的时间做更多的事情。</p>
<h2 id="异步请求的实现"><a href="#异步请求的实现" class="headerlink" title="异步请求的实现"></a>异步请求的实现</h2><ul>
<li><strong>获取AsyncContext</strong>：根据HttpServletRequest对象获取。</li>
<li><strong>设置监听器</strong>: 可设置其开始、完成、异常、超时等事件的回调处理.</li>
<li><strong>设置超时</strong>：通过setTimeout方法设置，单位：毫秒。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/QQ20200131001141.png" alt="监听器的接口代码"></p>
<p>说明：</p>
<ul>
<li><strong>onStartAsync</strong>：异步线程开始时调用</li>
<li><strong>onError</strong>：异步线程出错时调用</li>
<li><strong>onTimeout</strong>：异步线程执行超时调用</li>
<li><strong>onComplete</strong>：异步执行完毕时调用</li>
</ul>
<p>一般上，我们在超时或者异常时，会返回给前端相应的提示，比如说超时了，请再次请求等等，根据各业务进行自定义返回。同时，在异步调用完成时，一般需要执行一些清理工作或者其他相关操作。</p>
<p>需要注意的是只有在调用request.startAsync前将监听器添加到AsyncContext，监听器的onStartAsync方法才会起作用，而调用startAsync前AsyncContext还不存在，所以第一次调用startAsync是不会被监听器中的onStartAsync方法捕获的，只有在超时后又重新开始的情况下onStartAsync方法才会起作用。</p>
<h3 id="Servlet方式实现异步请求"><a href="#Servlet方式实现异步请求" class="headerlink" title="Servlet方式实现异步请求"></a>Servlet方式实现异步请求</h3><blockquote>
<p>通过HttpServletRequest对象中获得一个AsyncContext对象，该对象构成了异步处理的上下文。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ServletController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/servlet"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/origin"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">origin</span><span class="params">(HttpServletRequest request,</span></span></span><br><span class="line"><span class="function"><span class="params">            HttpServletResponse response)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        response.getWriter().println(<span class="string">"这是【正常】的请求返回"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/async"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">todoAsync</span><span class="params">(HttpServletRequest request,HttpServletResponse response)</span> </span>&#123;</span><br><span class="line">        AsyncContext asyncContext = request.startAsync();</span><br><span class="line">        asyncContext.addListener(<span class="keyword">new</span> AsyncListener() &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimeout</span><span class="params">(AsyncEvent event)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                log.info(<span class="string">"超时了："</span>);</span><br><span class="line">                <span class="comment">//做一些超时后的相关操作</span></span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStartAsync</span><span class="params">(AsyncEvent event)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                log.info(<span class="string">"线程开始"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onError</span><span class="params">(AsyncEvent event)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                log.info(<span class="string">"发生错误："</span>,event.getThrowable());</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(AsyncEvent event)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">                log.info(<span class="string">"执行完成"</span>);</span><br><span class="line">                <span class="comment">//这里可以做一些清理资源的操作</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置超时时间</span></span><br><span class="line">        asyncContext.setTimeout(<span class="number">200</span>);</span><br><span class="line"></span><br><span class="line">        asyncContext.start(<span class="keyword">new</span> Runnable() &#123;            </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">100</span>);</span><br><span class="line">                    log.info(<span class="string">"内部线程："</span> + Thread.currentThread().getName());</span><br><span class="line">                    asyncContext.getResponse().setCharacterEncoding(<span class="string">"utf-8"</span>);</span><br><span class="line">                    asyncContext.getResponse().setContentType(<span class="string">"text/html;charset=UTF-8"</span>);</span><br><span class="line">                    asyncContext.getResponse().getWriter().println(<span class="string">"这是【异步】的请求返回"</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    log.error(<span class="string">"异常："</span>,e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//异步请求完成通知,此时整个请求才完成</span></span><br><span class="line">                <span class="comment">//其实可以利用此特性 进行多条消息的推送把连接挂起。。</span></span><br><span class="line">                asyncContext.complete();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//此时之类 request的线程连接已经释放了</span></span><br><span class="line">        log.info(<span class="string">"线程："</span> + Thread.currentThread().getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Spring方式实现异步请求"><a href="#Spring方式实现异步请求" class="headerlink" title="Spring方式实现异步请求"></a>Spring方式实现异步请求</h3><p>在Spring中，有多种方式实现异步请求，比如callable、DeferredResult或者WebAsyncTask。每个的用法略有不同，可根据不同的业务场景选择不同的方式。</p>
<h4 id="使用Callable异步处理请求"><a href="#使用Callable异步处理请求" class="headerlink" title="使用Callable异步处理请求"></a>使用Callable异步处理请求</h4><blockquote>
<p>使用很简单，直接返回的参数包裹一层callable即可，可以继承WebMvcConfigurerAdapter类来设置默认线程池和超时处理</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@RequestMapping</span>(<span class="string">"/order"</span>)</span><br><span class="line">   <span class="function"><span class="keyword">public</span> Callable&lt;String&gt; <span class="title">order</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       log.info(<span class="string">"主线程开始"</span>);</span><br><span class="line">       Callable&lt;String&gt; result = <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               log.info(<span class="string">"副线程开始"</span>);</span><br><span class="line">               Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">               log.info(<span class="string">"副线程返回"</span>);</span><br><span class="line">               <span class="keyword">return</span> <span class="string">"success"</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;;</span><br><span class="line">       log.info(<span class="string">"主线程返回"</span>);</span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="使用DeferredResult异步处理请求"><a href="#使用DeferredResult异步处理请求" class="headerlink" title="使用DeferredResult异步处理请求"></a>使用DeferredResult异步处理请求</h4><blockquote>
<p>相比于Callable，DeferredResult可以处理一些相对复杂一些的业务逻辑，最主要还是可以在另一个线程里面进行业务处理及返回，即可在两个完全不相干的线程间的通信。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DeferredResultController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeferredResultController</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ExecutorService FIXED_THREAD_POOL = Executors.newFixedThreadPool(<span class="number">30</span>);     <span class="comment">// 线程池</span></span><br><span class="line">    </span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/deferredresult"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DeferredResult&lt;String&gt; <span class="title">deferredResult</span><span class="params">()</span></span>&#123;</span><br><span class="line">        log.info(<span class="string">"外部线程："</span> + Thread.currentThread().getName());</span><br><span class="line">        <span class="comment">//设置超时时间</span></span><br><span class="line">        DeferredResult&lt;String&gt; result = <span class="keyword">new</span> DeferredResult&lt;String&gt;(<span class="number">60</span>*<span class="number">1000L</span>);</span><br><span class="line">        <span class="comment">//处理超时事件 采用委托机制</span></span><br><span class="line">        result.onTimeout(() -&gt; &#123;</span><br><span class="line">            log.error(<span class="string">"DeferredResult超时"</span>);</span><br><span class="line">            result.setResult(<span class="string">"超时了!"</span>);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.onCompletion(() -&gt; log.info(<span class="string">"调用完成"</span>));</span><br><span class="line">        </span><br><span class="line">        FIXED_THREAD_POOL.execute(() -&gt; &#123;</span><br><span class="line">            log.info(<span class="string">"内部线程："</span> + Thread.currentThread().getName());</span><br><span class="line">            <span class="comment">//返回结果</span></span><br><span class="line">            result.setResult(<span class="string">"DeferredResult!!"</span>);</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面将通过DeferredResult方式，模拟以下流程：</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/page_10.png" alt="同步处理与异步处理请求流程"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> MockQueue mockQueue;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> DeferredResultHolder deferredResultHolder;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/order"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DeferredResult&lt;String&gt; <span class="title">order</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        log.info(<span class="string">"主线程开始"</span>);</span><br><span class="line">        String orderNumber= RandomStringUtils.randomNumeric(<span class="number">8</span>);</span><br><span class="line">        mockQueue.setPlaceOrder(orderNumber);</span><br><span class="line"></span><br><span class="line">        DeferredResult&lt;String&gt; result=<span class="keyword">new</span> DeferredResult&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        deferredResultHolder.getMap().put(orderNumber,result);</span><br><span class="line">        log.info(<span class="string">"主线程返回"</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MockQueue.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MockQueue</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String placeOrder;</span><br><span class="line">    <span class="keyword">private</span> String completeOrder;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPlaceOrder</span><span class="params">(String placeOrder)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">            log.info(<span class="string">"接到下单请求,"</span>+placeOrder);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.completeOrder=placeOrder;</span><br><span class="line">            log.info(<span class="string">"下单请求处理完毕,"</span>+placeOrder);</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// QueueListener.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueueListener</span> <span class="keyword">implements</span> <span class="title">ApplicationListener</span>&lt;<span class="title">ContextRefreshedEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> MockQueue mockQueue;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> DeferredResultHolder deferredResultHolder;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onApplicationEvent</span><span class="params">(ContextRefreshedEvent event)</span></span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(StringUtils.isNotBlank(mockQueue.getCompleteOrder()))&#123;</span><br><span class="line">                    String orderNumber=mockQueue.getCompleteOrder();</span><br><span class="line">                    log.info(<span class="string">"返回订单处理结果："</span>+orderNumber);</span><br><span class="line">                    deferredResultHolder.getMap().get(orderNumber).setResult(<span class="string">"place order"</span>);</span><br><span class="line">                    mockQueue.setCompleteOrder(<span class="keyword">null</span>);</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        Thread.sleep(<span class="number">100</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DeferredResultHolder.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeferredResultHolder</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, DeferredResult&lt;String&gt;&gt; map=<span class="keyword">new</span> HashMap&lt;String,DeferredResult&lt;String&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Map&lt;String,DeferredResult&lt;String&gt;&gt; getMap()&#123;</span><br><span class="line">        <span class="keyword">return</span> map;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setMap</span><span class="params">(Map&lt;String,DeferredResult&lt;String&gt;&gt; map)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.map=map;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="使用WebAsyncTask-异步处理请求"><a href="#使用WebAsyncTask-异步处理请求" class="headerlink" title="使用WebAsyncTask 异步处理请求"></a>使用WebAsyncTask 异步处理请求</h4><blockquote>
<p>和Callable差不多，在Callable外包一层，给WebAsyncTask设置一个超时回调，即可实现超时处理</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@RequestMapping</span>(<span class="string">"/webAsyncTask"</span>)</span><br><span class="line">   <span class="function"><span class="keyword">public</span> WebAsyncTask&lt;String&gt; <span class="title">asyncTaskTimeout</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 打印处理线程名</span></span><br><span class="line">        log.info(format(<span class="string">"请求处理线程："</span>+ currentThread().getName()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟开启一个异步任务，超时时间为10s</span></span><br><span class="line">        WebAsyncTask&lt;String&gt; asyncTask = <span class="keyword">new</span> WebAsyncTask&lt;&gt;(<span class="number">10</span> * <span class="number">1000L</span>, () -&gt; &#123;</span><br><span class="line">            log.info(format(<span class="string">"异步工作线程："</span>+ currentThread().getName()));</span><br><span class="line">            <span class="comment">// 任务处理时间5s，不超时</span></span><br><span class="line">            sleep(<span class="number">15</span> * <span class="number">1000L</span>);</span><br><span class="line">            <span class="keyword">return</span> TIME_MESSAGE;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 任务执行完成时调用该方法</span></span><br><span class="line">        asyncTask.onCompletion(() -&gt; log.info(<span class="string">"任务执行完成"</span>));</span><br><span class="line"></span><br><span class="line">        asyncTask.onTimeout(() -&gt; &#123;</span><br><span class="line">            log.info(<span class="string">"任务执行超时"</span>);</span><br><span class="line">            <span class="keyword">return</span> TIME_MESSAGE;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">"继续处理其他事情"</span>);</span><br><span class="line">        <span class="keyword">return</span> asyncTask;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="异步处理配置"><a href="#异步处理配置" class="headerlink" title="异步处理配置"></a>异步处理配置</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebConfig</span> <span class="keyword">extends</span> <span class="title">WebMvcConfigurationSupport</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configureAsyncSupport</span><span class="params">(AsyncSupportConfigurer configurer)</span></span>&#123;</span><br><span class="line">       configurer.setDefaultTimeout(<span class="number">1000</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CallableProcessingInterceptor与CallableInterceptor中的内容大致相似，但多了一些设置超时时间等方法。可以WebConfig中设置异步处理的超时时间，也可以setTaskExecutor设置相关线程池。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/1326f080/QQ20200131005837.png" alt="CallableProcessingInterceptor.java"></p>
<h2 id="异步调用的实现"><a href="#异步调用的实现" class="headerlink" title="异步调用的实现"></a>异步调用的实现</h2><p>在SpringBoot中使用异步调用是很简单的，只需要使用<code>@Async</code>注解即可实现方法的异步调用。</p>
<p>需要在启动类加入<code>@EnableAsync</code>使异步调用<code>@Async</code>注解生效。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableAsync</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter21Application</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Chapter21Application<span class="class">.<span class="keyword">class</span>, <span class="title">args</span>)</span>;</span><br><span class="line">        log.info(<span class="string">"Chapter21启动!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Async异步调用"><a href="#Async异步调用" class="headerlink" title="@Async异步调用"></a>@Async异步调用</h3><p>使用@Async很简单，只需要在需要异步执行的方法上加入此注解即可。这里创建一个控制层和一个服务层，进行简单示例下。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SyncService.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncService</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Async</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncEvent</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//休眠1s</span></span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">//log.info("异步方法输出：&#123;&#125;!", System.currentTimeMillis());</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">syncEvent</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">//log.info("同步方法输出：&#123;&#125;!", System.currentTimeMillis());</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// AsyncController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    SyncService syncService;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/async"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">doAsync</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"方法执行开始：&#123;&#125;"</span>, start);</span><br><span class="line">        <span class="comment">//调用同步方法</span></span><br><span class="line">        syncService.syncEvent();</span><br><span class="line">        <span class="keyword">long</span> syncTime = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"同步方法用时：&#123;&#125;"</span>, syncTime - start);</span><br><span class="line">        <span class="comment">//调用异步方法</span></span><br><span class="line">        syncService.asyncEvent();</span><br><span class="line">        <span class="keyword">long</span> asyncTime = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"异步方法用时：&#123;&#125;"</span>, asyncTime - syncTime);</span><br><span class="line">        log.info(<span class="string">"方法执行完成：&#123;&#125;!"</span>,asyncTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"async!!!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义线程池"><a href="#自定义线程池" class="headerlink" title="自定义线程池"></a>自定义线程池</h3><p>在默认情况下，系统使用的是默认的SimpleAsyncTaskExecutor进行线程创建。所以一般上我们会自定义线程池来进行线程的复用。</p>
<p>关于ThreadPoolTaskExecutor参数说明：</p>
<ul>
<li><strong>corePoolSize</strong>：线程池维护线程的最少数量</li>
<li><strong>keepAliveSeconds</strong>：允许的空闲时间,当超过了核心线程出之外的线程在空闲时间到达之后会被销毁</li>
<li><strong>maxPoolSize</strong>：线程池维护线程的最大数量,只有在缓冲队列满了之后才会申请超过核心线程数的线程</li>
<li><strong>queueCapacity</strong>：缓存队列</li>
<li><strong>rejectedExecutionHandler</strong>：线程池对拒绝任务（无线程可用）的处理策略。这里采用了CallerRunsPolicy策略，当线程池没有处理能力的时候，该策略会直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务。还有一个是AbortPolicy策略：处理程序遭到拒绝将抛出运行时RejectedExecutionException。</li>
</ul>
<p>而在一些场景下，若需要在关闭线程池时等待当前调度任务完成后才开始关闭，可以通过简单的配置，进行优雅的停机策略配置。关键就是通过setWaitForTasksToCompleteOnShutdown(true)和setAwaitTerminationSeconds方法。</p>
<ul>
<li><strong>setWaitForTasksToCompleteOnShutdown</strong>: 表明等待所有线程执行完，默认为false。</li>
<li><strong>setAwaitTerminationSeconds</strong>: 等待的时间，因为不能无限的等待下去。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Config.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Config</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置线程池</span></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"asyncPoolTaskExecutor"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ThreadPoolTaskExecutor <span class="title">getAsyncThreadPoolTaskExecutor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ThreadPoolTaskExecutor taskExecutor = <span class="keyword">new</span> ThreadPoolTaskExecutor();</span><br><span class="line">        taskExecutor.setCorePoolSize(<span class="number">20</span>);</span><br><span class="line">        taskExecutor.setMaxPoolSize(<span class="number">200</span>);</span><br><span class="line">        taskExecutor.setQueueCapacity(<span class="number">25</span>);</span><br><span class="line">        taskExecutor.setKeepAliveSeconds(<span class="number">200</span>);</span><br><span class="line">        taskExecutor.setThreadNamePrefix(<span class="string">"oKong-"</span>);</span><br><span class="line">        <span class="comment">// 线程池对拒绝任务（无线程可用）的处理策略，目前只支持AbortPolicy、CallerRunsPolicy；默认为后者</span></span><br><span class="line">        taskExecutor.setRejectedExecutionHandler(<span class="keyword">new</span> ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class="line">        taskExecutor.initialize();</span><br><span class="line">        <span class="keyword">return</span> taskExecutor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SyncService.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncService</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Async</span>(<span class="string">"asyncPoolTaskExecutor"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncEvent</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//休眠1s</span></span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        log.info(<span class="string">"异步方法内部线程名称：&#123;&#125;!"</span>, Thread.currentThread().getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="异步回调及超时处理"><a href="#异步回调及超时处理" class="headerlink" title="异步回调及超时处理"></a>异步回调及超时处理</h3><blockquote>
<p>对于一些业务场景下，需要异步回调的返回值时，就需要使用异步回调来完成了。主要就是通过Future进行异步回调。</p>
</blockquote>
<h4 id="异步回调"><a href="#异步回调" class="headerlink" title="异步回调"></a>异步回调</h4><ol>
<li>修改下异步方法的返回类型，加入Future。</li>
<li>其中AsyncResult是Spring提供的一个Future接口的子类。</li>
<li>然后通过isDone方法，判断是否已经执行完毕。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SyncService.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncService</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Async</span>(<span class="string">"asyncPoolTaskExecutor"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Future&lt;String&gt; <span class="title">asyncEvent</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//休眠1s</span></span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        log.info(<span class="string">"异步方法内部线程名称：&#123;&#125;!"</span>, Thread.currentThread().getName());</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> AsyncResult&lt;&gt;(<span class="string">"异步方法返回值"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// AsyncController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/async"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">doAsync</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"方法执行开始：&#123;&#125;"</span>, start);</span><br><span class="line">        <span class="comment">//调用同步方法</span></span><br><span class="line">        syncService.syncEvent();</span><br><span class="line">        <span class="keyword">long</span> syncTime = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"同步方法用时：&#123;&#125;"</span>, syncTime - start);</span><br><span class="line">        <span class="comment">//调用异步方法</span></span><br><span class="line">        Future&lt;String&gt; doFutrue = syncService.asyncEvent();</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">//判断异步任务是否完成</span></span><br><span class="line">            <span class="keyword">if</span>(doFutrue.isDone()) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">long</span> asyncTime = System.currentTimeMillis();</span><br><span class="line">        log.info(<span class="string">"异步方法用时：&#123;&#125;"</span>, asyncTime - syncTime);</span><br><span class="line">        log.info(<span class="string">"方法执行完成：&#123;&#125;!"</span>,asyncTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"async!!!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="超时处理"><a href="#超时处理" class="headerlink" title="超时处理"></a>超时处理</h4><blockquote>
<p>对于一些需要异步回调的函数，不能无期限的等待下去，所以一般上需要设置超时时间，超时后可将线程释放，而不至于一直堵塞而占用资源。</p>
</blockquote>
<ol>
<li>对于Future配置超时，很简单，通过get方法即可；</li>
<li>超时后，会抛出异常TimeoutException类，此时可进行统一异常捕获即可。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//get方法会一直堵塞，直到等待执行完成才返回</span></span><br><span class="line"><span class="comment">//get(long timeout, TimeUnit unit) 在设置时间类未返回结果，会直接排除异常TimeoutException，messages为null</span></span><br><span class="line">String result = doFutrue.get(<span class="number">60</span>, TimeUnit.SECONDS);<span class="comment">//60s</span></span><br></pre></td></tr></table></figure>
<h2 id="异步请求与异步调用的区别"><a href="#异步请求与异步调用的区别" class="headerlink" title="异步请求与异步调用的区别"></a>异步请求与异步调用的区别</h2><p>两者的使用场景不同，异步请求用来解决并发请求对服务器造成的压力，从而提高对请求的吞吐量；而异步调用是用来做一些非主线流程且不需要实时计算和响应的任务，比如同步日志到kafka中做日志分析等。<br>异步请求是会一直等待response相应的，需要返回结果给客户端的；而异步调用我们往往会马上返回给客户端响应，完成这次整个的请求，至于异步调用的任务后台自己慢慢跑就行，客户端不会关心。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>async</tag>
        <tag>runnable</tag>
        <tag>deferredresult</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
      </tags>
  </entry>
  <entry>
    <title>Java常用的工具类库</title>
    <url>/p/2020/01/30/a2c9f3de/</url>
    <content><![CDATA[<p>总结一些Java常用的工具类库，提高工作效率。</p>
<a id="more"></a>
<h1 id="Apache-Commons"><a href="#Apache-Commons" class="headerlink" title="Apache Commons"></a>Apache Commons</h1><h2 id="BeanUtils"><a href="#BeanUtils" class="headerlink" title="BeanUtils"></a>BeanUtils</h2><h2 id="Codec"><a href="#Codec" class="headerlink" title="Codec"></a>Codec</h2><h2 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h2><h2 id="I-O"><a href="#I-O" class="headerlink" title="I/O"></a>I/O</h2><h2 id="Lang"><a href="#Lang" class="headerlink" title="Lang"></a>Lang</h2><h1 id="Google-Guava"><a href="#Google-Guava" class="headerlink" title="Google Guava"></a>Google Guava</h1><h1 id="集成类"><a href="#集成类" class="headerlink" title="集成类"></a>集成类</h1><h2 id="Hutool"><a href="#Hutool" class="headerlink" title="Hutool"></a>Hutool</h2><blockquote>
<p><a href="https://hutool.cn/docs/#/" target="_blank" rel="noopener">https://hutool.cn/docs/#/</a></p>
</blockquote>
<p>Hutool是一个小而全的Java工具类库，通过静态方法封装，降低相关API的学习成本，提高工作效率，使Java拥有函数式语言般的优雅，让Java语言也可以“甜甜的”。<br>Hutool中的工具方法来自于每个用户的精雕细琢，它涵盖了Java开发底层代码中的方方面面，它既是大型项目开发中解决小问题的利器，也是小型项目中的效率担当；<br>Hutool是项目中“util”包友好的替代，它节省了开发人员对项目中公用类和公用工具方法的封装时间，使开发专注于业务，同时可以最大限度的避免封装不完善带来的bug。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.hutool<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hutool-all<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="包含组件"><a href="#包含组件" class="headerlink" title="包含组件"></a>包含组件</h3><p>一个Java基础工具类，对文件、流、加密解密、转码、正则、线程、XML等JDK方法进行封装，组成各种Util工具类，同时提供以下组件：</p>
<table>
<thead>
<tr>
<th>模块</th>
<th style="text-align:left">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td>hutool-aop</td>
<td style="text-align:left">JDK动态代理封装，提供非IOC下的切面支持</td>
</tr>
<tr>
<td>hutool-bloomFilter</td>
<td style="text-align:left">布隆过滤，提供一些Hash算法的布隆过滤</td>
</tr>
<tr>
<td>hutool-cache</td>
<td style="text-align:left">简单缓存实现</td>
</tr>
<tr>
<td>hutool-core</td>
<td style="text-align:left">核心，包括Bean操作、日期、各种Util等</td>
</tr>
<tr>
<td>hutool-cron</td>
<td style="text-align:left">定时任务模块，提供类Crontab表达式的定时任务</td>
</tr>
<tr>
<td>hutool-crypto</td>
<td style="text-align:left">加密解密模块，提供对称、非对称和摘要算法封装</td>
</tr>
<tr>
<td>hutool-db</td>
<td style="text-align:left">JDBC封装后的数据操作，基于ActiveRecord思想</td>
</tr>
<tr>
<td>hutool-dfa</td>
<td style="text-align:left">基于DFA模型的多关键字查找</td>
</tr>
<tr>
<td>hutool-extra</td>
<td style="text-align:left">扩展模块，对第三方封装（模板引擎、邮件、Servlet、二维码、Emoji、FTP、分词等）</td>
</tr>
<tr>
<td>hutool-http</td>
<td style="text-align:left">基于HttpUrlConnection的Http客户端封装</td>
</tr>
<tr>
<td>hutool-log</td>
<td style="text-align:left">自动识别日志实现的日志门面</td>
</tr>
<tr>
<td>hutool-script</td>
<td style="text-align:left">脚本执行封装，例如Javascript</td>
</tr>
<tr>
<td>hutool-setting</td>
<td style="text-align:left">功能更强大的Setting配置文件和Properties封装</td>
</tr>
<tr>
<td>hutool-system</td>
<td style="text-align:left">系统参数调用封装（JVM信息等）</td>
</tr>
<tr>
<td>hutool-json</td>
<td style="text-align:left">JSON实现</td>
</tr>
<tr>
<td>hutool-captcha</td>
<td style="text-align:left">图片验证码实现</td>
</tr>
<tr>
<td>hutool-poi</td>
<td style="text-align:left">针对POI中Excel的封装</td>
</tr>
<tr>
<td>hutool-socket</td>
<td style="text-align:left">基于Java的NIO和AIO的Socket封装</td>
</tr>
</tbody>
</table>
<p>可以根据需求对每个模块单独引入，也可以通过引入hutool-all方式引入所有模块。</p>
<h2 id="vjkit"><a href="#vjkit" class="headerlink" title="vjkit"></a>vjkit</h2><blockquote>
<p><a href="https://github.com/vipshop/vjtools/tree/master/vjkit" target="_blank" rel="noopener">https://github.com/vipshop/vjtools/tree/master/vjkit</a></p>
</blockquote>
<p>唯品会Java开发基础类库，综合各门各派众多开源类库的精华而成， 让开发人员避免底层代码的重复开发，默认就拥有最佳实践，尤其在性能的方面。</p>
<p>综合众多开源类库的精华而成， 让开发人员避免底层代码的重复开发，默认就拥有最佳实践，尤其在性能的方面。<br>针对“基础，文本，数字，日期，文件，集合，并发，反射”这些开发人员的日常，VJKit做了两件事情：</p>
<ol>
<li>对Guava 与Common Lang中最常用的API的提炼归类，避免了大家直面茫茫多的API(但有些工具类如Guava Cache还是建议直接使用，详见直用三方工具类 )</li>
<li>对各门各派的精华的借鉴移植：比如一些大项目的附送基础库： Netty，ElasticSearch， 一些专业的基础库 ： Jodd, commons-io, commons-collections； 一些大厂的基础库：Facebook JCommon，twitter commons</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.vip.vjtools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>vjkit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>tools</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>控制反转（IoC）、依赖注入（DI）、依赖查找（DL）</title>
    <url>/p/2020/02/06/2dc6b46a/</url>
    <content><![CDATA[<h1 id="控制反转（Inversion-of-Control，IoC）"><a href="#控制反转（Inversion-of-Control，IoC）" class="headerlink" title="控制反转（Inversion of Control，IoC）"></a>控制反转（Inversion of Control，IoC）</h1><blockquote>
<p>在Java开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。</p>
</blockquote>
<p>IoC不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。</p>
<p>其实IoC对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在IoC/DI思想中，应用程序就变成被动的了，被动的等待IoC容器来创建并注入它所需要的资源了。</p>
<p>IoC很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”；即由IoC容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。</p>
<p>理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”。</p>
<ul>
<li><p><strong>谁控制谁，控制什么</strong>：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。</p>
</li>
<li><p><strong>为何是反转，哪些方面反转了</strong>：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。</p>
</li>
</ul>
<a id="more"></a>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/2dc6b46a/page_2.png" alt="IoC图例说明"></p>
<h1 id="依赖注入（Dependency-Injection，DI）"><a href="#依赖注入（Dependency-Injection，DI）" class="headerlink" title="依赖注入（Dependency Injection，DI）"></a>依赖注入（Dependency Injection，DI）</h1><p>​ DI—Dependency Injection，即“依赖注入”：是组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。</p>
<p>理解DI的关键是：“<strong>谁依赖谁，为什么需要依赖，谁注入谁，注入了什么</strong>”，那我们来深入分析一下：</p>
<ul>
<li><strong>谁依赖于谁</strong>：当然是应用程序依赖于IoC容器；</li>
<li><strong>为什么需要依赖</strong>：应用程序需要IoC容器来提供对象需要的外部资源；</li>
<li><strong>谁注入谁</strong>：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象；</li>
<li><strong>注入了什么</strong>：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）</li>
</ul>
<p>本质上IoC和DI是同一思想下不同维度的表现 ,用通俗的话说就是，IoC是bean的注册，DI是bean的初始化</p>
<h2 id="IoC和DI"><a href="#IoC和DI" class="headerlink" title="IoC和DI"></a>IoC和DI</h2><p>IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。</p>
<h1 id="依赖查找（Dependency-Lookup，DL）"><a href="#依赖查找（Dependency-Lookup，DL）" class="headerlink" title="依赖查找（Dependency Lookup，DL）"></a>依赖查找（Dependency Lookup，DL）</h1><p>依赖查找（Dependency Lookup，简称 DL），它是控制反转设计原则的一种实现方式。它的大体思路是：容器中的受控对象通过容器的 API 来查找自己所依赖的资源和协作对象。这种方式虽然降低了对象间的依赖，但是同时也使用到了容器的 API，造成了我们无法在容器外使用和测试对象。依赖查找是一种更加传统的 IOC 实现方式。</p>
<p>依赖查找也有两种方式：</p>
<p>依赖拖拽：注入的对象如何与组件发生联系，这个过程就是通过依赖拖拽实现；<br>上下文依赖查找：在某些方面跟依赖拖拽类似，但是上下文依赖查找中，查找的过程是在容器管理的资源中进行的，而不是从集中注册表中，并且通常是作用在某些设置点上；（JNDI）</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Design Patterns</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>design patterns</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot 自定义异常类</title>
    <url>/p/2020/01/30/4f18b206/</url>
    <content><![CDATA[<p>有的时候，我们需要业务逻辑时抛出自定义异常，这个时候需要自定义业务异常类。</p>
<ul>
<li><strong>Exception</strong>：受检查的异常，这种异常是强制我们catch或throw的异常。你遇到这种异常必须进行catch或throw，如果不处理，编译器会报错。比如：IOException。</li>
<li><strong>RuntimeException</strong>：运行时异常，这种异常我们不需要处理，完全由虚拟机接管。比如我们常见的NullPointerException，我们在写程序时不会进行catch或throw。<br>RuntimeException也是继承自Exception的，只是虚拟机对这两种异常进行了区分。</li>
</ul>
<a id="more"></a>
<h2 id="常见异常类型"><a href="#常见异常类型" class="headerlink" title="常见异常类型"></a>常见异常类型</h2><h3 id="常见的RuntimeException类型的异常"><a href="#常见的RuntimeException类型的异常" class="headerlink" title="常见的RuntimeException类型的异常"></a>常见的RuntimeException类型的异常</h3><ul>
<li><strong>ArithmeticException</strong>：数学计算异常。</li>
<li><strong>NullPointerException</strong>：空指针异常。</li>
<li><strong>NegativeArraySizeException</strong>：负数组长度异常。</li>
<li><strong>ArrayOutOfBoundsException</strong>：数组索引越界异常。</li>
<li><strong>ClassNotFoundException</strong>：类文件未找到异常。</li>
<li><strong>ClassCastException</strong>：类型强制转换异常。</li>
<li><strong>SecurityException</strong>：违背安全原则异常。</li>
</ul>
<h3 id="其他非RuntimeException类型的常见异常"><a href="#其他非RuntimeException类型的常见异常" class="headerlink" title="其他非RuntimeException类型的常见异常"></a>其他非RuntimeException类型的常见异常</h3><ul>
<li><strong>NoSuchMethodException</strong>：方法未找到异常。</li>
<li><strong>IOException</strong>：输入输出异常。</li>
<li><strong>EOFException</strong>：文件已结束异常。</li>
<li><strong>FileNotFoundException</strong>：文件未找到异常。</li>
<li><strong>NumberFormatException</strong>：字符串转换为数字异常。</li>
<li><strong>SQLException</strong>：操作数据库异常</li>
</ul>
<h2 id="SpringBoot默认的错误处理机制"><a href="#SpringBoot默认的错误处理机制" class="headerlink" title="SpringBoot默认的错误处理机制"></a>SpringBoot默认的错误处理机制</h2><p>在浏览器中访问错误请求时，会返回一个状态码以及一个错误页面；当用App访问错误请求时，在Body中返回的是一个json，json中的status为状态码。<br>SpringBoot中是通过如下源代码来实现不同的请求有不同的相应。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4f18b206/QQ20200130202618.png" alt="BasicErrorController.java"></p>
<h2 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h2><ol>
<li>需要编写一个exception类继承RuntimeException类；</li>
<li>编写一个handler类处理controller层抛出的异常；</li>
<li>在controller的请求方法中抛出这个异常。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserNotExistException.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@EqualsAndHashCode</span>(callSuper=<span class="keyword">false</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserNotExistException</span> <span class="keyword">extends</span> <span class="title">RuntimeException</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">3762363757873984368L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserNotExistException</span><span class="params">(String id)</span></span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="string">"user not exist"</span>);</span><br><span class="line">        <span class="keyword">this</span>.id=id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ControllerExceptionHandler.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@ControllerAdvice</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ControllerExceptionHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ExceptionHandler</span>(UserNotExistException<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    @<span class="title">ResponseBody</span></span></span><br><span class="line"><span class="class">    @<span class="title">ResponseStatus</span>(<span class="title">HttpStatus</span>.<span class="title">INTERNAL_SERVER_ERROR</span>)</span></span><br><span class="line"><span class="class">    <span class="title">public</span> <span class="title">Map</span>&lt;<span class="title">String</span>,<span class="title">Object</span>&gt; <span class="title">handlerUserNotExistException</span>(<span class="title">UserNotExistException</span> <span class="title">ex</span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">        Map&lt;String,Object&gt; result=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        result.put(<span class="string">"id"</span>,ex.getId());</span><br><span class="line">        result.put(<span class="string">"message"</span>,ex.getMessage());</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/user/&#123;id:\\d+&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> User <span class="title">getErrorInfo</span><span class="params">(@PathVariable String id)</span></span>&#123;</span><br><span class="line"><span class="comment">//       throw  new RuntimeException("user not exist");</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UserNotExistException(id);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Spring相关常用注解"><a href="#Spring相关常用注解" class="headerlink" title="Spring相关常用注解"></a>Spring相关常用注解</h2><h3 id="ControllerAdvice"><a href="#ControllerAdvice" class="headerlink" title="@ControllerAdvice"></a>@ControllerAdvice</h3><p>这是一个增强的 Controller。可以实现三个方面的功能：</p>
<ul>
<li><strong>全局异常处理</strong>：结合@ExceptionHandler</li>
<li><strong>全局数据绑定</strong>：结合@ModelAttribute。全局数据绑定功能可以用来做一些初始化的数据操作，我们可以将一些公共的数据定义在添加了 @ControllerAdvice 注解的类中，这样，在每一个 Controller 的接口中，就都能够访问导致这些数据。</li>
<li><strong>全局数据预处理</strong>：结合@InitBinder</li>
</ul>
<p>如果全部异常处理都返回json，那么可以使用 @RestControllerAdvice 代替 @ControllerAdvice ，这样在方法上就可以不需要添加 @ResponseBody。@RestControllerAdvice在注解上已经添加了@ResponseBody。</p>
<h3 id="ExceptionHandler"><a href="#ExceptionHandler" class="headerlink" title="@ExceptionHandler"></a>@ExceptionHandler</h3><blockquote>
<p>用来指明异常的处理类型</p>
</blockquote>
<p>需要注意的是使用@ExceptionHandler注解传入的参数可以一个数组，且使用该注解时，传入的参数不能相同，也就是不能使用两个@ExceptionHandler去处理同一个异常。如果传入参数相同，则初始化ExceptionHandler时会失败。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security 认证流程源码详解</title>
    <url>/p/2020/02/01/e2629cf2/</url>
    <content><![CDATA[<p>项目启动后会自动寻找<code>UserDetailsService</code>实现类；<br>执行<code>UserDetailsService</code>的唯一方法<code>loadUserByName(String username)</code>并返回<code>UserDetail</code>类，注意，返回的<code>UserDetail</code>是根据用户名去数据库查询到用户信息；<br>拿到<code>UserDetail</code>后会对<code>UserDetail</code>进行一个预检查，检查<strong>用户是否存在，是否被锁定等等等</strong>；<br>全部认证成功后会调用<code>AuthenticationSuccess</code>成功处理类，失败则调用<code>AuthenticationFailHandler</code>类；<br>此时对于前后端分离项目而言，调用成功处理类，将验证结果返回给前端，前台拿到返回信息后，保存token致本地，然后每次请求都会拼接到head中。</p>
<h2 id="认证处理流程"><a href="#认证处理流程" class="headerlink" title="认证处理流程"></a>认证处理流程</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/page_2.png" alt="认证处理流程"></p>
<a id="more"></a> 
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200201225058.png" alt="UsernamePasswordAuthenticationFilter.java"></p>
<ol>
<li>获取用户名与密码。</li>
<li>拿用户名与密码构建<code>UsernamePasswordAuthenticationToken</code>对象。</li>
<li>实例化<code>UsernamePasswordAuthenticationToken</code>之后调用了<code>setDetails(request,authRequest)</code>将请求的信息设到<code>UsernamePasswordAuthenticationToken</code>中去，包括ip、session等内。</li>
<li>然后去调用<code>AuthenticationManager</code>，<code>AuthenticationManager</code>本身不包含验证的逻辑，它的作用是用来管理<code>AuthenticationProvider</code>。</li>
</ol>
<p><code>UsernamePasswordAuthenticationToken</code>对象其实是<code>Authentication</code>接口的实现，<code>Authentication</code>封装的是用户的验证信息。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200201230348.png" alt="UsernamePasswordAuthenticationToken.java：setAuthenticated(false)"></p>
<ol>
<li>用户名与密码设为本地变量</li>
<li><code>super((Collection)null);</code>中collection代表权限列表，在这传了一个<code>null</code>进去是因为刚开始并没有进行认证，因此用户此时没有任何权限，并且设置没有认证的信息<code>setAuthenticated(false)</code>；</li>
</ol>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200201235300.png" alt="ProviderManager.java"></p>
<h3 id="校验逻辑"><a href="#校验逻辑" class="headerlink" title="校验逻辑"></a>校验逻辑</h3><p>真正的校验逻辑写在<code>AuthenticationProvider</code>中，</p>
<ol>
<li>遍历<code>Provider</code>集合。因为不同的登陆方式认证逻辑是不一样的，可能是微信等社交平台登陆，也可能是用户名密码登陆。<code>AuthenticationManager</code>其实是将<code>AuthenticationProvider</code>收集起来，然后登陆的时候挨个去<code>AuthenticationProvider</code>中问你这种验证逻辑支不支持此次登陆的方式，根据传进来的<code>Authentication</code>类型会挑出一个适合的<code>Provider</code>来进行校验处理。</li>
<li>挑一种<code>Provider</code>进行判断，调用<code>Provider.supports()</code>方法，判断<code>Provider</code>是否支持<code>authentication.getClass()</code>的类型。</li>
<li>真正的执行校验逻辑。</li>
</ol>
<p><code>provider.authenticate(authentication)</code>中，<code>authenticate</code>是<code>DaoAuthenticationProvider</code>类中的一个方法。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203123932.png" alt="DaoAuthenticationProvider.java"></p>
<p><code>DaoAuthenticationProvider</code>继承了<code>AbstractUserDetailsAuthenticationProvider</code>。实际上<code>authenticate</code>的校验逻辑写在了<code>AbstractUserDetailsAuthenticationProvider</code>抽象类中。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203124253.png" alt="AbstractUserDetailsAuthenticationProvider.java-1"><br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203124343.png" alt="AbstractUserDetailsAuthenticationProvider.java-2"></p>
<ol>
<li>首先实例化<code>UserDetails</code>对象，调用了<code>retrieveUser</code>方法获取到了一个<code>user</code>对象，<code>retrieveUser</code>是一个抽象方法。</li>
<li>如果没拿到信息就会抛出异常，如果查到了就会去调用<code>preAuthenticationChecks</code>的<code>check(user)</code>方法去进行预检查。在预检查中进行了三个检查，因为<code>UserDetail</code>类中有四个布尔类型，去检查其中的三个，<strong>用户是否锁定</strong>、<strong>用户是否过期</strong>，<strong>用户是否可用</strong>。</li>
<li>预检查之后紧接着去调用了<code>additionalAuthenticationChecks</code>方法去进行附加检查，这个方法也是一个抽象方法，<strong>检查密码是否匹配</strong>，在<code>DaoAuthenticationProvider</code>中去具体实现，在里面进行了加密解密去校验当前的密码是否匹配。</li>
<li>如果通过了预检查和附加检查，还会进行厚检查，检查4个布尔中的最后一个，<strong>检查身份认证是否已过期</strong>。</li>
<li>所有的检查都通过，则认为用户认证是成功的。用户认证成功之后，会将这些认证信息和user传递进去，调用<code>createSuccessAuthentication</code>方法。</li>
</ol>
<p>在这个方法中同样会实例化一个user，但是这个方法不会调用之前传两个参数的函数，而是会调用三个参数的构造函数。这个时候，在调<code>super</code>的构造函数中不会再传<code>null</code>，会将<code>authorities</code>权限设进去，之后将用户密码设进去，最后<code>setAuthenticated(true)</code>，代表验证已经通过。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203132312.png" alt="UsernamePasswordAuthenticationToken.java：setAuthenticated(true)"></p>
<p>最后创建一个<code>authentication</code>会沿着验证的这条线返回回去。</p>
<h3 id="认证成功"><a href="#认证成功" class="headerlink" title="认证成功"></a>认证成功</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203140345.png" alt="AbstractAuthenticationProcessingFilter.java：doFilter()"></p>
<p>在拿到<code>authentication</code>后，会调用<code>successHandler.onAuthenticationSuccess(request, response, authResult);</code>，这个就是住在调用我们自己写的认证成功的处理器。<br>如果验证成功，则在这条路中调用我们系统的业务逻辑。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203135248.png" alt="AbstractAuthenticationProcessingFilter.java：successfulAuthentication()"></p>
<h3 id="认证失败"><a href="#认证失败" class="headerlink" title="认证失败"></a>认证失败</h3><p>如果在任何一处发生问题，就会抛出异常，在AbstractAuthenticationProcessingFilter中进行捕获，然后进行<code>unsuccessfulAuthentication</code>，这里调用<code>failureHandler.onAuthenticationFailure(request, response, failed);</code>，也就是调用我们自己定义的认证失败的处理器。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203135322.png" alt="AbstractAuthenticationProcessingFilter.java：unsuccessfulAuthentication()"></p>
<h2 id="认证结果如何在多个请求之间共享"><a href="#认证结果如何在多个请求之间共享" class="headerlink" title="认证结果如何在多个请求之间共享"></a>认证结果如何在多个请求之间共享</h2><blockquote>
<p>多个请求之间共享肯定是放在session中，但是它是什么时候，把什么东西放到了session中，什么时候在session中读出来。</p>
</blockquote>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/page_1.png" alt="认证结果共享流程"></p>
<h3 id="什么时候共享"><a href="#什么时候共享" class="headerlink" title="什么时候共享"></a>什么时候共享</h3><p>在验证成功最后会调用我们自定义的<code>successHandler</code>登陆成功处理器，在调用这个方法之前会调用<code>SecurityContextHolder.getContext().setAuthentication(authResult);</code>，会将我们验证成功的那个<code>Authentication</code>放到<code>SecurityContext</code>中，然后再放到<code>SecurityContextHolder</code>中。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/QQ20200203140737.png" alt="AbstractAuthenticationProcessingFilter.java：successfulAuthentication()"></p>
<p><code>SecurityContextImpl</code>是SecurityContext的一个实现类，它包装了<code>authentication</code>，重写了<code>hashcode</code>方法和<code>equals</code>方法去保证<code>authentication</code>的唯一。</p>
<h3 id="怎么共享"><a href="#怎么共享" class="headerlink" title="怎么共享"></a>怎么共享</h3><p><code>SecurityContextHolder</code>是<code>ThreadLocal</code>的一个封装，<code>ThreadLocal</code>是线程绑定的一个map，在同一个线程里在这个方法里往<code>ThreadLocal</code>里设置的变量是可以在另一个线程中读取到的。可以理解为<code>SecurityContextHolder</code>是一个线程级的全局变量，在一个线程中操作<code>ThreadLocal</code>中的数据会影响另一个线程。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/page_4.png" alt="SpringSecurity过滤器链"></p>
<p>从<code>setAuthentication(authResult)</code>，将<code>authentication</code>放在当前的线程<code>SecurityContextHolder</code>中去，在整个认证处理过程中，在任何一个方法里，通过<code>SecurityContextHolder</code>的静态方法，都能讲<code>authentication</code>读出来。</p>
<h3 id="谁来用"><a href="#谁来用" class="headerlink" title="谁来用"></a>谁来用</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/e2629cf2/page_3.png" alt="SpringSecurity过滤器链"></p>
<p>登录成功后，登录所有的请求都会通过<code>SecurityContextPersisenceFilter</code>去<code>SecurityContextHolder</code>拿那个<code>authentication</code>。<code>SecurityContextPersisenceFilter</code>在整个过滤器的最前面。</p>
<p><code>SecurityContextPersisenceFilter</code>的作用：</p>
<ol>
<li>当请求进入过滤器链时，先进它，检查<code>session</code>是否有<code>securityContext</code>。如果有，就把从<code>securityContext</code>中从<code>session</code>取出，放到线程中。如果没有，则跳过。</li>
<li>当响应最后过它时，检查线程。如果线程中有<code>securityContext</code>，就发出来放到<code>session</code>中去。</li>
</ol>
<p>这样不同的请求，可以从线程中拿到相同的用户认证信息。整个请求与相应的过程都是在一个线程中完成的，因此在线程的其他位置，随时可以用<code>SecurityContextHolder</code>来拿到用户认证信息。</p>
<h2 id="获取认证用户信息"><a href="#获取认证用户信息" class="headerlink" title="获取认证用户信息"></a>获取认证用户信息</h2><blockquote>
<p>如何用<code>SecurityContextHolder</code>来拿到用户认证信息</p>
</blockquote>
<p>可以使用SecurityContextHolder去获取用户的认证信息。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/me"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">getCurrentUser</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SecurityContextHolder.getContext().getAuthentication();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以直接获取<code>authentication</code>。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/me"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">getCurrentUser</span><span class="params">(Authentication authentication)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> authentication;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果不需要返回全部的用户信息，可以使用<code>@AuthenticationPrincipal</code>注解，返回部分用户信息。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/me"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">getCurrentUser</span><span class="params">(@AuthenticationPrincipal UserDetails userDetails)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> userDetails;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>authentication</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义Hibenate Validator</title>
    <url>/p/2020/01/30/c60a51d1/</url>
    <content><![CDATA[<p>Spring中使用@vaild 注解进行验证传过来的参数校验，然后通过统一异常处理。如@not null、@not blank、@not empty、@Email等等。但是实际业务中可能默认提供的注解不够用，因此需要自定义验证注解。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c60a51d1/20200130195331.png" alt="MyConstraintValidator.java"></p>
<p>实现ConstraintValidator接口，它有两个泛型，如上图</p>
<ol>
<li>自定义的注解类，</li>
<li>要验证的数据的类型(例如写了String类型的数据，那么这个注解就要放在String类型的字段上才会起作用，若写成Object，那么它可以接收任何数据类型的数据)。</li>
<li>initialize是初始化方法；</li>
<li>isValid为验证的逻辑方法，返回true,则验证通过，否则不通过。</li>
</ol>
<p>在这个实现中，可以用@Autowire注入需要的类。</p>
<p>MyConstraint需要有message()、groups()、payload()三个属性。</p>
<a id="more"></a>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// MyConstraint.java</span></span><br><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD,ElementType.FIELD&#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Constraint</span>(validatedBy = MyConstraintValidator<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class"><span class="title">public</span> @<span class="title">interface</span> <span class="title">MyConstraint</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">message</span><span class="params">()</span> <span class="keyword">default</span> "MyConstraint <span class="keyword">default</span> msg"</span>;</span><br><span class="line"></span><br><span class="line">    Class&lt;?&gt;[] groups() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    Class&lt;? extends Payload&gt;[] payload() <span class="keyword">default</span> &#123;&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MyConstraintValidator.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConstraintValidator</span> <span class="keyword">implements</span> <span class="title">ConstraintValidator</span>&lt;<span class="title">MyConstraint</span>,<span class="title">Object</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> HelloService helloService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MyConstraint constraintAnnotation)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"my validator init"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(Object value, ConstraintValidatorContext context)</span> </span>&#123;</span><br><span class="line">        helloService.greeting(<span class="string">"tom"</span>);</span><br><span class="line">        System.out.println(value);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// User.java</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@MyConstraint</span>(message = <span class="string">"这是一个测试"</span>)</span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@NotBlank</span>(message = <span class="string">"密码不能为空"</span>)</span><br><span class="line">    <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Past</span>(message = <span class="string">"生日必须是过去的时间"</span>)</span><br><span class="line">    <span class="keyword">private</span> Date birthday;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Target"><a href="#Target" class="headerlink" title="@Target"></a>@Target</h2><blockquote>
<p>注解的作用目标</p>
</blockquote>
<ul>
<li><strong>@Target(ElementType.TYPE)</strong>：接口、类、枚举、注解</li>
<li><strong>@Target(ElementType.FIELD)</strong>：字段、枚举的常量</li>
<li><strong>@Target(ElementType.METHOD)</strong>：方法</li>
<li><strong>@Target(ElementType.PARAMETER)</strong>：方法参数</li>
<li><strong>@Target(ElementType.CONSTRUCTOR)</strong>：构造函数</li>
<li><strong>@Target(ElementType.LOCAL_VARIABLE)</strong>：局部变量</li>
<li><strong>@Target(ElementType.ANNOTATION_TYPE)</strong>：注解</li>
<li><strong>@Target(ElementType.PACKAGE)</strong>：包</li>
</ul>
<h2 id="Retention"><a href="#Retention" class="headerlink" title="@Retention"></a>@Retention</h2><blockquote>
<p>注解的保留位置</p>
</blockquote>
<ul>
<li><strong>RetentionPolicy.SOURCE</strong>: 这种类型的Annotations只在源代码级别保留,编译时就会被忽略,在class字节码文件中不包含。</li>
<li><strong>RetentionPolicy.CLASS</strong>: 这种类型的Annotations编译时被保留,默认的保留策略,在class文件中存在,但JVM将会忽略,运行时无法获得。</li>
<li><strong>RetentionPolicy.RUNTIME</strong>: 这种类型的Annotations将被JVM保留,所以他们能在运行时被JVM或其他使用反射机制的代码所读取和使用。</li>
</ul>
<h2 id="Constraint"><a href="#Constraint" class="headerlink" title="@Constraint"></a>@Constraint</h2><blockquote>
<p>来限定自定义注解的方法<br><code>@Constraint(validatedBy = MyConstraintValidator.class)</code><br><code>@Constraint(validatedBy = {MyConstraintValidator.StringChecker.class, MyConstraintValidator.LongChecker.class})</code><br>可以代表一个或者多个校验处理逻辑。</p>
</blockquote>
<h2 id="Document"><a href="#Document" class="headerlink" title="@Document"></a>@Document</h2><blockquote>
<p>说明该注解将被包含在javadoc中</p>
</blockquote>
<h2 id="Inherited"><a href="#Inherited" class="headerlink" title="@Inherited"></a>@Inherited</h2><blockquote>
<p>说明子类可以继承父类中的该注解</p>
</blockquote>
]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>valid</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot 拦截机制</title>
    <url>/p/2020/01/30/c4f6b17e/</url>
    <content><![CDATA[<p>在日常的项目中，我们经常需要对请求进行拦截然后进行一些逻辑操作，拦截的方式有三种，分别是Filter、Interceptor和Aspect。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/20190603215352885.png" alt="拦截顺序"></p>
<p>Spring中的拦截机制，如果出现异常的话，异常的顺序是从里面到外面一步一步的进行处理，如果到了最外层都没有进行处理的话，就会由tomcat容器抛出异常。</p>
<a id="more"></a>
<h2 id="Filter-过滤器"><a href="#Filter-过滤器" class="headerlink" title="Filter (过滤器)"></a>Filter (过滤器)</h2><p>首先，过滤器是服务端的一个组件，是基于servlet实现从客户端访问服务端web资源的一种拦截机制，对请求request和响应response都进行过滤，依赖于serverlet容器，使用时，实现Filter接口，在web.xml里配置对应的class还有mapping-url，springboot工程可以通FilterRegisteration配置后，设置要过滤的URL，注意：两种方式过滤器都是有序的，谁在前就先调用谁！定义过滤器后会重写三个方法，分别是init()，doFilter()和destory()。</p>
<ul>
<li><strong>init方法</strong>是过滤器的初始化方法，当web容器创建这个bean的时候就会执行，这个方法可以读取web.xml里面的参数</li>
<li><strong>idoFilter方法</strong>是执行过滤的请求的核心，当客户端请求访问web资源时，这个时候我们可以拿到request里面的参数，对数据进行处理后，通过filterChain方法将请求将请求放行，放行后我们也可以通过response对响应进行处理（比如压缩响应），然后会传递到下一个过滤器</li>
<li><strong>idestory方法</strong>是当web容器中的过滤器实例被销毁时，会被执行，释放资源</li>
</ul>
<h3 id="通过-Component注解配置"><a href="#通过-Component注解配置" class="headerlink" title="通过@Component注解配置"></a>通过@Component注解配置</h3><p>实现Filter方法，通过@Component将TimeFilter添加到Spring容器中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TimeFilter.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeFilter</span> <span class="keyword">implements</span> <span class="title">Filter</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(FilterConfig filterConfig)</span> <span class="keyword">throws</span> ServletException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"time filter init"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doFilter</span><span class="params">(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain)</span> <span class="keyword">throws</span> IOException, ServletException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"time filter start"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> start=System.currentTimeMillis();</span><br><span class="line">        filterChain.doFilter(servletRequest,servletResponse);</span><br><span class="line">        System.out.println(<span class="string">"time filter cost:"</span>+(System.currentTimeMillis()-start));</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"time filter finish"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"time filter destroy"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="通过-Configuration注解配置"><a href="#通过-Configuration注解配置" class="headerlink" title="通过@Configuration注解配置"></a>通过@Configuration注解配置</h3><p>新建WebConfig类，并添加@Configuration注解，将该类配置为一个配置类。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WebConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Bean</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> FilterRegistrationBean <span class="title">timeFilter</span><span class="params">()</span></span>&#123;</span><br><span class="line">       FilterRegistrationBean registrationBean=<span class="keyword">new</span> FilterRegistrationBean();</span><br><span class="line"></span><br><span class="line">       TimeFilter timeFilter=<span class="keyword">new</span> TimeFilter();</span><br><span class="line">       registrationBean.setFilter(timeFilter);</span><br><span class="line"></span><br><span class="line">       List&lt;String&gt; urls=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       urls.add(<span class="string">"/*"</span>);</span><br><span class="line">       registrationBean.setUrlPatterns(urls);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> registrationBean;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Interceptor-拦截器"><a href="#Interceptor-拦截器" class="headerlink" title="Interceptor (拦截器)"></a>Interceptor (拦截器)</h2><p>拦截器，顾名思义，他的作用就是拦截，这个要和过滤器区分开，过滤器依赖serverlet容器，获取request和response处理，是基于函数回调，简单说就是“去取你想取的”，拦截器是通过java反射机制，动态代理来拦截web请求，是“拒你想拒绝的”，他只拦截web请求，但不拦截静态资源。</p>
<ul>
<li><strong>preHandler()</strong>: 这个方法是在controller调用之前调用，通过返回true或者false决定是否进入Controller层</li>
<li><strong>postHandler()</strong>：在请求进入控制层之后调用，但是在处理请求抛出异常时不会调用</li>
<li><strong>afterCompletion()</strong>: 在请求处理完成之后，也就是在DispatherServlet渲染了视图之后执行，也就是说这个方法必定是执行，包含异常信息，它的主要作用就是清理资源</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/page_4.png" alt="拦截顺序"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/QQ20200130220238.png" alt="DispatcherServlet.java"></p>
<ol>
<li>applyPreHandle方法中调用自定义拦截器中的preHandler方法，如果preHandler方法中返回的是False，则程序跳出，不继续往下进行。 </li>
<li>ha.handle真正调用handler去处理。请求中方法参数的拼装在此进行。</li>
</ol>
<p>因此Interceptor中，无法知道请求参数的内容。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TimeInterceptor.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeInterceptor</span> <span class="keyword">implements</span> <span class="title">HandlerInterceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"preHandle"</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(((HandlerMethod) handler).getBean().getClass().getName());</span><br><span class="line">        System.out.println(((HandlerMethod) handler).getMethod().getName());</span><br><span class="line"></span><br><span class="line">        request.setAttribute(<span class="string">"startTime"</span>,System.currentTimeMillis());</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"postHandle"</span>);</span><br><span class="line">        Long start= (Long) request.getAttribute(<span class="string">"startTime"</span>);</span><br><span class="line">        System.out.println(<span class="string">"time interceptor cost:"</span>+(System.currentTimeMillis()-start));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler,Exception ex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"afterCompletion"</span>);</span><br><span class="line">        Long start= (Long) request.getAttribute(<span class="string">"startTime"</span>);</span><br><span class="line">        System.out.println(<span class="string">"time interceptor cost:"</span>+(System.currentTimeMillis()-start));</span><br><span class="line">        System.out.println(<span class="string">"ex is "</span>+ex);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Aspect-切片"><a href="#Aspect-切片" class="headerlink" title="Aspect (切片)"></a>Aspect (切片)</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/page_8.png" alt="拦截顺序"></p>
<h3 id="AOP-简介"><a href="#AOP-简介" class="headerlink" title="AOP 简介"></a>AOP 简介</h3><p>AOP(Aspect Oriented Programming)，即为面向切面编程。在软件开发中，散布于应用中多处的功能被称为横切关注点(cross-cutting concern)，通常来说，这些横切关注点从概念上是与应用的业务逻辑分离的。比如，声明式事务、日志、安全、缓存等等，都与业务逻辑无关，可以将这些东西抽象成为模块，采用面向切面编程的方式，通过声明方式定义这些功能用于何处，通过预编译方式和运行期动态代理实现这些模块化横切关注点程序功能进行统一维护，从而将横切关注点与它们所影响的对象之间分离出来，就是实现解耦。</p>
<p>横切关注点可以被模块化为特殊的类，这些类被称为切面(aspect)。这样做有两个优点:</p>
<ol>
<li>每个关注点都集中于一个地方，而不是分散到多处代码中;</li>
<li>服务模块更简洁，因为它们只包含主要的关注点的代码（核心业务逻辑），</li>
</ol>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/page_5.png" alt="Spring AOP"></p>
<p>而次要关注点的代码（日志，事务，安全等）都被转移到切面中。</p>
<h3 id="AOP术语"><a href="#AOP术语" class="headerlink" title="AOP术语"></a>AOP术语</h3><h4 id="Advice-通知"><a href="#Advice-通知" class="headerlink" title="Advice(通知)"></a>Advice(通知)</h4><p>切面类有自己要完成的工作，切面类的工作就称为通知。通知定义了切面是做什么以及何时使用。</p>
<ul>
<li><strong>“做什么”</strong>：即切面类中定义的方法是干什么的；</li>
<li><strong>“何时使用”</strong>：即5种通知类型，是在目标方法执行前，还是目标方法执行后等等；</li>
<li><strong>“何处做”</strong>：即通知定义了做什么，何时使用，但是不知道用在何处，而切点定义的就是告诉通知应该用在</li>
</ul>
<p>哪个类的哪个目标方法上，从而完美的完成横切点功能。<br>Spring切面定义了5种类型通知：</p>
<ol>
<li><strong>前置通知(Before)</strong>：在目标方法被调用之前调用通知功能。</li>
<li><strong>后置通知(After)</strong>：在目标方法完成之后调用通知，不会关心方法的输出是什么。</li>
<li><strong>返回通知(After-returning)</strong>： 在目标方法成功执行之后调用通知。</li>
<li><strong>异常通知(After-throwing)</strong>：在目标方法抛出异常后调用通知。</li>
<li><strong>环绕通知(Around)</strong>：通知包裹了被通知的方法，在被通知的方法调用之前和之后执行自定义的行为。</li>
</ol>
<h4 id="Join-point-连接点"><a href="#Join-point-连接点" class="headerlink" title="Join point(连接点)"></a>Join point(连接点)</h4><p>在我们的应用程序中有可能有数以万计的时机可以应用通知，而这些时机就被称为连接点。</p>
<p>连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。<br>连接点是一个虚概念，可以把连接点看成是切点的集合。</p>
<h4 id="Poincut-切点"><a href="#Poincut-切点" class="headerlink" title="Poincut(切点)"></a>Poincut(切点)</h4><p>连接点谈的是一个飘渺的大范围，而切点是一个具体的位置，用于缩小切面所通知的连接点的范围。前面说过，通知定义的是切面的”要做什么”和”在何时做”，是不是没有去哪里做，而切点就定义了”去何处做”。</p>
<p>切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或者是使用正则表达式定义所匹配的类和方法名称来指定切点。说白了，切点就是让通知找到”发泄的地方”。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/page_6.png" alt="切点的结构"></p>
<h4 id="Aspect-切面"><a href="#Aspect-切面" class="headerlink" title="Aspect(切面)"></a>Aspect(切面)</h4><p>切面是通知和切点的结合，通知和切点共同定义了切面的全部内容。因为通知定义的是切面的”要做什么”和”在何时做”，而切点定义的是切面的”在何地做”。将两者结合在一起，就可以完美的展现切面在何时，何地，做什么(功能)。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/c4f6b17e/page_7.png" alt="切入点表达式"></p>
<ul>
<li><p><strong>包名切面</strong><br>对 com.app.controller 包中所有的类的所有方法切面<br><code>@Pointcut(&quot;execution(public * com.app.controller.*.*(..))&quot;)</code></p>
</li>
<li><p><strong>包名及子包切面</strong><br>对 com.app.controller 及其子包中所有的类的所有方法切面<br><code>@Pointcut(&quot;execution(public * com.app.controller..*.*(..))&quot;)</code></p>
</li>
<li><p><strong>类名切面</strong><br>只针对 StudentController 类切面<br><code>@Pointcut(&quot;execution(public * com.app.controller.StudentController.*(..))&quot;)</code></p>
</li>
</ul>
<h4 id="Introduction-引入"><a href="#Introduction-引入" class="headerlink" title="Introduction(引入)"></a>Introduction(引入)</h4><p>引入这个概念就比较高大尚，引入允许我们向现有的类添加新方法或属性。</p>
<p>用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。</p>
<p>主要目的是想在无需修改A的情况下，引入B的行为和状态。</p>
<h4 id="Weaving-织入"><a href="#Weaving-织入" class="headerlink" title="Weaving(织入)"></a>Weaving(织入)</h4><p>织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。</p>
<p>把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。</p>
<h3 id="Spring相关常用注解"><a href="#Spring相关常用注解" class="headerlink" title="Spring相关常用注解"></a>Spring相关常用注解</h3><p>切片编程，面对的是处理过程中的方法或者阶段，以获得各部分的低耦合性的隔离效果，它是基于动态代理，它关注的是行为和过程，它常用的注解如下：</p>
<ul>
<li><strong>@Aspect</strong>：声明一个切面</li>
<li><strong>@Before</strong>：相当于拦截器preHandler，在方法执行前调用</li>
<li><strong>@After</strong>：相当于拦截器的afterComplement()在方法执行后调用</li>
<li><strong>@AfterThrowing</strong>：方法抛出异常时调用</li>
<li><strong>@AfterReturning</strong>：当方法返回时调用</li>
<li><strong>@Around</strong>：包含以上方的执行顺序</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TimeAspect.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeAspect</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Around</span>(<span class="string">"execution(* net.ling.security.web.controller.UserController.*(..))"</span>)</span><br><span class="line">   <span class="function"><span class="keyword">public</span> Object <span class="title">handlerControllerMethod</span><span class="params">(ProceedingJoinPoint pjp)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time aspect start"</span>);</span><br><span class="line">       Object[] args=pjp.getArgs();</span><br><span class="line">       <span class="keyword">for</span>(Object arg:args)&#123;</span><br><span class="line">           System.out.println(<span class="string">"arg is "</span>+arg);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">long</span> start=System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">       Object object=pjp.proceed();</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time aspect cost:"</span>+(System.currentTimeMillis()-start));</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time aspect end"</span>);</span><br><span class="line">       <span class="keyword">return</span> object;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul>
<li><strong>Filter</strong>：与Spring 无关，基于Servlet， 可以获取request 和 response，但是具体处理方法及相关参数获取不到；</li>
<li><strong>Interceptor</strong>：与Spring相关，可以获取request 和 response 以及具体处理方法，但是获取不到具体方法参数的值；</li>
<li><strong>Aspect</strong>： 与Spring相关，不可获取request和response ， 可以获取具体方法及具体方法参数的值；</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>interceptor</tag>
        <tag>filter</tag>
        <tag>aspect</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security 基本原理</title>
    <url>/p/2020/01/31/4d513018/</url>
    <content><![CDATA[<p>在web应用开发中，安全无疑是十分重要的，选择Spring Security来保护web应用是一个非常好的选择。Spring Security 是spring项目之中的一个安全模块，可以非常方便与spring项目无缝集成。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spring Security 的核心主要是一系列的过滤器链。</p>
<p>Spring Security 在 Servlet 的过滤链（filter chain）中注册了一个过滤器 FilterChainProxy，它会把请求代理到 Spring Security 自己维护的多个过滤链，每个过滤链会匹配一些 URL，如果匹配则执行对应的过滤器。过滤链是有顺序的，一个请求只会执行第一条匹配的过滤链。Spring Security 的配置本质上就是新增、删除、修改过滤器。</p>
<p>Spring Security的过滤链<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/page_11.png" alt="Spring Security的过滤链"></p>
<a id="more"></a> 
<p>本质上是一连串的Filter， 然后又以一个独立的Filter的形式插入到Filter Chain里，其名为FilterChainProxy。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/page_12.png" alt="Spring Security的过滤链"></p>
<p>实际上FilterChainProxy下面可以有多条Filter Chain，来针对不同的URL做验证，而Filter Chain中所拥有的Filter则会根据定义的服务自动增减。所以无需要显示再定义这些Filter，除非想要实现自己的逻辑。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/page_13.png" alt="FilterChainProxy"></p>
<ul>
<li><p><strong>绿色部分</strong>：<br>代表过滤器，每个方块都代表一个过滤器。</p>
<ul>
<li>UsernamePasswordAuthenticationFilter用来处理表单登录的。检查当前的请求是不是一个登录请求，然后再这个请求里面带没带用户名和密码，如果带了用户名和密码，这个过滤器就会用用户名和密码尝试去登录，如果这个请求里面没有带用户名和密码，继续进行下面的过滤器。</li>
<li>BasicAuthenticationFilter用来处理HTTPBasic登录的。检查请求的请求头中是不是有Basic开头的authentication的信息，如果有的话，会尝试拿出来做basic流的字节码然后从中取出用户名和密码尝试做登录。<br>springsecurity还提供了许多其他的认证方式，任何一个过滤器，成功的完成用户登录以后，都会再这个请求上做个标记，表明这个用户已经认证成功了。</li>
</ul>
</li>
<li><p><strong>蓝色部分</strong>：<br>在Exception Translation Filter中，捕获橙色部分FilterSecurity Interceptor抛出的异常，会根据抛出的异常，会做相应的处理。</p>
</li>
<li><p><strong>橙色部分</strong>：<br>请求经历过这些过滤器后，就会到达这个写橘黄色的拦截器上比如FilterSecurity Interceptor，这个拦截器是整个Spring Security过滤器的最后一环，是最终的守门人，在它身后是我们自己写的controller的rest服务，在该拦截器中，它会去决定当前的请求能不能去访问后面真正的服务。如果验证不通过，则会往前抛出异常。</p>
</li>
</ul>
<p>绿色的部分可以通过配置来设置是否生效，除了绿色之外的过滤器，是不能通过配置来控制的，而且位置也不能更改，不能移除到过滤器链外的。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/QQ20200131211014.png" alt="FilterSecurityInterceptor.java"></p>
<p>在FilterSecurityInterceptor中，</p>
<ol>
<li>我们可以看到所有的判断逻辑是在beforeInvocation中执行的。</li>
<li>当判断逻辑通过后，执行doFilter，是真正开始调用controller中的restful服务了。</li>
</ol>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/QQ20200131211645.png" alt="ExceptionTranslationFilter.java"></p>
<p>在ExceptionTranslationFilter中，主要是用来处理异常的，</p>
<ol>
<li>其中doFilter方法比较简单，主要是调用后面过滤器中的doFilter方法。</li>
<li>主要逻辑主要是在catch中，捕获到异常以后如何处理。</li>
</ol>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/QQ20200131212124.png" alt="UsernamePasswordAuthenticationFilter.java"></p>
<p>在UsernamePasswordAuthenticationFilter中，主要完成了对登录的验证，</p>
<ol>
<li>只处理POST类型的/login的请求</li>
<li>收到请求后，从Request中拿到用户名与密码，然后进行登录。</li>
</ol>
<h2 id="两种认证方式"><a href="#两种认证方式" class="headerlink" title="两种认证方式"></a>两种认证方式</h2><h3 id="HTTP-Basic"><a href="#HTTP-Basic" class="headerlink" title="HTTP Basic"></a>HTTP Basic</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// BrowserSecurityConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BrowserSecurityConfig</span> <span class="keyword">extends</span> <span class="title">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(HttpSecurity http)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        http.httpBasic()</span><br><span class="line">          .and()</span><br><span class="line">          .authorizeRequests()</span><br><span class="line">          .anyRequest()</span><br><span class="line">          .authenticated();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Form表单"><a href="#Form表单" class="headerlink" title="Form表单"></a>Form表单</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// BrowserSecurityConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BrowserSecurityConfig</span> <span class="keyword">extends</span> <span class="title">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(HttpSecurity http)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        http.formLogin()</span><br><span class="line">          .and()</span><br><span class="line">          .authorizeRequests()</span><br><span class="line">          .anyRequest()</span><br><span class="line">          .authenticated();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="过滤器链中常用的过滤器"><a href="#过滤器链中常用的过滤器" class="headerlink" title="过滤器链中常用的过滤器"></a>过滤器链中常用的过滤器</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/4d513018/QQ20200131220605.png" alt="FilterChainProxy.java"></p>
<p>SpringSecurity 采用的是责任链的设计模式，它有一条很长的过滤器链。现在对这条过滤器链的各个进行说明:</p>
<ul>
<li><strong>WebAsyncManagerIntegrationFilter</strong>：将 Security 上下文与 Spring Web 中用于处理异步请求映射的 WebAsyncManager 进行集成。</li>
<li><strong>SecurityContextPersistenceFilter</strong>：在每次请求处理之前将该请求相关的安全上下文信息加载到 SecurityContextHolder 中，然后在该次请求处理完成之后，将 SecurityContextHolder 中关于这次请求的信息存储到一个“仓储”中，然后将 SecurityContextHolder 中的信息清除，例如在Session中维护一个用户的安全信息就是这个过滤器处理的。</li>
<li><strong>HeaderWriterFilter</strong>：用于将头信息加入响应中。</li>
<li><strong>CsrfFilter</strong>：用于处理跨站请求伪造。</li>
<li><strong>LogoutFilter</strong>：用于处理退出登录。</li>
<li><strong>UsernamePasswordAuthenticationFilter</strong>：用于处理基于表单的登录请求，从表单中获取用户名和密码。默认情况下处理来自 /login 的请求。从表单中获取用户名和密码时，默认使用的表单 name 值为 username 和 password，这两个值可以通过设置这个过滤器的usernameParameter 和 passwordParameter 两个参数的值进行修改。</li>
<li><strong>DefaultLoginPageGeneratingFilter</strong>：如果没有配置登录页面，那系统初始化时就会配置这个过滤器，并且用于在需要进行登录时生成一个登录表单页面。</li>
<li><strong>DefaultLogoutPageGeneratingFilter</strong>：如果没有配置登出页面，那系统初始化时就会配置这个过滤器。</li>
<li><strong>BasicAuthenticationFilter</strong>：检测和处理 http basic 认证。</li>
<li><strong>RequestCacheAwareFilter</strong>：用来处理请求的缓存。</li>
<li><strong>SecurityContextHolderAwareRequestFilter</strong>：主要是包装请求对象request。</li>
<li><strong>RememberMeAuthenticationFilter</strong>：当用户没有登录而直接访问资源时, 从 cookie 里找出用户的信息, 如果 Spring Security 能够识别出用户提供的remember me cookie, 用户将不必填写用户名和密码, 而是直接登录进入系统，该过滤器默认不开启。</li>
<li><strong>AnonymousAuthenticationFilter</strong>：检测 SecurityContextHolder 中是否存在 Authentication 对象，如果不存在为其提供一个匿名 Authentication。</li>
<li><strong>SessionManagementFilter</strong>：管理 session 的过滤器</li>
<li><strong>ExceptionTranslationFilter</strong>：处理 AccessDeniedException 和 AuthenticationException 异常。</li>
<li><strong>FilterSecurityInterceptor</strong>：可以看做过滤器链的出口。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security中的授权控制</title>
    <url>/p/2020/02/14/18906fa6/</url>
    <content><![CDATA[<h2 id="基于Spring-Security的控制授权"><a href="#基于Spring-Security的控制授权" class="headerlink" title="基于Spring Security的控制授权"></a>基于Spring Security的控制授权</h2><h2 id="基于数据库RBAC数据模型的控制权限"><a href="#基于数据库RBAC数据模型的控制权限" class="headerlink" title="基于数据库RBAC数据模型的控制权限"></a>基于数据库RBAC数据模型的控制权限</h2><h2 id="权限表达式"><a href="#权限表达式" class="headerlink" title="权限表达式"></a>权限表达式</h2><table>
<thead>
<tr>
<th style="text-align:left">表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">permitAll</td>
<td>永远返回true</td>
</tr>
<tr>
<td style="text-align:left">denyAll</td>
<td>永远返回false</td>
</tr>
<tr>
<td style="text-align:left">anonymous</td>
<td>当前用户是anonymous时返回true</td>
</tr>
<tr>
<td style="text-align:left">rememberMe</td>
<td>当前用户是rememberMe用户时返回true</td>
</tr>
<tr>
<td style="text-align:left">authenticated</td>
<td>当前用户不是anonymous时返回true</td>
</tr>
<tr>
<td style="text-align:left">fullAuthenticated</td>
<td>当前用户既不是anonymous也不是rememberMe用户时返回true</td>
</tr>
<tr>
<td style="text-align:left">hasRole(role)</td>
<td>用户拥有指定的角色权限时返回true</td>
</tr>
<tr>
<td style="text-align:left">hasAnyRole([role1,role2])</td>
<td>用户拥有任意一个指定的角色权限时返回true</td>
</tr>
<tr>
<td style="text-align:left">hasAuthotity(authority)</td>
<td>用户拥有指定的权限时返回true</td>
</tr>
<tr>
<td style="text-align:left">hasAnyAuthority([authority1,authority2])</td>
<td>用户拥有任意一个指定的权限时返回true</td>
</tr>
<tr>
<td style="text-align:left">hasIpAddress(‘192.168.1.0/24’)</td>
<td>请求发送的IP匹配时返回true</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>rbac</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security 记住我</title>
    <url>/p/2020/02/03/635b5af8/</url>
    <content><![CDATA[<p>在用户登陆一次以后，系统会记住用户一段时间，在这段时间，用户不用反复登陆就可以使用我们的系统。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/page_2.png" alt="RememberMe流程"></p>
<ul>
<li>用户登录时，请求先进入<code>UsernamePasswordAuthenticationFilter</code>，当这个过滤器认证成功之后，会调用一个<code>RemeberMeService</code>服务，在<code>RemeberMeService</code>类里面有一个<code>TokenRepository</code>方法。<code>RemeberMeService</code>会生成一个<code>token</code>，然后将这个<code>token</code>存入到浏览器的<code>Cookie</code>中去。同时<code>TokenRepository</code>方法将这个<code>Token</code>写入到数据库中，因为这个动作是在通过<code>UsernamePasswordAuthenticationFilter</code>认证成功之后去做的，所以在存入数据的时候会将用户名和token存入进去，即token和用户名是一一对应的。</li>
<li>当同一个用户再次访问系统的时候，不需要登录了，这个请求在经过过滤器链的时候会经过<code>RememberMeAuthenticationFilter</code>，这个过滤器的作用就是读取<code>cookie</code>中的<code>token</code>，然后交给<code>RemeberMeService</code>，<code>RemeberMeService</code>会用<code>TokenRepository</code>到数据库中去查询这个<code>token</code>在数据库中有没有记录，如果有记录会将用户名取出来，取出来之后会调用<code>UserDetailsService</code>去获取用户信息，然后将用户信息存入到<code>SecurityContext</code>中去，这样用户就登录了。</li>
</ul>
<a id="more"></a> 
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/page_3.png" alt="SpringSecurity过滤器链"><br><code>RememberMeAuthenticationFilter</code>位于过滤器链中的倒数第二个位置，当其他用户认证都无法认证用户信息时，RememberMe会尝试做认证。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在用户认证配置中，注入<code>DataSource</code>与<code>UserDetailsService</code>，在<code>configure</code>方法中添加<code>rememberMe()</code>配置。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// BrowserSecurityConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@EnableConfigurationProperties</span>(SecurityProperties<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">ComponentScan</span>(<span class="title">basePackages</span> </span>= &#123;<span class="string">"net.ling.security"</span>&#125;)</span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BrowserSecurityConfig</span> <span class="keyword">extends</span> <span class="title">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> SecurityProperties securityProperties;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> DataSource dataSource;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> UserDetailsService userDetailsService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> AuthenticationSuccessHandler imoocAuthenticationSuccessHandler;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> AuthenticationFailureHandler imoocAuthenticationFailureHandler;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PasswordEncoder <span class="title">passwordEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> BCryptPasswordEncoder();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PersistentTokenRepository <span class="title">persistentTokenRepository</span><span class="params">()</span></span>&#123;</span><br><span class="line">        JdbcTokenRepositoryImpl tokenRepository=<span class="keyword">new</span> JdbcTokenRepositoryImpl();</span><br><span class="line">        tokenRepository.setDataSource(dataSource);</span><br><span class="line">        <span class="comment">// 启动的时候自动创建表</span></span><br><span class="line"><span class="comment">//        tokenRepository.setCreateTableOnStartup(true);</span></span><br><span class="line">        <span class="keyword">return</span> tokenRepository;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(HttpSecurity http)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        ValidateCodeFilter validateCodeFilter=<span class="keyword">new</span> ValidateCodeFilter();</span><br><span class="line">        validateCodeFilter.setAuthenticationFailureHandler(imoocAuthenticationFailureHandler);</span><br><span class="line">        validateCodeFilter.setSecurityProperties(securityProperties);</span><br><span class="line">        validateCodeFilter.afterPropertiesSet();</span><br><span class="line"></span><br><span class="line">        http.addFilterBefore(validateCodeFilter, UsernamePasswordAuthenticationFilter<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">formLogin</span>()</span></span><br><span class="line">                .loginPage("/authentication/require")</span><br><span class="line">                .loginProcessingUrl(<span class="string">"/authentication/form"</span>)</span><br><span class="line">                .successHandler(imoocAuthenticationSuccessHandler)</span><br><span class="line">                .failureHandler(imoocAuthenticationFailureHandler)</span><br><span class="line">                .and()</span><br><span class="line">                .authorizeRequests()</span><br><span class="line">                .antMatchers(<span class="string">"/authentication/require"</span>, <span class="string">"/swagger-ui.html"</span>,<span class="string">"/webjars/**"</span>,<span class="string">"/v2/**"</span>,<span class="string">"/swagger-resources/**"</span>,<span class="string">"/imooc-signIn.html"</span>,securityProperties.getBrowser().getLoginPage(),<span class="string">"/code/image"</span>).permitAll()</span><br><span class="line">                .anyRequest()</span><br><span class="line">                .authenticated()</span><br><span class="line">                .and()</span><br><span class="line">                .logout()</span><br><span class="line">                .and()</span><br><span class="line">                <span class="comment">// rememberMe start</span></span><br><span class="line">                .rememberMe()</span><br><span class="line">                .tokenRepository(persistentTokenRepository())</span><br><span class="line">                .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds())</span><br><span class="line">                .userDetailsService(userDetailsService)</span><br><span class="line">                <span class="comment">// rememberMe end</span></span><br><span class="line">                .and()</span><br><span class="line">                .csrf().disable();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在配置中，新增rememberMeSeconds的超时时间配置。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// BrowserProperties.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BrowserProperties</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String loginPage=<span class="string">"/imooc-signIn.html"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> LoginResponseType loginType= LoginResponseType.JSON;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rememberMeSeconds=<span class="number">3600</span>; <span class="comment">// 一小时</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h3 id="第一次登录"><a href="#第一次登录" class="headerlink" title="第一次登录"></a>第一次登录</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/QQ20200203210015.png" alt="AbstractAuthenticationProcessingFilter.java"></p>
<p>请求会进入<code>UsernamePasswordAuthenticationFilter</code>中，校验完用户名密码之后会调用<code>rememberMeServices.loginSuccess(request, response, authResult);</code>方法。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/QQ20200203210354.png" alt="PersistentTokenBasedRememberMeServices.java：onLoginSuccess()"></p>
<ol>
<li>用tokenRepository去创建一个新的token存入到数据库中。</li>
<li>将生成的token存入到浏览器的cookie中去。</li>
</ol>
<h3 id="在有RememberMe的情况下再次登录"><a href="#在有RememberMe的情况下再次登录" class="headerlink" title="在有RememberMe的情况下再次登录"></a>在有RememberMe的情况下再次登录</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/QQ20200203212248.png" alt="PersistentTokenBasedRememberMeServices.java-1：processAutoLoginCookie()"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/QQ20200203212348.png" alt="PersistentTokenBasedRememberMeServices.java-2：processAutoLoginCookie()"></p>
<ol>
<li>从请求的cookies中拿到token和tokenSeries。</li>
<li>从数据库中通过token和tokenSeries获取用户信息。</li>
<li>如果查不到相关的用户信息，就会抛出异常；如果有数据，则进行和一系列的判断，如：是否已经过期</li>
<li>如果检查都已经通过，通过找到的用户信息中的用户名，调用<code>UerDetailsService</code>获取详细的用户信息。</li>
</ol>
<h3 id="存储RememberMe获取到的用户信息"><a href="#存储RememberMe获取到的用户信息" class="headerlink" title="存储RememberMe获取到的用户信息"></a>存储RememberMe获取到的用户信息</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/635b5af8/QQ20200204211355.png" alt="RememberMeAuthenticationFilter.java"></p>
<p>将获取的用户信息存储至<code>SecurityContextHolder</code>中。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>remember me</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Social 简介</title>
    <url>/p/2020/02/08/929eb02b/</url>
    <content><![CDATA[<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h2 id="用户名-密码登录"><a href="#用户名-密码登录" class="headerlink" title="用户名+密码登录"></a>用户名+密码登录</h2><h2 id="手机号-短信登录"><a href="#手机号-短信登录" class="headerlink" title="手机号+短信登录"></a>手机号+短信登录</h2><h2 id="社交网站账号登录"><a href="#社交网站账号登录" class="headerlink" title="社交网站账号登录"></a>社交网站账号登录</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>spring social</tag>
      </tags>
  </entry>
  <entry>
    <title>Restful API 简介</title>
    <url>/p/2020/01/30/73de0aa5/</url>
    <content><![CDATA[<blockquote>
<p>Restful是一种网络应用程序的设计风格和开发方式，基于HTTP，可以使用XML格式定义或JSON格式定义。Restful适用于移动互联网厂商作为业务使能接口的场景，实现第三方OTT调用移动网络资源的功能，动作类型为新增、变更、删除所调用资源。</p>
</blockquote>
<h2 id="Restful-特点"><a href="#Restful-特点" class="headerlink" title="Restful 特点"></a>Restful 特点</h2><ol>
<li>每一个URI代表1种资源；</li>
<li>客户端使用GET、POST、PUT、DELETE4个表示操作方式的动词对服务端资源进行操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源；</li>
<li>通过操作资源的表现形式来操作资源；</li>
<li>资源的表现形式是XML或者HTML；</li>
<li>客户端与服务端之间的交互在请求之间是无状态的，从客户端到服务端的每个请求都必须包含理解请求所必需的信息。</li>
</ol>
<a id="more"></a>
<h2 id="Restful-常用URL"><a href="#Restful-常用URL" class="headerlink" title="Restful 常用URL"></a>Restful 常用URL</h2><table>
<thead>
<tr>
<th style="text-align:center">操作</th>
<th style="text-align:left">URL</th>
<th style="text-align:center">RequestMethod</th>
<th style="text-align:left">Restful URL</th>
<th style="text-align:center">Restful RequestMethod</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">查询</td>
<td style="text-align:left">/user/query?name=tom</td>
<td style="text-align:center">GET</td>
<td style="text-align:left">/user?name=tom</td>
<td style="text-align:center">GET</td>
</tr>
<tr>
<td style="text-align:center">详情</td>
<td style="text-align:left">/user/getInfo?id=1</td>
<td style="text-align:center">GET</td>
<td style="text-align:left">/user/1</td>
<td style="text-align:center">GET</td>
</tr>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:left">/user/create?name=tom</td>
<td style="text-align:center">POST</td>
<td style="text-align:left">/user</td>
<td style="text-align:center">POST</td>
</tr>
<tr>
<td style="text-align:center">修改</td>
<td style="text-align:left">/user/update?id=1&amp;name=jerry</td>
<td style="text-align:center">POST</td>
<td style="text-align:left">/user/1</td>
<td style="text-align:center">PUT</td>
</tr>
<tr>
<td style="text-align:center">删除</td>
<td style="text-align:left">/user/delete?id=1</td>
<td style="text-align:center">GET</td>
<td style="text-align:left">/user/1</td>
<td style="text-align:center">DELETE</td>
</tr>
</tbody>
</table>
<h2 id="Spring相关常用注解"><a href="#Spring相关常用注解" class="headerlink" title="Spring相关常用注解"></a>Spring相关常用注解</h2><h3 id="RestController"><a href="#RestController" class="headerlink" title="@RestController"></a>@RestController</h3><blockquote>
<p>标明Controller提供的是 Restful API</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Api</span>(tags = <span class="string">"用户管理"</span>, description = <span class="string">"管理用户的基本信息"</span>)</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="RequestMapping-及其变体"><a href="#RequestMapping-及其变体" class="headerlink" title="@RequestMapping 及其变体"></a>@RequestMapping 及其变体</h3><blockquote>
<p>映射http请求的url到Java方法中</p>
</blockquote>
<ul>
<li><strong>value</strong>: 指定request的地址</li>
<li><strong>method</strong>: 指定请求的method类型， GET、POST、PUT、DELETE等</li>
<li><strong>params</strong>: 指定request中包含的某些参数值，作为方法的输入，如：<code>@RequestParam(value = &quot;disable&quot;, required = false)</code><ul>
<li><strong>value</strong>: 为传入的参数</li>
<li><strong>required</strong>: 设置为 false，若传入的 value 为空值，则报错;设置为 true，若传入的 value 为空值，则返回 null.</li>
</ul>
</li>
<li><strong>consumes</strong>: 指定处理请求的提交内容类型，例如 application/json, text/html;</li>
<li><strong>produces</strong>: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回</li>
<li><strong>headers</strong>: 指定request中包含某些指定的header值，让该方法处理请求</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ApiOperation</span>(value = <span class="string">"查询所有用户信息"</span>, notes = <span class="string">"查询所有用户信息"</span>, httpMethod = <span class="string">"GET"</span>)</span><br><span class="line">    <span class="meta">@RequestMapping</span>(value = <span class="string">"/user"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;User&gt; <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getUsers();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>随着Spring的发展，可以采用如下注解，简化@RequestMapping。</p>
<h4 id="GetMapping"><a href="#GetMapping" class="headerlink" title="@GetMapping"></a>@GetMapping</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/user/&#123;id:\\d+&#125;"</span>)</span><br><span class="line">    <span class="meta">@JsonView</span>(User.UserDetailView<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    <span class="title">public</span> <span class="title">User</span> <span class="title">getInfo1</span>(@<span class="title">PathVariable</span> <span class="title">String</span> <span class="title">id</span>)</span>&#123;</span><br><span class="line">        User user=<span class="keyword">new</span> User();</span><br><span class="line">        user.setUsername(<span class="string">"tom"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> user;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="PostMapping"><a href="#PostMapping" class="headerlink" title="@PostMapping"></a>@PostMapping</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> User <span class="title">create</span><span class="params">(@Valid @RequestBody User user, BindingResult errors)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(errors.hasErrors())&#123;</span><br><span class="line">            errors.getAllErrors().stream().forEach(error-&gt;System.err.println(error.getDefaultMessage()));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(user.getId());</span><br><span class="line">        System.out.println(user.getUsername());</span><br><span class="line">        System.out.println(user.getPassword());</span><br><span class="line">        System.out.println(user.getBirthday());</span><br><span class="line"></span><br><span class="line">        user.setId(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> user;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="PutMapping"><a href="#PutMapping" class="headerlink" title="@PutMapping"></a>@PutMapping</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PutMapping</span>(<span class="string">"/user/&#123;id:\\d+&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> User <span class="title">update</span><span class="params">(@Valid @RequestBody User user, BindingResult errors)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(errors.hasErrors())&#123;</span><br><span class="line">            errors.getAllErrors().forEach(error-&gt;&#123;</span><br><span class="line">                System.out.println(error.getDefaultMessage());</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(user.getId());</span><br><span class="line">        System.out.println(user.getUsername());</span><br><span class="line">        System.out.println(user.getPassword());</span><br><span class="line">        System.out.println(user.getBirthday());</span><br><span class="line"></span><br><span class="line">        user.setId(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> user;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="DeleteMapping"><a href="#DeleteMapping" class="headerlink" title="@DeleteMapping"></a>@DeleteMapping</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@DeleteMapping</span>(<span class="string">"/user/&#123;id:\\d+&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">(@PathVariable String id)</span></span>&#123;</span><br><span class="line">        System.out.println(id);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="PatchMapping"><a href="#PatchMapping" class="headerlink" title="@PatchMapping"></a>@PatchMapping</h4><blockquote>
<p>Patch方式是对Put方式的一种补充。put方式是可以更新，但是更新的是整体。patch是对局部更新。</p>
</blockquote>
<h3 id="RequestParam"><a href="#RequestParam" class="headerlink" title="@RequestParam"></a>@RequestParam</h3><blockquote>
<p>映射请求参数到Java方法的参数中</p>
</blockquote>
<p><code>@RequestParam(value=”参数名”,required=”true/false”,defaultValue=””)</code></p>
<ul>
<li><strong>value</strong>：参数名</li>
<li><strong>required</strong>：是否包含该参数，默认为true，表示该请求路径中必须包含该参数，如果不包含就报错。</li>
<li><strong>defaultValue</strong>：默认参数值，如果设置了该值，required=true将失效，自动为false,如果没有传该参数，就使用默认值</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ApiOperation</span>(value = <span class="string">"查询用户信息"</span>, notes = <span class="string">"根据用户名查询用户信息"</span>, httpMethod = <span class="string">"GET"</span>)</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;User&gt; <span class="title">query</span><span class="params">(@RequestParam(name = <span class="string">"username"</span>,required = <span class="keyword">false</span>,defaultValue = <span class="string">"tom"</span>)</span> String nickname) </span>&#123;</span><br><span class="line">        System.out.println(nickname);</span><br><span class="line">        <span class="keyword">return</span> getUsers();</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="RequestBody"><a href="#RequestBody" class="headerlink" title="@RequestBody"></a>@RequestBody</h3><blockquote>
<p>映射请求参数到Java的实体类中</p>
</blockquote>
<p>该注解常用来处理Content-Type: 不是<code>application/x-www-form-urlencoded</code>编码的内容，例如<code>application/json</code>, <code>application/xml</code>等；</p>
<p>它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。</p>
<p>因为配置有FormHttpMessageConverter，所以也可以用来处理<code>application/x-www-form-urlencoded</code>的内容，处理完的结果放在一个<code>MultiValueMap&lt;String, String&gt;</code>里，这种情况在某些特殊需求下使用，详情查看FormHttpMessageConverter api;</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/something"</span>, method = RequestMethod.PUT)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(@RequestBody String body, Writer writer)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  writer.write(body);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PathVariable-7"><a href="#PathVariable-7" class="headerlink" title="@PathVariable ^7"></a>@PathVariable <a href="https://docs.spring.io/spring/docs/5.0.0.M1/spring-framework-reference/htmlsingle/#mvc-ann-requestmapping-uri-templates" target="_blank" rel="noopener">^7</a></h3><h3 id="PageableDefault"><a href="#PageableDefault" class="headerlink" title="@PageableDefault"></a>@PageableDefault</h3><blockquote>
<p>指定分页参数默认值</p>
</blockquote>
<ul>
<li><strong>page</strong>: 第几页，从0开始，默认为第0页</li>
<li><strong>size</strong>: 每一页的大小，默认为20</li>
<li><strong>sort</strong>: 排序相关的信息，以property,property(,ASC|DESC)的方式组织，例如sort=firstname&amp;sort=lastname,desc表示在按firstname正序排列基础上按lastname倒序排列。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// UserController.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ApiOperation</span>(value = <span class="string">"分页查询用户数据"</span>, notes = <span class="string">"分页查询用户数据"</span>, httpMethod = <span class="string">"GET"</span>)</span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/user"</span>)</span><br><span class="line">    <span class="meta">@JsonView</span>(User.UserSimpleView<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    <span class="title">public</span> <span class="title">List</span>&lt;<span class="title">User</span>&gt; <span class="title">query</span>(<span class="title">UserQueryCondition</span> <span class="title">condition</span>, @<span class="title">PageableDefault</span>(<span class="title">page</span></span>=<span class="number">1</span>,size = <span class="number">10</span>,sort = <span class="string">"username,asc"</span>) Pageable pageable) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TEST：在控制台中打印不出来</span></span><br><span class="line">        System.out.println(pageable.getPageSize());</span><br><span class="line">        System.out.println(pageable.getPageNumber());</span><br><span class="line">        System.out.println(pageable.getSort());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> getUsers();</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// User.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserSimpleView</span></span>&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserDetailView</span> <span class="keyword">extends</span> <span class="title">UserSimpleView</span></span>&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@MyConstraint</span>(message = <span class="string">"这是一个测试"</span>)</span><br><span class="line">    <span class="meta">@JsonView</span>(UserSimpleView<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    <span class="title">private</span> <span class="title">String</span> <span class="title">username</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@NotBlank</span>(message = <span class="string">"密码不能为空"</span>)</span><br><span class="line">    <span class="meta">@JsonView</span>(UserDetailView<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    <span class="title">private</span> <span class="title">String</span> <span class="title">password</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Past</span>(message = <span class="string">"生日必须是过去的时间"</span>)</span><br><span class="line">    <span class="keyword">private</span> Date birthday;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// UserQueryCondition.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserQueryCondition</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> ageTo;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String xxx;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Valid"><a href="#Valid" class="headerlink" title="@Valid"></a>@Valid</h3><h4 id="Hibernate-Validation注解"><a href="#Hibernate-Validation注解" class="headerlink" title="Hibernate Validation注解"></a>Hibernate Validation注解</h4><table>
<thead>
<tr>
<th>注解</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>@NotNull</td>
<td style="text-align:left">值不能为空</td>
</tr>
<tr>
<td>@Null</td>
<td style="text-align:left">值必须为空</td>
</tr>
<tr>
<td>@Pattern(regex=)</td>
<td style="text-align:left">字符串必须匹配正则表达式</td>
</tr>
<tr>
<td>@Size(min=,max=)</td>
<td style="text-align:left">集合的元素数量必须在min和max之间</td>
</tr>
<tr>
<td>@CreditCardNumber(ignoreNoneDigitCharacters=)</td>
<td style="text-align:left">字符串必须是信用卡号（美国标准）</td>
</tr>
<tr>
<td>@Email</td>
<td style="text-align:left">字符串必须是Email地址</td>
</tr>
<tr>
<td>@Length(min=,max=)</td>
<td style="text-align:left">检查字符串的长度</td>
</tr>
<tr>
<td>@NotBlank</td>
<td style="text-align:left">字符串必须有字符</td>
</tr>
<tr>
<td>@NotEmpty</td>
<td style="text-align:left">字符串不能为null，集合内须有元素</td>
</tr>
<tr>
<td>@Range(min=,max=)</td>
<td style="text-align:left">值数字必须大于等于min，小于等于max</td>
</tr>
<tr>
<td>@SafeHtml</td>
<td style="text-align:left">字符串是安全的html</td>
</tr>
<tr>
<td>@URL(protocol=,host=,port=,regexp=,flags=)</td>
<td style="text-align:left">字符串是合法的url</td>
</tr>
<tr>
<td>@AssertFalse</td>
<td style="text-align:left">值必须是False</td>
</tr>
<tr>
<td>@AssertTrue</td>
<td style="text-align:left">值必须是True</td>
</tr>
<tr>
<td>@DecimalMax(value=,inclusive=)</td>
<td style="text-align:left">值必须小于等于（inclusive=true）/小于（inclusive=false）value属性指定的值。可以注释在字符串类型的属性上。</td>
</tr>
<tr>
<td>@DecimalMin(value=,inclusive=)</td>
<td style="text-align:left">值必须大于等于（inclusive=true）/大于（inclusive=false）value属性指定的值。可以注释在字符串类型的属性上。</td>
</tr>
<tr>
<td>@Digits(integer=,fraction=)</td>
<td style="text-align:left">数字格式检查。integer指定整数部分额最大长度，fraction指定小数部分的最大长度</td>
</tr>
<tr>
<td>@Future</td>
<td style="text-align:left">值必须是未来的日期</td>
</tr>
<tr>
<td>@Past</td>
<td style="text-align:left">值必须是过去的日期</td>
</tr>
<tr>
<td>@Max(value=)</td>
<td style="text-align:left">值必须小于等于value指定的值。不能注释在字符串类型的属性上。</td>
</tr>
<tr>
<td>@Min(value=)</td>
<td style="text-align:left">值必须大于等于value指定的值。不能注释在字符串类型的属性上。</td>
</tr>
</tbody>
</table>
<h4 id="BindingResult"><a href="#BindingResult" class="headerlink" title="BindingResult"></a>BindingResult</h4><p>@Valid 和 BindingResult 是一一对应的，如果有多个@Valid，那么每个@Valid后面跟着的BindingResult就是这个@Valid的验证结果，顺序不能乱。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">doSomething</span><span class="params">(@Validated @RequestBody OneDto oneDto, </span></span></span><br><span class="line"><span class="function"><span class="params">                          BindingResult result)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 参数通不过校验也会进入方法执行，校验结果会通过result参数传递进来</span></span><br><span class="line">    <span class="keyword">if</span> (result.hasErrors())&#123;</span><br><span class="line">         <span class="comment">// 没通过校验</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="Validated和-Valid的区别"><a href="#Validated和-Valid的区别" class="headerlink" title="@Validated和@Valid的区别"></a>@Validated和@Valid的区别</h4><ul>
<li><strong>@Valid</strong>是使用Hibernate validation的时候使用</li>
<li><strong>@Validated</strong>是只用Spring Validator校验机制使用</li>
</ul>
<blockquote>
<p>说明：java的JSR303声明了@Valid这类接口，而Hibernate-validator对其进行了实现。@Validation对@Valid进行了二次封装，在使用上并没有区别，但在分组、注解位置、嵌套验证等功能上有所不同，这里主要就这几种情况进行说明。</p>
</blockquote>
<h5 id="注解位置"><a href="#注解位置" class="headerlink" title="注解位置"></a>注解位置</h5><ul>
<li><strong>@Validated</strong>：用在类型、方法和方法参数上。但不能用于成员属性（field），如果@Validated注解在成员属性上，则会报  不适用于field错误</li>
<li><strong>@Valid</strong>：可以用在方法、构造函数、方法参数和成员属性（field）上</li>
</ul>
<h5 id="分组校验"><a href="#分组校验" class="headerlink" title="分组校验"></a>分组校验</h5><ul>
<li><strong>@Validated</strong>：提供分组功能，可以在参数验证时，根据不同的分组采用不同的验证机制</li>
<li><strong>@Valid</strong>：没有分组功能</li>
</ul>
<h5 id="组序列"><a href="#组序列" class="headerlink" title="组序列"></a>组序列</h5><p>默认情况下 不同级别的约束验证是无序的，但是在一些情况下，顺序验证却是很重要。</p>
<p>一个组可以定义为其他组的序列，使用它进行验证的时候必须符合该序列规定的顺序。在使用组序列验证的时候，如果序列前边的组验证失败，则后面的组将不再给予验证。</p>
<h5 id="嵌套校验"><a href="#嵌套校验" class="headerlink" title="嵌套校验"></a>嵌套校验</h5><p>一个待验证的pojo类，其中还包含了待验证的对象，需要在待验证对象上注解@Valid，才能验证待验证对象中的成员属性，这里不能使用@Validated。</p>
<h2 id="HTTP状态码"><a href="#HTTP状态码" class="headerlink" title="HTTP状态码"></a>HTTP状态码</h2><h3 id="GET"><a href="#GET" class="headerlink" title="GET"></a>GET</h3><blockquote>
<ul>
<li>安全且幂等</li>
<li>获取表示</li>
<li>变更时获取表示（缓存）</li>
</ul>
</blockquote>
<ul>
<li>200（OK） - 表示已在响应中发出</li>
<li>204（无内容） - 资源有空表示</li>
<li>301（Moved Permanently） - 资源的URI已被更新</li>
<li>303（See Other） - 其他（如，负载均衡）</li>
<li>304（not modified）- 资源未更改（缓存）</li>
<li>400 （bad request）- 指代坏请求（如，参数错误）</li>
<li>404 （not found）- 资源不存在</li>
<li>406 （not acceptable）- 服务端不支持所需表示</li>
<li>500 （internal server error）- 通用错误响应</li>
<li>503 （Service Unavailable）- 服务端当前无法处理请求</li>
</ul>
<h3 id="POST"><a href="#POST" class="headerlink" title="POST"></a>POST</h3><blockquote>
<ul>
<li>不安全且不幂等</li>
<li>使用服务端管理的（自动产生）的实例号创建资源</li>
<li>创建子资源</li>
<li>部分更新资源</li>
<li>如果没有被修改，则不过更新资源（乐观锁）</li>
</ul>
</blockquote>
<ul>
<li>200（OK）- 如果现有资源已被更改</li>
<li>201（created）- 如果新资源被创建</li>
<li>202（accepted）- 已接受处理请求但尚未完成（异步处理）</li>
<li>301（Moved Permanently）- 资源的URI被更新</li>
<li>303（See Other）- 其他（如，负载均衡）</li>
<li>400（bad request）- 指代坏请求</li>
<li>404 （not found）- 资源不存在</li>
<li>406 （not acceptable）- 服务端不支持所需表示</li>
<li>409 （conflict）- 通用冲突</li>
<li>412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突）</li>
<li>415 （unsupported media type）- 接受到的表示不受支持</li>
<li>500 （internal server error）- 通用错误响应</li>
<li>503 （Service Unavailable）- 服务当前无法处理请求</li>
</ul>
<h3 id="PUT"><a href="#PUT" class="headerlink" title="PUT"></a>PUT</h3><blockquote>
<ul>
<li>不安全但幂等</li>
<li>用客户端管理的实例号创建一个资源</li>
<li>通过替换的方式更新资源</li>
<li>如果未被修改，则更新资源（乐观锁）</li>
</ul>
</blockquote>
<ul>
<li>200 （OK）- 如果已存在资源被更改</li>
<li>201 （created）- 如果新资源被创建</li>
<li>301（Moved Permanently）- 资源的URI已更改</li>
<li>303 （See Other）- 其他（如，负载均衡）</li>
<li>400 （bad request）- 指代坏请求</li>
<li>404 （not found）- 资源不存在</li>
<li>406 （not acceptable）- 服务端不支持所需表示</li>
<li>409 （conflict）- 通用冲突</li>
<li>412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突）</li>
<li>415 （unsupported media type）- 接受到的表示不受支持</li>
<li>500 （internal server error）- 通用错误响应</li>
<li>503 （Service Unavailable）- 服务当前无法处理请求</li>
</ul>
<h3 id="DELETE"><a href="#DELETE" class="headerlink" title="DELETE"></a>DELETE</h3><blockquote>
<ul>
<li>不安全但幂等</li>
<li>删除资源</li>
</ul>
</blockquote>
<ul>
<li>200 （OK）- 资源已被删除</li>
<li>301 （Moved Permanently）- 资源的URI已更改</li>
<li>303 （See Other）- 其他，如负载均衡</li>
<li>400 （bad request）- 指代坏请求</li>
<li>404 （not found）- 资源不存在</li>
<li>409 （conflict）- 通用冲突</li>
<li>500 （internal server error）- 通用错误响应</li>
<li>503 （Service Unavailable）- 服务端当前无法处理请求</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>Restful是一种用URL描述资源的方式</li>
<li>使用HTTP方法描述行为，使用HTTP状态码来表示不同的结果</li>
<li>使用json交互数据</li>
<li>Restful只是一种风格，并不是强制的标准</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title>OAuth2 简介</title>
    <url>/p/2020/02/08/9fedbe87/</url>
    <content><![CDATA[<h2 id="OAuth"><a href="#OAuth" class="headerlink" title="OAuth"></a>OAuth</h2><h3 id="授权码模式（authorization-code）"><a href="#授权码模式（authorization-code）" class="headerlink" title="授权码模式（authorization code）"></a>授权码模式（authorization code）</h3><h3 id="密码模式（resource-owner-password-credentials）"><a href="#密码模式（resource-owner-password-credentials）" class="headerlink" title="密码模式（resource owner password credentials）"></a>密码模式（resource owner password credentials）</h3><h3 id="客户端模式（client-credentials）"><a href="#客户端模式（client-credentials）" class="headerlink" title="客户端模式（client credentials）"></a>客户端模式（client credentials）</h3><h3 id="简化模式（implicit）"><a href="#简化模式（implicit）" class="headerlink" title="简化模式（implicit）"></a>简化模式（implicit）</h3>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>spring social</tag>
        <tag>oauth2</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security中的Session管理</title>
    <url>/p/2020/02/10/2ce46874/</url>
    <content><![CDATA[<h2 id="Session超时处理"><a href="#Session超时处理" class="headerlink" title="Session超时处理"></a>Session超时处理</h2><h2 id="Session并发控制"><a href="#Session并发控制" class="headerlink" title="Session并发控制"></a>Session并发控制</h2><h2 id="集群Session管理"><a href="#集群Session管理" class="headerlink" title="集群Session管理"></a>集群Session管理</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>spring social</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Security中的Token处理</title>
    <url>/p/2020/02/14/2a553567/</url>
    <content><![CDATA[<h2 id="基本的Token参数配置"><a href="#基本的Token参数配置" class="headerlink" title="基本的Token参数配置"></a>基本的Token参数配置</h2><h2 id="使用JWT替换默认的Token"><a href="#使用JWT替换默认的Token" class="headerlink" title="使用JWT替换默认的Token"></a>使用JWT替换默认的Token</h2><h3 id="JWT-简介"><a href="#JWT-简介" class="headerlink" title="JWT 简介"></a>JWT 简介</h3><p> JWT(Json Web Token)</p>
<ul>
<li>自包含</li>
<li>密签</li>
<li>可扩展</li>
</ul>
<h2 id="扩展和解析JWT的信息"><a href="#扩展和解析JWT的信息" class="headerlink" title="扩展和解析JWT的信息"></a>扩展和解析JWT的信息</h2><h2 id="基于JWT实现SSO"><a href="#基于JWT实现SSO" class="headerlink" title="基于JWT实现SSO"></a>基于JWT实现SSO</h2><h3 id="SSO-简介"><a href="#SSO-简介" class="headerlink" title="SSO 简介"></a>SSO 简介</h3><h3 id="授权表达式"><a href="#授权表达式" class="headerlink" title="授权表达式"></a>授权表达式</h3>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>spring social</tag>
        <tag>token</tag>
        <tag>jwt</tag>
      </tags>
  </entry>
  <entry>
    <title>Swagger介绍及使用</title>
    <url>/p/2020/01/31/631e780c/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://swagger.io/" target="_blank" rel="noopener">https://swagger.io/</a></p>
</blockquote>
<p>相信无论是前端还是后端开发，都或多或少地被接口文档折磨过，期望有一个好的接口文档。Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。</p>
<ul>
<li>接口的文档在线自动生成。</li>
<li>功能测试。</li>
</ul>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/631e780c/813533-774318cdb25c338d.png" alt="Swagger"></p>
<a id="more"></a>
<h2 id="框架说明及使用"><a href="#框架说明及使用" class="headerlink" title="框架说明及使用"></a>框架说明及使用</h2><p>Swagger官网中提供了一系列的组件，可以在线边写接口文档。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/631e780c/QQ20200131103132.png" alt="Swagger 组件"></p>
<h3 id="开源版"><a href="#开源版" class="headerlink" title="开源版"></a>开源版</h3><ul>
<li><strong>Swagger Codegen</strong>: 通过Codegen 可以将描述文件生成html格式和cwiki形式的接口文档，同时也能生成多钟语言的服务端和客户端的代码。支持通过jar包，docker，node等方式在本地化执行生成。也可以在后面的Swagger Editor中在线生成。</li>
<li><strong>Swagger Editor</strong>: 类似于markendown编辑器的编辑Swagger描述文件的编辑器，该编辑支持实时预览描述文件的更新效果。也提供了在线编辑器和本地部署编辑器两种方式。</li>
<li><strong>Swagger UI</strong>:提供了一个可视化的UI页面展示描述文件。接口的调用方、测试、项目经理等都可以在该页面中对相关接口进行查阅和做一些简单的接口请求。该项目支持在线导入描述文件和本地部署UI项目。</li>
</ul>
<h3 id="专业版"><a href="#专业版" class="headerlink" title="专业版"></a>专业版</h3><ul>
<li><strong>Swagger Inspector</strong>: 感觉和postman差不多，是一个可以对接口进行测试的在线版的postman。比在Swagger UI里面做接口请求，会返回更多的信息，也会保存你请求的实际请求参数等数据。</li>
<li><strong>SwaggerHub</strong>：集成了上面所有项目的各个功能，你可以以项目和版本为单位，将你的描述文件上传到Swagger Hub中。在Swagger Hub中可以完成上面项目的所有工作，需要注册账号，分免费版和收费版。</li>
<li><strong>SwaggerHub Enterprise</strong>：为企业打造的Swagger扩展，使团队协作开发API。SwaggerHub Enterprise可在云和本地安装中使用。</li>
</ul>
<h2 id="基于Spring框架的Swagger"><a href="#基于Spring框架的Swagger" class="headerlink" title="基于Spring框架的Swagger"></a>基于Spring框架的Swagger</h2><blockquote>
<p><a href="http://springfox.github.io/springfox/" target="_blank" rel="noopener">http://springfox.github.io/springfox/</a></p>
</blockquote>
<ul>
<li><strong>Springfox Swagger</strong>: Spring 基于swagger规范，可以将基于SpringMVC和Spring Boot项目的项目代码，自动生成JSON格式的描述文件。本身不是属于Swagger官网提供的。</li>
</ul>
<h2 id="SpringBoot整合Swagger2"><a href="#SpringBoot整合Swagger2" class="headerlink" title="SpringBoot整合Swagger2"></a>SpringBoot整合Swagger2</h2><h3 id="添加相关依赖"><a href="#添加相关依赖" class="headerlink" title="添加相关依赖"></a>添加相关依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.springfox<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springfox-swagger2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;swagger2.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.springfox<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springfox-swagger-ui<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;swagger2.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="配置Swagger2"><a href="#配置Swagger2" class="headerlink" title="配置Swagger2"></a>配置Swagger2</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@EnableSwagger</span>2</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SwaggerConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化创建Swagger Api</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Docket <span class="title">createRestApi</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Docket(DocumentationType.SWAGGER_2)</span><br><span class="line">                <span class="comment">// 详细信息定制</span></span><br><span class="line">                .apiInfo(apiInfo())</span><br><span class="line">                .select()</span><br><span class="line">                <span class="comment">// 指定当前包路径</span></span><br><span class="line">                .apis(RequestHandlerSelectors.basePackage(<span class="string">"net.ling.security"</span>))</span><br><span class="line">                <span class="comment">// 扫描所有 .apis(RequestHandlerSelectors.any())</span></span><br><span class="line">                .paths(PathSelectors.any())</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加摘要信息</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> ApiInfo <span class="title">apiInfo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 用ApiInfoBuilder进行定制</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ApiInfoBuilder()</span><br><span class="line">                .title(<span class="string">"Spring Security开发安全的REST服务"</span>)</span><br><span class="line">                .description(<span class="string">"Swagger文档"</span>)</span><br><span class="line">                .contact(<span class="keyword">new</span> Contact(<span class="string">"https://lingmoumou.github.io/"</span>, <span class="keyword">null</span>, <span class="keyword">null</span>))</span><br><span class="line">                .version(<span class="string">"1.0.0"</span>)</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="创建测试实例"><a href="#创建测试实例" class="headerlink" title="创建测试实例"></a>创建测试实例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="启动程序"><a href="#启动程序" class="headerlink" title="启动程序"></a>启动程序</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/631e780c/QQ20200131130243.png" alt="Swagger页面"></p>
<h2 id="SpringSecurity中使用Swagger2"><a href="#SpringSecurity中使用Swagger2" class="headerlink" title="SpringSecurity中使用Swagger2"></a>SpringSecurity中使用Swagger2</h2><p>开放Swagger的权限<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SecurityConfig.java</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SecurityConfig</span> <span class="keyword">extends</span> <span class="title">WebSecurityConfigurerAdapter</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(WebSecurity web)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        web.ignoring(). antMatchers(<span class="string">"/swagger-ui.html"</span>)</span><br><span class="line">                .antMatchers(<span class="string">"/webjars/**"</span>)</span><br><span class="line">                .antMatchers(<span class="string">"/v2/**"</span>)</span><br><span class="line">                .antMatchers(<span class="string">"/swagger-resources/**"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Swagger2相关常用注解"><a href="#Swagger2相关常用注解" class="headerlink" title="Swagger2相关常用注解"></a>Swagger2相关常用注解</h2><ul>
<li><strong>@Api</strong>：用在类上，说明该类的作用。</li>
<li><strong>@ApiOperation</strong>：注解来给API增加方法说明。</li>
<li><strong>@ApiImplicitParams</strong>: 用在方法上包含一组参数说明。<ul>
<li><strong>@ApiImplicitParam</strong>：用来注解来给方法入参增加说明。</li>
</ul>
</li>
<li><strong>@ApiResponses</strong>：用于表示一组响应<ul>
<li><strong>@ApiResponse</strong>：用在@ApiResponses中，一般用于表达一个错误的响应信息<ul>
<li><strong>code</strong>：数字，例如400</li>
<li><strong>message</strong>：信息，例如”请求参数没填好”</li>
<li><strong>response</strong>：抛出异常的类   </li>
</ul>
</li>
</ul>
</li>
<li><strong>@ApiModel</strong>：描述一个Model的信息（一般用在请求参数无法使用@ApiImplicitParam注解进行描述的时候）<ul>
<li><strong>@ApiModelProperty</strong>：描述一个model的属性</li>
</ul>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>swagger</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot 文件的上传与下载</title>
    <url>/p/2020/01/30/50bf1f8f/</url>
    <content><![CDATA[<h2 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/file"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostMapping</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> FileInfo <span class="title">upload</span><span class="params">(MultipartFile file)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        System.out.println(file.getName());</span><br><span class="line">        System.out.println(file.getOriginalFilename());</span><br><span class="line">        System.out.println(file.getSize());</span><br><span class="line"></span><br><span class="line">        File localFile=<span class="keyword">new</span> File(folder,System.currentTimeMillis()+<span class="string">".txt"</span>);</span><br><span class="line">        file.transferTo(localFile);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FileInfo(localFile.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一般情况下，上传的文件不会存放在本地，因此可以将file.transferTo()改成file.getInputStream()，获取文件流，然后写入其他地方中，如OSS中。</p>
<a id="more"></a>
<p>对上传的文件进行一些配置:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spring.servlet.multipart.enabled=true # 是否支持 multipart 上传文件</span><br><span class="line">spring.servlet.multipart.file-size-threshold=0 # 支持文件写入磁盘</span><br><span class="line">spring.servlet.multipart.location= # 上传文件的临时目录</span><br><span class="line">spring.servlet.multipart.max-file-size=10Mb # 最大支持文件大小</span><br><span class="line">spring.servlet.multipart.max-request-size=10Mb # 最大支持请求大小</span><br><span class="line">spring.servlet.multipart.resolve-lazily=false # 是否支持 multipart 上传文件时懒加载</span><br></pre></td></tr></table></figure></p>
<p>如果不限制大小，则设置为-1即可<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spring.servlet.multipart.maxFileSize=-1</span><br><span class="line">spring.servlet.multipart.maxRequestSize=-1</span><br></pre></td></tr></table></figure></p>
<h2 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h2><p>以下方法是通过浏览器直接下载。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/file"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/&#123;id&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">download</span><span class="params">(@PathVariable String id, HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> (InputStream inputStream=<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(folder,id+<span class="string">".txt"</span>));</span><br><span class="line">            OutputStream outputStream=response.getOutputStream();)&#123;</span><br><span class="line">            response.setContentType(<span class="string">"application/x-download"</span>);</span><br><span class="line">            response.addHeader(<span class="string">"Content-Disposition"</span>,<span class="string">"attachment;filename=test.txt"</span>);</span><br><span class="line"></span><br><span class="line">            IOUtils.copy(inputStream,outputStream);</span><br><span class="line">            outputStream.flush();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>upload</tag>
        <tag>download</tag>
      </tags>
  </entry>
  <entry>
    <title>WireMock介绍及使用</title>
    <url>/p/2020/01/31/6e8bf520/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://wiremock.org/" target="_blank" rel="noopener">http://wiremock.org/</a></p>
</blockquote>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/6e8bf520/3050552-7383e4eb67a0b331.png" alt="WireMock"></p>
<p>WireMock 是基于 HTTP 的模拟器。它具备 HTTP 响应存根、请求验证、代理/拦截、记录和回放功能。<br>当开发人员的开发进度不一致时，可以依赖 WireMock 构建的接口，模拟不同请求与响应，从而避某一模块的开发进度。</p>
<a id="more"></a>
<h2 id="安装WireMock-服务端"><a href="#安装WireMock-服务端" class="headerlink" title="安装WireMock 服务端"></a>安装WireMock 服务端</h2><blockquote>
<p><a href="https://hub.docker.com/r/holomekc/wiremock-gui" target="_blank" rel="noopener">https://hub.docker.com/r/holomekc/wiremock-gui</a></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull holomekc/wiremock-gui</span><br><span class="line">docker run -d -p 30080:8080 --name wiremock holomekc/wiremock-gui</span><br></pre></td></tr></table></figure>
<p>待服务运行成功后，输入<a href="http://127.0.0.1:30080/__admin/webapp" target="_blank" rel="noopener">http://127.0.0.1:30080/__admin/webapp</a> ，登录可视化页面。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/6e8bf520/QQ20200131140011.png" alt="wiremock-gui"></p>
<h2 id="引入WireMock依赖"><a href="#引入WireMock依赖" class="headerlink" title="引入WireMock依赖"></a>引入WireMock依赖</h2><p>有标准的和独立的两种版本，根据需要自行引入。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.tomakehurst<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>wiremock-jre8<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.25.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="添加模拟请求映射"><a href="#添加模拟请求映射" class="headerlink" title="添加模拟请求映射"></a>添加模拟请求映射</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// MockServer.java</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> com.github.tomakehurst.wiremock.client.WireMock.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MockServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        configureFor(<span class="number">30080</span>);</span><br><span class="line">        removeAllMappings();</span><br><span class="line"></span><br><span class="line">        mock(<span class="string">"/order/1"</span>,<span class="string">"01"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mock</span><span class="params">(String url,String file)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ClassPathResource resource=<span class="keyword">new</span> ClassPathResource(<span class="string">"mock/response/"</span>+file+<span class="string">".txt"</span>);</span><br><span class="line">        String content= StringUtils.join(FileUtils.readLines(resource.getFile(),<span class="string">"UTF-8"</span>).toArray(),<span class="string">"\n"</span>);</span><br><span class="line">        stubFor(get(urlPathEqualTo(url)).willReturn(aResponse().withBody(content).withStatus(<span class="number">200</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="在IntelliJ中添加静态包导入的方法"><a href="#在IntelliJ中添加静态包导入的方法" class="headerlink" title="在IntelliJ中添加静态包导入的方法"></a>在IntelliJ中添加静态包导入的方法</h2><p>在Eclipse中，可以配置某些最喜欢的”类，当调用代码完成时，将查找这些类以查看是否可以为方法添加静态导入（这在首选项&gt; Java&gt;编辑器&gt;下内容辅助&gt;收藏夹）。</p>
<p>在IntelliJ中，通过File -&gt; Settings -&gt; Code Style -&gt; Java -&gt; Imports，添加需要经常引入的静态包方法。</p>
<p>这里在引入wiremock.client相关方法时，设置了IntelliJ中的静态方法。</p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/6e8bf520/QQ20200131144637.png" alt="在IntelliJ中添加静态包导入"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>Java</category>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>Spring Security开发安全的REST服务</tag>
        <tag>wiremock</tag>
      </tags>
  </entry>
  <entry>
    <title>API安全</title>
    <url>/p/2020/02/23/834ac76e/</url>
    <content><![CDATA[<h2 id="API安全的要素"><a href="#API安全的要素" class="headerlink" title="API安全的要素"></a>API安全的要素</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/2dc6b46a/page_2.png" alt="API安全"></p>
<a id="more"></a>
<h3 id="API安全的目标-CIA"><a href="#API安全的目标-CIA" class="headerlink" title="API安全的目标(CIA)"></a>API安全的目标(CIA)</h3><ul>
<li><strong>机密性(Confientiality)</strong>：确保信息只被预期的读者访问</li>
<li><strong>完整性(Integrity)</strong>：防止未授权的创建、修改和删除</li>
<li><strong>可用性(Availability)</strong>：当用户性需要访问API时，API总是可用的</li>
</ul>
<h3 id="常见的API风险-STRIDE"><a href="#常见的API风险-STRIDE" class="headerlink" title="常见的API风险(STRIDE)"></a>常见的API风险(STRIDE)</h3><ul>
<li><strong>欺骗(Spoofing)</strong>：伪装成某个人</li>
<li><strong>干预(Tampering)</strong>：将不希望被修改的数据、消息或设置改掉</li>
<li><strong>否认(Repudiation)</strong>：拒绝承认做过的事情</li>
<li><strong>信息泄露(Information disclosure)</strong>：将你希望保密的信息纰漏出来</li>
<li><strong>拒绝服务(Denial of Service)</strong>：阻止用户访问信息和服务</li>
<li><strong>越权(Elevation of Privilege)</strong>：做了你不希望他能做的事</li>
</ul>
<h3 id="风险与安全机制的对应关系"><a href="#风险与安全机制的对应关系" class="headerlink" title="风险与安全机制的对应关系"></a>风险与安全机制的对应关系</h3><ul>
<li><strong>认证-欺骗</strong>：确保你的用户或客户端真的是他们自己</li>
<li><strong>授权-信息泄露/干预/越权</strong>：确保每个针对API的访问都是经过授权的</li>
<li><strong>审计-否认</strong>：确保所有的操作都被记录，以便追溯和监控</li>
<li><strong>流控-拒绝服务</strong>：防止用户请求淹没你的API</li>
<li><strong>加密-信息泄露</strong>：确保出入API的数据是机密的</li>
</ul>
<h2 id="API安全基本机制"><a href="#API安全基本机制" class="headerlink" title="API安全基本机制"></a>API安全基本机制</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/2dc6b46a/page_2.png" alt="常见的安全机制"></p>
<h3 id="流控"><a href="#流控" class="headerlink" title="流控"></a>流控</h3><h4 id="使用Guava做简单的限流"><a href="#使用Guava做简单的限流" class="headerlink" title="使用Guava做简单的限流"></a>使用Guava做简单的限流</h4><p>继承Spring框架中的OncePerRequestFilter保证一个请求在每个Filter只被执行一次</p>
]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>springcloud</tag>
        <tag>api</tag>
        <tag>Spring Cloud微服务安全实战</tag>
      </tags>
  </entry>
  <entry>
    <title>微服务网关安全</title>
    <url>/p/2020/02/23/f945e7d5/</url>
    <content><![CDATA[<h2 id="微服务安全面临的挑战"><a href="#微服务安全面临的挑战" class="headerlink" title="微服务安全面临的挑战"></a>微服务安全面临的挑战</h2><ul>
<li>跨多个微服务的请求难以追踪</li>
<li>容器化不熟导致的证书和访问控制问题</li>
<li>如何在微服务间共享用户登录状态</li>
<li>多语言架构要求每个团队都有一定的安全经验</li>
</ul>
<p>matrix关注两个关键要素，一个是时间窗口，一个是指标，如流量，每秒钟有多少个请求。</p>
<p>日志、matrix、tracing都是难点。</p>
<h2 id="常见的微服务安全整体架构"><a href="#常见的微服务安全整体架构" class="headerlink" title="常见的微服务安全整体架构"></a>常见的微服务安全整体架构</h2><h2 id="OAuth2协议与微服务安全"><a href="#OAuth2协议与微服务安全" class="headerlink" title="OAuth2协议与微服务安全"></a>OAuth2协议与微服务安全</h2><h2 id="Zuul网关安全"><a href="#Zuul网关安全" class="headerlink" title="Zuul网关安全"></a>Zuul网关安全</h2><h2 id="Gateway网关安全"><a href="#Gateway网关安全" class="headerlink" title="Gateway网关安全"></a>Gateway网关安全</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>oauth2</tag>
        <tag>springcloud</tag>
        <tag>Spring Cloud微服务安全实战</tag>
        <tag>zuul</tag>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring cloud微服务安全实战</title>
    <url>/p/2020/01/30/28310546/</url>
    <content><![CDATA[<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/28310546/702434-20191109181839528-1931241018.png" alt="架构图"></p>
<a id="more"></a>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>springcloud</tag>
        <tag>Spring Cloud微服务安全实战</tag>
      </tags>
  </entry>
  <entry>
    <title>爬大众点评的商铺列表</title>
    <url>/p/2019/05/15/839addd8/</url>
    <content><![CDATA[<p>斗智斗勇，不同时间不同地点的页面内容不同，反爬需要解析的内容也不同。大众点评中商铺列表的最大页数是50页，所以根据分类进行搜索。爬取的时候不需要登录。这次使用的是vscode+anconda+mysql+redis，前期试着用scrapy不通过模拟网页去爬取，需要看运气什么时候能正常获取到页面，这个方法等回头再试试。后来就用request+selenium模拟网页进行爬取，虽然慢，但是不会跳出验证码页面，还是能够接受的。<br><a id="more"></a></p>
<h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><p>访问大众点评的页面时，若没有user-agent与cookie会自动跳转至美团的验证页面，让用户输入验证码。因此访问页面时，需要带着user-agent，先进入首页获取cookie，再跳转至其他页面中。</p>
<h2 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h2><p>可以获取浏览器中的user-agent，也可以百度出user-agent的列表，自己轮换着使用。在scrapy框架下，可以使用scrapy-redis中新建一个user-agent的文件，随机读取。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"</span>,</span><br><span class="line">&#125;</span><br><span class="line">res = current_session.get(<span class="string">"http://www.dianping.com/zhengzhou/ch10/g112r2038"</span>, headers=headers)</span><br></pre></td></tr></table></figure></p>
<h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>使用request库时请求链接，可以从selenium打开的浏览器中获取cookie，也可以直接用request请求主页，然后记录cookie传递给下一个request。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">current_session = requests.session()</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">"http://www.dianping.com/zhengzhou"</span>)</span><br><span class="line">cookie = browser.get_cookies() <span class="comment">#获取浏览器cookies</span></span><br><span class="line">c = requests.cookies.RequestsCookieJar()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cookie: <span class="comment">#添加cookie到CookieJar</span></span><br><span class="line">    c.set(i[<span class="string">"name"</span>], i[<span class="string">"value"</span>])</span><br><span class="line"></span><br><span class="line">current_session.cookies.update(c) <span class="comment">#更新session里的cookie</span></span><br></pre></td></tr></table></figure>
<h2 id="延时"><a href="#延时" class="headerlink" title="延时"></a>延时</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="顺藤摸瓜"><a href="#顺藤摸瓜" class="headerlink" title="顺藤摸瓜"></a>顺藤摸瓜</h1><p>解析页面的时候发现使用图片代替文字和使用文字字体两种进行反爬。使用图片代替文字的解决思路主要是通过css找到文字出现在svg图片中的位置(x,y)。字体的话，就只能依靠辅助方式来把文字识别出来了。</p>
<h2 id="图片代替文字"><a href="#图片代替文字" class="headerlink" title="图片代替文字"></a>图片代替文字</h2><h3 id="数字反爬"><a href="#数字反爬" class="headerlink" title="数字反爬"></a>数字反爬</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/659d3d1eae514e9019021deea4d88393.png?x-oss-process=style/blog" alt="商铺列表1"><br>chrome里按F12，在调试页面可以看到css中引用了背景图片，先找到这个css文件和svg文件。<br>css中记录的相对位置：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/bc6231b5b69c957277d1b945a8cec58e.png?x-oss-process=style/blog" alt="css文件"></p>
<p>再看看svg的源码如下：<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/abf9640395f4145c477b7ca6317594b7.png?x-oss-process=style/blog" alt="svg文件1"></p>
<p>现将css中这个svg相关的样式都提出来，形成各字典(样式名称,x,y)，解析svg文件，将text标签中的y值和文字提取出来。由于svg中使用的字的大小都是12px，所以先确定css中的样式在svg的哪一行，再确定是第几个就行了。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svg_num_dict</span><span class="params">(css_list,svg_html)</span>:</span></span><br><span class="line">    <span class="comment"># css中的样式进行格式转换</span></span><br><span class="line">    css_list = [[i[<span class="number">0</span>], abs(float(i[<span class="number">1</span>])), abs(float(i[<span class="number">2</span>]))] <span class="keyword">for</span> i <span class="keyword">in</span> css_list ]</span><br><span class="line">    </span><br><span class="line">    digits_list = re.findall(<span class="string">r'&gt;(\d+)&lt;'</span>, svg_html) <span class="comment"># 解析svg中的数字</span></span><br><span class="line">    ys=re.findall(<span class="string">r'y="(\d+)"&gt;'</span>,svg_html) <span class="comment"># 解析svg中的y值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> css_list:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(ys)):</span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">2</span>]&lt;= int(ys[j]):</span><br><span class="line">                other_dic[i[<span class="number">0</span>]]=digits_list[j][int(i[<span class="number">1</span>])//<span class="number">12</span>]  <span class="comment"># 存入other_dic全局字典中</span></span><br><span class="line">                <span class="comment"># redis_con.set(i[0], digits_list[j][int(i[1])//12])  # 存入redis中</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<h3 id="文字反爬"><a href="#文字反爬" class="headerlink" title="文字反爬"></a>文字反爬</h3><p>目前在大众点评中见到的是两种，在页面中体现的方式都一样，用svgmtsi包裹着，用class中的前缀来区分是使用哪个svg，css中样式提取与形成字典的方式与数字的差不多。主要区别在于svg的解析方式不太一样。一种svg中使用的标签是text，另一种使用的是path和textPath。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/c0c72e3d881de5e78ee9282714f6d0e7.png?x-oss-process=style/blog" alt="商铺列表2"></p>
<h4 id="text标签"><a href="#text标签" class="headerlink" title="text标签"></a>text标签</h4><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/be03d96c163c80ecd9f8b622d5e06b51.png?x-oss-process=style/blog" alt="svg文件2"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svg_font_dict</span><span class="params">(css_list,svg_html)</span>:</span></span><br><span class="line">    <span class="comment"># css中的样式进行格式转换</span></span><br><span class="line">    css_list = [[i[<span class="number">0</span>], abs(float(i[<span class="number">1</span>])), abs(float(i[<span class="number">2</span>]))] <span class="keyword">for</span> i <span class="keyword">in</span> css_list ]</span><br><span class="line">    </span><br><span class="line">    digits_list = re.findall(<span class="string">r'&lt;text x="\d+" y="\d+"&gt;(.*?)&lt;'</span>, svg_html)</span><br><span class="line">    ys=re.findall(<span class="string">r'y="(\d+)"&gt;'</span>,svg_html)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> css_list:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(ys)):</span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">2</span>]&lt;= int(ys[j]):</span><br><span class="line">                other_dic[i[<span class="number">0</span>]]=digits_list[j][int(i[<span class="number">1</span>])//<span class="number">12</span>]  <span class="comment"># 存入other_dic全局字典中</span></span><br><span class="line">                <span class="comment"># redis_con.set(i[0], digits_list[j][int(i[1])//12]) # 存入redis中</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="textPath标签"><a href="#textPath标签" class="headerlink" title="textPath标签"></a>textPath标签</h4><p>通过path知道x与y的值<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svg_dict</span><span class="params">(css_list,svg_html)</span>:</span></span><br><span class="line">    <span class="comment"># css中的样式进行格式转换</span></span><br><span class="line">    css_list = [[i[<span class="number">0</span>], abs(float(i[<span class="number">1</span>])), abs(float(i[<span class="number">2</span>]))] <span class="keyword">for</span> i <span class="keyword">in</span> css_list ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解析textPath标签</span></span><br><span class="line">    svg_text_r = <span class="string">r'&lt;textPath xlink:href="(.*?)" textLength="(.*?)"&gt;(.*?)&lt;/textPath&gt;'</span></span><br><span class="line">    svg_text_re = re.findall(svg_text_r, svg_html)</span><br><span class="line">    </span><br><span class="line">    dict_avg = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> svg_text_re:</span><br><span class="line">        dict_avg[int(data[<span class="number">0</span>].replace(<span class="string">"#"</span>, <span class="string">""</span>))<span class="number">-1</span>] = list(data[<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解析path标签</span></span><br><span class="line">    svg_y_r = <span class="string">r'&lt;path id="(.*?)" d="(.*?) (.*?) (.*?)"/&gt;'</span></span><br><span class="line">    svg_y_re = re.findall(svg_y_r, svg_html)</span><br><span class="line">    list_y = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> svg_y_re:</span><br><span class="line">        list_y.append(data[<span class="number">2</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> css_list:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(list_y)):</span><br><span class="line">            <span class="keyword">if</span> i[<span class="number">2</span>]&lt;= int(list_y[j]):</span><br><span class="line">                other_dic[i[<span class="number">0</span>]]=dict_avg[j][int(i[<span class="number">1</span>])//<span class="number">12</span>]  <span class="comment"># 存入other_dic全局字典中</span></span><br><span class="line">                <span class="comment"># redis_con.set(i[0], dict_avg[j][int(i[1])//12]) # 存入redis中</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<h2 id="字体代替文字"><a href="#字体代替文字" class="headerlink" title="字体代替文字"></a>字体代替文字</h2><h3 id="字体反爬"><a href="#字体反爬" class="headerlink" title="字体反爬"></a>字体反爬</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/dbab07735d82589f5fd640ee9ccc51e3.png?x-oss-process=style/blog" alt="商铺列表3"><br>查看源代码如下：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"addr"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe457;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xf0c8;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe0e4;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xf392;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    优</span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xea3a;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xeae4;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe0e4;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe0ce;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe057;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xf230;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe235;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xf616;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    100</span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe2cd;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xe0e4;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">svgmtsi</span> <span class="attr">class</span>=<span class="string">"address"</span>&gt;</span><span class="symbol">&amp;#xf03d;</span><span class="tag">&lt;/<span class="name">svgmtsi</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>唯一突破口就是css中的address，发现他的唯一属性就是有个字体。所以去css的文件中搜索这个字体，发现使用了自定义字体。<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/63cb1b3d3d5310566b2deb07035f328c.png?x-oss-process=style/blog" alt="css文件"></p>
<blockquote>
<p>在新的 @font-face 规则中，您必须首先定义字体的名称（比如 myFirstFont），然后指向该字体文件。</p>
</blockquote>
<p>然后下载字体，通过FontCreator或者FontEditor打开字体文件，我这里用了百度AI里开放API，将打开后的字体文件截图后进行文字识别，获取下面代码中的list数组。<br>将字体与编号组成字典。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_font</span><span class="params">()</span>:</span></span><br><span class="line">    font2 = TTFont(<span class="string">'./font/f1c26632.woff'</span>)</span><br><span class="line">    keys = font2[<span class="string">'glyf'</span>].keys()</span><br><span class="line">    values =list(<span class="string">' .1234567890店中美家馆小车大市公酒行国品发电金心业商司超生装园场食有新限天面工服海华水房饰城乐汽香部利子老艺花专东肉菜学福饭人百餐茶务通味所山区门药银农龙停尚安广鑫一容动南具源兴鲜记时机烤文康信果阳理锅宝达地儿衣特产西批坊州牛佳化五米修爱北养卖建材三会鸡室红站德王光名丽油院堂烧江社合星货型村自科快便日民营和活童明器烟育宾精屋经居庄石顺林尔县手厅销用好客火雅盛体旅之鞋辣作粉包楼校鱼平彩上吧保永万物教吃设医正造丰健点汤网庆技斯洗料配汇木缘加麻联卫川泰色世方寓风幼羊烫来高厂兰阿贝皮全女拉成云维贸道术运都口博河瑞宏京际路祥青镇厨培力惠连马鸿钢训影甲助窗布富牌头四多妆吉苑沙恒隆春干饼氏里二管诚制售嘉长轩杂副清计黄讯太鸭号街交与叉附近层旁对巷栋环省桥湖段乡厦府铺内侧元购前幢滨处向座下県凤港开关景泉塘放昌线湾政步宁解白田町溪十八古双胜本单同九迎第台玉锦底后七斜期武岭松角纪朝峰六振珠局岗洲横边济井办汉代临弄团外塔杨铁浦字年岛陵原梅进荣友虹央桂沿事津凯莲丁秀柳集紫旗张谷的是不了很还个也这我就在以可到错没去过感次要比觉看得说常真们但最喜哈么别位能较境非为欢然他挺着价那意种想出员两推做排实分间甜度起满给热完格荐喝等其再几只现朋候样直而买于般豆量选奶打每评少算又因情找些份置适什蛋师气你姐棒试总定啊足级整带虾如态且尝主话强当更板知己无酸让入啦式笑赞片酱差像提队走嫩才刚午接重串回晚微周值费性桌拍跟块调糕'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> zip(keys, values):</span><br><span class="line">        font_dic[re.sub(<span class="string">r'uni'</span>, <span class="string">r'\\u'</span>, k).encode().decode(<span class="string">'unicode_escape'</span>)]=v</span><br></pre></td></tr></table></figure></p>
<h2 id="获取CSS和对应的SVG"><a href="#获取CSS和对应的SVG" class="headerlink" title="获取CSS和对应的SVG"></a>获取CSS和对应的SVG</h2><p>由于css是随机的，因此每次打开的打开的页面自动获取css和对应的svg的样式。我这里虽然把目前见到的解析都写了，但是需要指定哪一种前缀用哪种解析方式，后续可以改进成为自动判定。<br>然后把css中的每个样式形成个列表，传到需要解析的方法中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_dic</span><span class="params">(html)</span>:</span></span><br><span class="line">    <span class="comment"># 获取页面中使用的css样式的地址</span></span><br><span class="line">    css_url = <span class="string">"http:"</span> + re.search(<span class="string">r'(//.+svgtextcss.+\.css)'</span>, html).group()</span><br><span class="line">    css_res = current_session.get(css_url, headers=headers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据svgmtsi标签，确定svg对应的css样式的前缀和地址</span></span><br><span class="line">    css_list = re.findall(<span class="string">r'svgmtsi(.+)'</span>,<span class="string">'\n'</span>.join(css_res.text.split(<span class="string">'&#125;'</span>)))</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> css_list:</span><br><span class="line">        svg_prefix = re.findall(<span class="string">r'class\^="(\w+)"'</span>, <span class="string">'\n'</span>.join(c.split(<span class="string">'&#125;'</span>)))[<span class="number">0</span>]</span><br><span class="line">        svg_url = <span class="string">"http:"</span> + re.findall(<span class="string">r'class\^="\w+".+(//.+svgtextcss.+\.svg)'</span>, <span class="string">'\n'</span>.join(c.split(<span class="string">'&#125;'</span>)))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        dic[svg_prefix]=svg_url</span><br><span class="line">        svg_res = current_session.get(svg_url, headers=headers)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据前缀合成css样式的字典列表</span></span><br><span class="line">        pattern=<span class="string">'('</span>+svg_prefix+<span class="string">r'\w+)&#123;background:(.+)px (.+)px;'</span></span><br><span class="line">        c_list = re.findall(pattern, <span class="string">'\n'</span>.join(css_res.text.split(<span class="string">'&#125;'</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前缀解析</span></span><br><span class="line">        <span class="keyword">if</span> svg_prefix==<span class="string">'gvj'</span> :</span><br><span class="line">            svg_num_dict(c_list,svg_res.text)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> svg_prefix==<span class="string">'bwh'</span>:</span><br><span class="line">            svg_font_dict(c_list,svg_res.text)</span><br></pre></td></tr></table></figure>
<h2 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h2><p>由于使用图片代替文字的css翻译的字典存成了一个，所以用一个方法进行解析。而字体的class都是一个，主要根据内容不同来解析，所以单独写了一下。可以使用全局字典存储这些字典，也可以使用redis存储这些字典。替换成redis后，会面临这些值取出来以后是byte类型的，需要转换，经过byte、str、urf-8、unicode来回折腾后，我还是决定用全局字典方便点。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用图片代替文字的替换</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_keyword</span><span class="params">(html)</span>:</span></span><br><span class="line">    review_kw=re.findall(<span class="string">r'&lt;svgmtsi class="(.*?)"&gt;'</span>,html)</span><br><span class="line">    <span class="keyword">for</span> kw <span class="keyword">in</span> review_kw:</span><br><span class="line">        html=html.replace(<span class="string">'&lt;svgmtsi class="'</span>+kw+<span class="string">'"&gt;&lt;/svgmtsi&gt;'</span>,other_dic[kw])</span><br><span class="line">    <span class="keyword">return</span> str(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用文字字体的替换</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_keyword_font</span><span class="params">(html)</span>:</span></span><br><span class="line">    tag=re.findall(<span class="string">r'&lt;svgmtsi class="(.*?)"&gt;'</span>,html)[<span class="number">0</span>]</span><br><span class="line">    address_kw=re.findall(<span class="string">r'&lt;svgmtsi class="\w+"&gt;(.*?)&lt;'</span>,html)</span><br><span class="line">    <span class="keyword">for</span> kw <span class="keyword">in</span> address_kw:</span><br><span class="line">        html=html.replace(<span class="string">'&lt;svgmtsi class="'</span>+tag+<span class="string">'"&gt;'</span>+kw+<span class="string">'&lt;/svgmtsi&gt;'</span>,font_dic[kw])</span><br><span class="line">    <span class="keyword">return</span> str(html)</span><br></pre></td></tr></table></figure></p>
<h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><h2 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h2><h3 id="搜索结果为空"><a href="#搜索结果为空" class="headerlink" title="搜索结果为空"></a>搜索结果为空</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/b22873dbdc13c176cb999d252a7293cc.png?x-oss-process=style/blog" alt="搜索结果为空"></p>
<p>在点评中表现为整个content-wrap中只有div[@class=not-found]一个div，所以直接判断是否这个div是否存在。</p>
<h3 id="解析的某个内容为空"><a href="#解析的某个内容为空" class="headerlink" title="解析的某个内容为空"></a>解析的某个内容为空</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/6c04ffe3f631252752e730fbdd4a19af.png?x-oss-process=style/blog" alt="商铺列表"></p>
<p>在解析页面内容的时候会有很多判断，大部分都是判断某个标签是否存在即可。</p>
<h2 id="获取页数"><a href="#获取页数" class="headerlink" title="获取页数"></a>获取页数</h2><p>由于进入页面后，首先获取当前搜索结果的页数，因此把搜索结果为空的判断加在获取页数的前面了。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_size</span><span class="params">()</span>:</span></span><br><span class="line">    content=browser.find_element_by_xpath(<span class="string">'//*[@class="content-wrap"]'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">    <span class="comment"># 判断是否搜索结果为空</span></span><br><span class="line">    <span class="keyword">if</span> re.search(<span class="string">r'class="not-found"'</span>,str(content))==<span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 若仅有一页，没有页码条</span></span><br><span class="line">        <span class="keyword">if</span> re.search(<span class="string">r'class="page"'</span>,str(content))==<span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pages=browser.find_elements_by_xpath(<span class="string">'//*[@class="page"]/a[@class="PageLink"]'</span>)</span><br><span class="line">            list=[]</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> pages:</span><br><span class="line">                list.append(int(page.text))</span><br><span class="line">            <span class="comment"># 若有页码条，找出页码的最大值</span></span><br><span class="line">            <span class="keyword">return</span> max(list)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h2 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h2><p>通过redis的一个set对重复数据进行过滤。如果set用这个商铺的id，则跳过解析，若没有这个商铺的id，解析保存到数据库中后，在set中新增这个id。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> redis_con.sismember(<span class="string">'shops'</span>, sid)==<span class="literal">False</span>:</span><br><span class="line">    shop=parse_item(item)</span><br><span class="line">    insert_shop(shop)</span><br><span class="line">    redis_con.sadd(<span class="string">"shops"</span>,sid)</span><br></pre></td></tr></table></figure></p>
<h1 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h1><h2 id="分类与地点"><a href="#分类与地点" class="headerlink" title="分类与地点"></a>分类与地点</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/4d152e205d10d7132bd9a5c9ed8a42ec.png?x-oss-process=style/blog" alt="分类与地点"></p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">category_classfy</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">"http://www.dianping.com/zhengzhou"</span>)</span><br><span class="line">    browser.get(<span class="string">"http://www.dianping.com/zhengzhou/ch10/g112"</span>)</span><br><span class="line">    classfies=browser.find_elements_by_xpath(<span class="string">'//*[@id="classfy"]/a'</span>)</span><br><span class="line">    print(<span class="string">"len:"</span>+str(len(classfies)<span class="number">-1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">14</span>,len(classfies)<span class="number">-1</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i==<span class="number">18</span>:</span><br><span class="line">            browser.find_element_by_xpath(<span class="string">'//a[@class="more J_packdown"]'</span>).click()</span><br><span class="line">            </span><br><span class="line">        classfy=browser.find_elements_by_xpath(<span class="string">'//*[@id="classfy"]/a'</span>)[i]</span><br><span class="line">        cat_id=classfy.get_attribute(<span class="string">"data-cat-id"</span>)</span><br><span class="line">        cid=<span class="string">'g'</span>+cat_id</span><br><span class="line">        name=classfy.get_attribute(<span class="string">"data-click-name"</span>).split(<span class="string">'_'</span>)[<span class="number">2</span>]</span><br><span class="line">        url=classfy.get_attribute(<span class="string">"href"</span>)</span><br><span class="line">        insert_category_classfy(cat_id,cid,name,url,<span class="literal">None</span>)</span><br><span class="line">        classfy.click()</span><br><span class="line">        flag=is_element_by_xpath_exist(<span class="string">'//*[@id="classfy-sub"]/a'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> flag==<span class="literal">True</span>:</span><br><span class="line">            classfies_sub=browser.find_elements_by_xpath(<span class="string">'//*[@id="classfy-sub"]/a'</span>)</span><br><span class="line">            <span class="comment"># if classfies_sub!=None and len(classfies_sub)&gt;0:</span></span><br><span class="line">            <span class="keyword">for</span> sub <span class="keyword">in</span> classfies_sub:</span><br><span class="line">                sub_name=sub.get_attribute(<span class="string">"data-click-name"</span>).split(<span class="string">'_'</span>)[<span class="number">3</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'all'</span>!=sub_name:</span><br><span class="line">                    cat_sub_id=sub.get_attribute(<span class="string">"data-cat-id"</span>)</span><br><span class="line">                    sub_cid=<span class="string">'g'</span>+cat_sub_id</span><br><span class="line">                    sub_url=sub.get_attribute(<span class="string">"href"</span>)</span><br><span class="line">                    insert_category_classfy(cat_sub_id,sub_cid,sub_name,sub_url,cid)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h3 id="地点"><a href="#地点" class="headerlink" title="地点"></a>地点</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">category_region</span><span class="params">(n,root_url,tip,xpath_p,xpath_s)</span>:</span></span><br><span class="line">    browser.get(<span class="string">"http://www.dianping.com/zhengzhou"</span>)</span><br><span class="line">    browser.get(root_url)</span><br><span class="line">    bussi=browser.find_elements_by_xpath(xpath_p) <span class="comment">#('//*[@id="bussi-nav"]/a')</span></span><br><span class="line">    time.sleep(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    browser.find_elements_by_xpath(<span class="string">'//*[@id="J_nav_tabs"]/a'</span>)[n].click()</span><br><span class="line">    print(<span class="string">"len:"</span>+str(len(bussi)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(bussi)):</span><br><span class="line">        browser.get(root_url)</span><br><span class="line">        browser.find_elements_by_xpath(<span class="string">'//*[@id="J_nav_tabs"]/a'</span>)[n].click()</span><br><span class="line">        bussi=browser.find_elements_by_xpath(xpath_p)[i]</span><br><span class="line">        cat_id=bussi.get_attribute(<span class="string">"data-cat-id"</span>)</span><br><span class="line">        cid=<span class="string">'r'</span>+cat_id</span><br><span class="line">        name=bussi.get_attribute(<span class="string">"data-click-title"</span>)</span><br><span class="line">        url=bussi.get_attribute(<span class="string">"href"</span>)</span><br><span class="line">        insert_category_region(cat_id,cid,name,url,<span class="literal">None</span>,tip)</span><br><span class="line">        bussi.click()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> i==<span class="number">2</span>:</span><br><span class="line">            <span class="comment"># if i==34:</span></span><br><span class="line">            browser.find_element_by_xpath(xpath_s+<span class="string">'[@class="more J_packdown"]'</span>).click()</span><br><span class="line">        flag=is_element_by_xpath_exist(xpath_s) <span class="comment">#('//*[@id="bussi-nav-sub"]/a')</span></span><br><span class="line">        print(name+<span class="string">":"</span>+str(flag))</span><br><span class="line">        <span class="keyword">if</span> flag==<span class="literal">True</span>:</span><br><span class="line">            bussi_sub=browser.find_elements_by_xpath(xpath_s)</span><br><span class="line">            <span class="comment"># if classfies_sub!=None and len(classfies_sub)&gt;0:</span></span><br><span class="line">            <span class="keyword">for</span> sub <span class="keyword">in</span> bussi_sub:</span><br><span class="line">                cat_sub_id=sub.get_attribute(<span class="string">"data-cat-id"</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'0'</span>!=cat_sub_id <span class="keyword">and</span> cat_sub_id!=<span class="literal">None</span>:</span><br><span class="line">                    sub_name=sub.text</span><br><span class="line">                    sub_cid=<span class="string">'r'</span>+cat_sub_id</span><br><span class="line">                    sub_url=sub.get_attribute(<span class="string">"href"</span>)</span><br><span class="line">                    insert_category_region(cat_sub_id,sub_cid,sub_name,sub_url,cid,tip)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">"http://www.dianping.com/zhengzhou"</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    cookie = browser.get_cookies() <span class="comment">#获取浏览器cookies</span></span><br><span class="line">    c = requests.cookies.RequestsCookieJar()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> cookie: <span class="comment">#添加cookie到CookieJar</span></span><br><span class="line">        c.set(i[<span class="string">"name"</span>], i[<span class="string">"value"</span>])</span><br><span class="line">    current_session.cookies.update(c) <span class="comment">#更新session里的cookie</span></span><br><span class="line"></span><br><span class="line">    res = current_session.get(<span class="string">"http://www.dianping.com/zhengzhou/ch10/g112r2038"</span>, headers=headers)</span><br><span class="line">    <span class="comment"># 初始化css字典</span></span><br><span class="line">    init_dic(res.text)</span><br><span class="line">    init_font()</span><br></pre></td></tr></table></figure>
<h2 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(base_url)</span>:</span></span><br><span class="line">    browser.get(base_url)</span><br><span class="line">    <span class="comment"># 获取页码</span></span><br><span class="line">    pageSize=parse_page_size()</span><br><span class="line">    <span class="keyword">if</span> pageSize!=<span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(pageSize):</span><br><span class="line">            print(i)</span><br><span class="line">            <span class="comment"># 翻页</span></span><br><span class="line">            <span class="keyword">if</span> i!=<span class="number">0</span>:</span><br><span class="line">                url=base_url+<span class="string">"p"</span>+str(i+<span class="number">1</span>)</span><br><span class="line">                browser.get(url)</span><br><span class="line">            items=browser.find_elements_by_xpath(<span class="string">'//*[@id="shop-all-list"]/ul/li'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 遍历内容信息</span></span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">                tit=item.find_element_by_xpath(<span class="string">'./div[2]/div[@class="tit"]/a'</span>)</span><br><span class="line">                sid=tit.get_attribute(<span class="string">'data-shopid'</span>)</span><br><span class="line">                <span class="comment"># 重复的不再解析</span></span><br><span class="line">                <span class="keyword">if</span> redis_con.sismember(<span class="string">'shops'</span>, sid)==<span class="literal">False</span>:</span><br><span class="line">                    shop=parse_item(item)</span><br><span class="line">                    insert_shop(shop)</span><br><span class="line">                    redis_con.sadd(<span class="string">"shops"</span>,sid)</span><br></pre></td></tr></table></figure>
<h2 id="解析信息"><a href="#解析信息" class="headerlink" title="解析信息"></a>解析信息</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(item)</span>:</span></span><br><span class="line">    shop=&#123;&#125;</span><br><span class="line">    <span class="comment"># 商铺ID，商铺名称，商铺URL</span></span><br><span class="line">    tit=item.find_element_by_xpath(<span class="string">'./div[2]/div[@class="tit"]/a'</span>)</span><br><span class="line">    shop[<span class="string">'id'</span>]=tit.get_attribute(<span class="string">'data-shopid'</span>)</span><br><span class="line">    shop[<span class="string">'name'</span>]=tit.get_attribute(<span class="string">'title'</span>)</span><br><span class="line">    shop[<span class="string">'url'</span>]=tit.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  是否有团购、外卖信息</span></span><br><span class="line">    promoes=item.find_elements_by_xpath(<span class="string">'./div[2]/div[@class="tit"]/div[@class="promo-icon J_promo_icon"]/a'</span>)</span><br><span class="line">    <span class="keyword">for</span> promo <span class="keyword">in</span> promoes:</span><br><span class="line">        data_click_name=promo.get_attribute(<span class="string">'data-click-name'</span>)</span><br><span class="line">        <span class="keyword">if</span> data_click_name==<span class="string">'shop_group_icon_click'</span>:</span><br><span class="line">        group_title=promo.get_attribute(<span class="string">'title'</span>)</span><br><span class="line">            shop[<span class="string">'groupdeal'</span>] = re.findall(<span class="string">r'.*(\d+).*'</span>, group_title)[<span class="number">0</span>] <span class="comment"># 若有团购，记录团购信息的个数</span></span><br><span class="line">        <span class="keyword">if</span> data_click_name==<span class="string">'shop_icon_takeaway_click'</span>:</span><br><span class="line">            shop[<span class="string">'takeway'</span>]=<span class="string">'1'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  评分</span></span><br><span class="line">    comment=item.find_element_by_xpath(<span class="string">'./div[2]/div[@class="comment"]'</span>)</span><br><span class="line">    stars=comment.find_element_by_xpath(<span class="string">'./span'</span>)</span><br><span class="line">    shop[<span class="string">'stars'</span>]=stars.get_attribute(<span class="string">'title'</span>)</span><br><span class="line">    star_num_class=stars.get_attribute(<span class="string">'class'</span>)</span><br><span class="line">    shop[<span class="string">'stars_num'</span>]=re.findall(<span class="string">r'.*-str(\d+).*'</span>,star_num_class)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  评价条数</span></span><br><span class="line">    review_btn=comment.find_element_by_xpath(<span class="string">'./a[@data-click-name="shop_iwant_review_click"]'</span>)</span><br><span class="line">    <span class="keyword">if</span> re.search(<span class="string">r'&lt;(.*)&gt;'</span>,str(review_btn.get_attribute(<span class="string">'innerHTML'</span>)))!=<span class="literal">None</span>:</span><br><span class="line">        review=review_btn.find_element_by_xpath(<span class="string">'./b'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">        shop[<span class="string">'reviews'</span>]=replace_keyword(review)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shop[<span class="string">'reviews'</span>]=<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">#  人均消费</span></span><br><span class="line">    avgprice_btn=comment.find_element_by_xpath(<span class="string">'./a[@data-click-name="shop_avgprice_click"]'</span>)</span><br><span class="line">    <span class="keyword">if</span> re.search(<span class="string">r'&lt;(.*)&gt;'</span>,str(avgprice_btn.get_attribute(<span class="string">'innerHTML'</span>)))!=<span class="literal">None</span>:</span><br><span class="line">        avgprice=avgprice_btn.find_element_by_xpath(<span class="string">'./b'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">        shop[<span class="string">'avgprice'</span>]=replace_keyword(avgprice)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shop[<span class="string">'avgprice'</span>]=<span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    tag_addr=item.find_element_by_xpath(<span class="string">'./div[2]/div[@class="tag-addr"]'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  所属分类</span></span><br><span class="line">    classfy_href=tag_addr.find_element_by_xpath(<span class="string">'./a[@data-click-name="shop_tag_cate_click"]'</span>).get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    shop[<span class="string">'classfy'</span>]=classfy_href[int(classfy_href.rfind(<span class="string">'/'</span>))+<span class="number">1</span>:int(len(classfy_href))]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  所属地区</span></span><br><span class="line">    region_href=tag_addr.find_element_by_xpath(<span class="string">'./a[@data-click-name="shop_tag_region_click"]'</span>).get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    shop[<span class="string">'region'</span>]=region_href[int(region_href.rfind(<span class="string">'/'</span>))+<span class="number">1</span>:int(len(region_href))]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  地址</span></span><br><span class="line">    address=tag_addr.find_element_by_xpath(<span class="string">'./span[@class="addr"]'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">    shop[<span class="string">'address'</span>]=replace_keyword_font(address)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  推荐菜</span></span><br><span class="line">    recommends=item.find_elements_by_xpath(<span class="string">'./div[2]/div[@class="recommend"]/a'</span>)</span><br><span class="line">    recommend=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> rc <span class="keyword">in</span> recommends:</span><br><span class="line">        recommends_href=rc.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">        recommends_idx=recommends_href[int(recommends_href.rfind(<span class="string">'/'</span>))+<span class="number">1</span>:int(len(recommends_href))]</span><br><span class="line">        recommend[recommends_idx]=rc.text</span><br><span class="line">    shop[<span class="string">'recommend'</span>]=str(recommend)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  评价：口味、环境、服务</span></span><br><span class="line">    <span class="keyword">if</span> re.search(<span class="string">r'class="comment-list"'</span>,str(item.find_element_by_xpath(<span class="string">'./div[2]'</span>).get_attribute(<span class="string">'innerHTML'</span>)))!=<span class="literal">None</span>:</span><br><span class="line">        comment_lists=item.find_elements_by_xpath(<span class="string">'./div[2]/span[@class="comment-list"]/span'</span>)</span><br><span class="line">        favor=comment_lists[<span class="number">0</span>].find_element_by_xpath(<span class="string">'./b'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">        shop[<span class="string">'comment_favor'</span>]=replace_keyword(favor)</span><br><span class="line">        env=comment_lists[<span class="number">1</span>].find_element_by_xpath(<span class="string">'./b'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">        shop[<span class="string">'comment_env'</span>]=replace_keyword(env)</span><br><span class="line">        service=comment_lists[<span class="number">2</span>].find_element_by_xpath(<span class="string">'./b'</span>).get_attribute(<span class="string">'innerHTML'</span>)</span><br><span class="line">        shop[<span class="string">'comment_service'</span>]=replace_keyword(service)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shop[<span class="string">'comment_favor'</span>]=<span class="literal">None</span></span><br><span class="line">        shop[<span class="string">'comment_env'</span>]=<span class="literal">None</span></span><br><span class="line">        shop[<span class="string">'comment_service'</span>]=<span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"--- shop "</span>+shop[<span class="string">'id'</span>]+<span class="string">"---"</span>)</span><br><span class="line">    <span class="keyword">return</span> shop</span><br></pre></td></tr></table></figure>
<h2 id="主程序"><a href="#主程序" class="headerlink" title="主程序"></a>主程序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 从redis中取分类组合</span></span><br><span class="line">    list=redis_con.lrange(<span class="string">"category"</span>,<span class="number">0</span>,<span class="number">999</span>)</span><br><span class="line">    base=<span class="string">"http://www.dianping.com/zhengzhou/ch10/"</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">        parse_page(base+str(i,encoding=<span class="string">"utf-8"</span>))</span><br><span class="line">        redis_con.lrem(<span class="string">"category"</span>,<span class="number">1</span>,i)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/cf6104896cfa7bd92f44d532c41c50a4.png?x-oss-process=style/blog" alt="商铺分类"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/968e4c464a9f7091aea7f65a869a25d3.png?x-oss-process=style/blog" alt="商铺地点"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/50b3ea8de978f34bdd7512bc88de1365.png?x-oss-process=style/blog" alt="商铺结果"></p>
<h2 id="辅助工具"><a href="#辅助工具" class="headerlink" title="辅助工具"></a>辅助工具</h2><h3 id="Xpath-Helper"><a href="#Xpath-Helper" class="headerlink" title="Xpath Helper"></a>Xpath Helper</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/88ec1c1e373bbce8f4abdd47bf21c6f5.png?x-oss-process=style/blog" alt="Xpath Helper"></p>
<h3 id="FontCreator"><a href="#FontCreator" class="headerlink" title="FontCreator"></a>FontCreator</h3><p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/d4fb291a1831ca5ca0c8a04ee38dd0ed.png?x-oss-process=style/blog" alt="FontCreator"></p>
<p>百度字体编辑器FontEditor<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/839addd8/740b39d67c145ff1e4bc6346b5427d5b.png?x-oss-process=style/blog" alt="FontEditor"></p>
<hr>
<p>参考链接：<br>[1] <a href="https://cuiqingcai.com/4048.html" target="_blank" rel="noopener">小白进阶之Scrapy第三篇（基于Scrapy-Redis的分布式以及cookies池）</a></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title>springcloud-security-sso</title>
    <url>/p/2020/02/23/e2508b6a/</url>
    <content><![CDATA[<h2 id="单点登录基本架构"><a href="#单点登录基本架构" class="headerlink" title="单点登录基本架构"></a>单点登录基本架构</h2><h2 id="授权码认证"><a href="#授权码认证" class="headerlink" title="授权码认证"></a>授权码认证</h2><h2 id="基于session的SSO"><a href="#基于session的SSO" class="headerlink" title="基于session的SSO"></a>基于session的SSO</h2><h2 id="基于token的SSO"><a href="#基于token的SSO" class="headerlink" title="基于token的SSO"></a>基于token的SSO</h2>]]></content>
      <categories>
        <category>TODO</category>
        <category>Java</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>spring security</tag>
        <tag>imooc</tag>
        <tag>oauth2</tag>
        <tag>springcloud</tag>
        <tag>api</tag>
        <tag>Spring Cloud微服务安全实战</tag>
        <tag>sso</tag>
      </tags>
  </entry>
</search>
