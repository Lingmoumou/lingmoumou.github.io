<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lingmoumou.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="任务理论部分 特征选择：信息增益（熵、联合熵、条件熵）、信息增益比、基尼系数 决策树生成：ID3决策树、C4.5决策树、CART决策树（CART分类树、CART回归树） 决策树剪枝 sklearn参数详解  练习部分https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb">
<meta name="keywords" content="datawhale,ml,机器学习,machine learning,decision tree">
<meta property="og:type" content="article">
<meta property="og:title" content="Day4 决策树">
<meta property="og:url" content="https://lingmoumou.github.io/p/2020/01/15/5f13e255/index.html">
<meta property="og:site_name" content="Lingmoumou&#39;s Blog">
<meta property="og:description" content="任务理论部分 特征选择：信息增益（熵、联合熵、条件熵）、信息增益比、基尼系数 决策树生成：ID3决策树、C4.5决策树、CART决策树（CART分类树、CART回归树） 决策树剪枝 sklearn参数详解  练习部分https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-01-31T13:53:26.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Day4 决策树">
<meta name="twitter:description" content="任务理论部分 特征选择：信息增益（熵、联合熵、条件熵）、信息增益比、基尼系数 决策树生成：ID3决策树、C4.5决策树、CART决策树（CART分类树、CART回归树） 决策树剪枝 sklearn参数详解  练习部分https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb">

<link rel="canonical" href="https://lingmoumou.github.io/p/2020/01/15/5f13e255/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Day4 决策树 | Lingmoumou's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lingmoumou's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">きっといつかって愿うまま</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lingmoumou.github.io/p/2020/01/15/5f13e255/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ling Moumou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lingmoumou's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Day4 决策树
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-01-15 21:03:57" itemprop="dateCreated datePublished" datetime="2020-01-15T21:03:57+08:00">2020-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-31 21:53:26" itemprop="dateModified" datetime="2021-01-31T21:53:26+08:00">2021-01-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Datawhale/" itemprop="url" rel="index"><span itemprop="name">Datawhale</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Datawhale/初级算法梳理/" itemprop="url" rel="index"><span itemprop="name">初级算法梳理</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><ul>
<li>特征选择：信息增益（熵、联合熵、条件熵）、信息增益比、基尼系数</li>
<li>决策树生成：ID3决策树、C4.5决策树、CART决策树（CART分类树、CART回归树）</li>
<li>决策树剪枝</li>
<li>sklearn参数详解</li>
</ul>
<h3 id="练习部分"><a href="#练习部分" class="headerlink" title="练习部分"></a>练习部分</h3><p><a href="https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/初级算法梳理/Task4_decision_tree.ipynb</a></p>
<p>利用sklearn解决分类问题和回归预测。</p>
<ul>
<li>sklearn.tree.DecisionTreeClassifier</li>
<li>sklearn.tree.DecisionTreeRegressor</li>
</ul>
<a id="more"></a>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树是一种树型结构的机器学习算法,它每个节点验证数据一个属性,根据该属性进行分割数据,将数据分布到不同的分支上,直到叶子节点,叶子结点上表示该样本的label。 每一条从根节点到叶子节点的路径表示分类[回归]的规则。 下面我们先来看看sklearn中决策树怎么用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris, load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类树</span></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Classifier Score:"</span>, clf.score(X_test, y_test)) <span class="comment"># Classifier Score: 1.0</span></span><br><span class="line"></span><br><span class="line">tree.plot_tree(clf.fit(X, y)) </span><br><span class="line">plt.show() <span class="comment"># pic1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回归树</span></span><br><span class="line">X, y = load_boston(return_X_y=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">clf = tree.DecisionTreeRegressor()</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Regression Score:"</span>, clf.score(X_test, y_test)) <span class="comment"># Regression Score: 0.4830393038746458</span></span><br><span class="line"></span><br><span class="line">tree.plot_tree(clf.fit(X, y)) </span><br><span class="line">plt.show() <span class="comment"># pic2</span></span><br></pre></td></tr></table></figure>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/pic1.png" alt="pic1"></p>
<p><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/pic2.png" alt="pic2"></p>
<h3 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h3><p>首先先来几个概念,我们后面介绍决策树原理的时候会提到,这里可以先扫一眼,用到的时候再回来看。</p>
<h4 id="熵和信息熵"><a href="#熵和信息熵" class="headerlink" title="熵和信息熵"></a>熵和信息熵</h4><p>熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。 可以看出,熵表示的是体系的不确定性大小。 熵越大, 物理的不确定性越大。 1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。 同理, 信息熵越小,数据的稳定性越好,我们更加相信此时数据得到的结论。 换言之, 我们现在目的肯定熵越小,机器学习得到的结果越准确。</p>
<p>信息熵表示随机变量不确定性的度量,设随机标量X是一个离散随机变量，其概率分布为:<br>$$P(X=x_i)=p_i, i=1,2,…,n$$<br>则随机变量X的熵定义为:<br>$$H(X)=-\sum_{i=1}^{n}p_ilog{p_i}$$<br>熵越大，随机变量的不确定性就越大，当$p_i=\frac{1}{n}$时， 随机变量的熵最大等于logn，故$0 \leq H(P) \leq logn$。</p>
<h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><p>条件熵就是在给定X的条件的情况下，随机标量Y的条件，记作$H(Y|X)$，可以结合贝叶斯公式进行理解，定义如下<br>$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$<br>这里$p_i=P(X=x_i),i=1,2,…,n$。 一般在基于数据的估计中，我们使用的基于极大似然估计出来的经验熵和经验条件熵。</p>
<h4 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h4><p>联合熵是相对两个及其以上的变量而言的, 两个变量X和Y的联合信息熵为:<br>$$ H(X,Y)=-\sum_x \sum_y P(x,y)log_2[P(x,y)] $$<br>其中: x和y是X和Y的特定值, 相应地, 是这些值一起出现的联合概率, 若为0, 则定义为0。</p>
<p>对于两个以上的变量$X_1,…,X_n$,一般形式位:<br>$$H(X_1,…,X_n)=-\sum_{x_1}\cdot \cdot \cdot\sum_{x_n}P(x1,…,x_n)log_2[P(x_1,…,x_n)]$$</p>
<p>性质:</p>
<ul>
<li>大于每个独立的熵<br>$$H(X,Y) \geq max(H(X),H(Y))$$<br>$$H(X_1,…,X_n) \geq max(H(X_1),…,H(X_n))$$</li>
<li>小于独立熵的和<br>$$H(X_1,…,X_n) \leq sum(H(X_1),…,H(X_n))$$</li>
<li>和条件熵的关系<br>$$H(Y|X)=H(X,Y)-H(X)$$</li>
<li>和互信息的关系<br>$$I(Y;X)=H(X)+H(Y)-H(X,Y)=H(Y)-(H(X,Y)-H(X))$$</li>
</ul>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>信息增益又叫互信息,它表示的是在的得知特征X的信息后,使得类Y的信息的不确定性(熵)减少的程度。<br>$$g(Y,X)=H(Y)-H(Y|X)$$</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>基尼指数又称基尼系数或者基尼不纯度,基尼系数是指国际上通用的、用以衡量一个国家或地区居民收入差距的常用指标。 在信息学中,例如分类问题, 假设有K个类,样本点属于第k类的概率是$p_k$,则该概率分布的基尼指数定义为:<br>$$Gini(p)=\sum_k^Kp_k(1-p_k)=1-\sum_k^Kp_k^2$$</p>
<h3 id="决策树解释"><a href="#决策树解释" class="headerlink" title="决策树解释"></a>决策树解释</h3><p>决策树是什么东西？就是我们平常所说的if-then条件，我们把它组合成树的结构。 决策树中有两种结点，叶子结点和非叶子结点。 其中非叶节点代表的条件，叶子结点表示的实例所属的类别。</p>
<p>我们如何生成这个决策树呢，最主要的一点就是选择那个特征作为当前树的分割结点，这就叫做特征选择，有了特征选择就有了决策树的生成，最后我们还有进行决策树剪枝(后面会提到为什么剪枝)。</p>
<p>看个统计学习方法上的例子:</p>
<p>现在我们有下面一张表的数据，想生成一个决策树模型，预测某个人是否符合贷款条件。<br>现在假如我们通过”某种方法”构造了一颗下面的决策树。 从下图可以看到特征对应的是非叶子结点，如果这个被分到这个叶节点，就预测为这个叶节点的类别。 从图中我们可以知道以下两点:</p>
<ul>
<li>每一个叶子节点都对应一条从根节点到叶节点的规则，这表示决策树是if-then规则的集合</li>
<li>如果一个实例到达了某个叶节点，一定表示它满足了从根节点到该叶子节点的所有条件，而后才得到类别，这不就是先满足条件再得到概率嘛，我们一般叫做条件概率分布。</li>
</ul>
<blockquote>
<p><strong>问题</strong>:为什么我们要选择是否有房子作为第一个构造特征呢？我们构造学习模型，会遵守经验风险最小化或者似然函数极大规则，选择损失函数，我们如何根据风险最小化，选择特征呢？</p>
</blockquote>
<h3 id="ID3-amp-C4-5"><a href="#ID3-amp-C4-5" class="headerlink" title="ID3&amp;C4.5"></a>ID3&amp;C4.5</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>给定训练数据集</p>
<p>$$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$$<br>其中，$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$特征向量，n是特征的个数，$y_i \in {1,2,…,K}$表示类别. N是样本个数. 基于这个数据生成决策树模型.</p>
<h4 id="决策树-1"><a href="#决策树-1" class="headerlink" title="决策树"></a>决策树</h4><p>常见的决策树模型有以下三种(CART决策树既可以做分类也可以做回归):</p>
<ol>
<li><strong>ID3</strong>: 使用信息增益准则选择特征, 相当于用极大似然法进行概率模型选择。 </li>
<li><strong>C4.5</strong>: 和ID3算法相似, 只是用信息增益比选择特征。 </li>
<li><strong>CART</strong>: 递归构建二叉决策树, 回归树:使用平方误差; 分类树:使用基尼指数。</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left">model</th>
<th style="text-align:left">feature select</th>
<th style="text-align:center">树的类型</th>
<th style="text-align:left">计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ID3</td>
<td style="text-align:left">{分类:信息增益}</td>
<td style="text-align:center">多叉树</td>
<td style="text-align:left">$g(D,A)=H(D)-H(D)-H(D\mid\mid A)$</td>
</tr>
<tr>
<td style="text-align:left">C4.5</td>
<td style="text-align:left">{分类:信息增益比}</td>
<td style="text-align:center">多叉树</td>
<td style="text-align:left">$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$</td>
</tr>
<tr>
<td style="text-align:left">CART</td>
<td style="text-align:left">{回归:平方误差;分类:基尼指数}</td>
<td style="text-align:center">二叉树</td>
<td style="text-align:left">$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$</td>
</tr>
</tbody>
</table>
<p>其中, $H_A(D)=H(D\mid A)$。从表格中，我们总结(ID3,C4.5)决策树算法伪代码:</p>
<blockquote>
<p><strong>输入</strong>：数据集D，特征集合A，阈值e<br><strong>输出</strong>：决策树T<br>1：如果D中所有实例输出同一类$C_k$, 则T作为单节点树，并将类$C_k$作为该节点的类标记，返回T；<br>2: 若$A=\varnothing$,则T为单节点树，将D中实例数最多的类$C_k$作为该节点的类标记，返回T；<br>3: 否则，根据<strong>信息增益</strong>(ID3)或者<strong>信息增益比</strong>(C4.5)计算特征A对D的值，选择当前最优的特征$A_g$；<br>4: 如果$A_g$的信息增益小于阈值e，则置T为单节点数，并将$D$中实例最多的类$C_k$作为当前的类标记，返回T；<br>5: 否则，根据$A_g$中的每一个不同的$a_i$,根据$A_g=a_i$将$D$分成若干个非空子集$D_i$，将$D_i$中样本数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$，返回$T$<br>6：对于第i个子节点，以$D_i$为数据集，以$A-{A_g}$为特征集，递归(重复1-5)构造决策树$T_i$,返回$T_i$。<br>7: 对决策树模型T进行剪枝。</p>
</blockquote>
<h4 id="过拟合和剪枝"><a href="#过拟合和剪枝" class="headerlink" title="过拟合和剪枝"></a>过拟合和剪枝</h4><p>决策树建立的过程中,只考虑经验损失最小化,没有考虑结构损失。 因此可能在训练集上表现良好,但是会出现过拟合问题。(我们构造决策树的时候，是完全的在训练数据上得到最优的模型。 这就是过拟合问题，训练误差很小，但是验证集上就不怎么好用。) 为了解决过拟合,我们从模型损失进行考虑:<br>$$模型损失=经验风险最小化+正则项=结构风险最小化$$</p>
<p>思路很简单,给损失函数加上正则项再进行优化。 正则项表示树节点的个数,因此有如下公式:<br>$$C_{\alpha}(T)=C(T)+\alpha|T|$$</p>
<p>进一步详细定义,解决问题:<br>重新定义损失函数，树的叶子节点个数|T|,t是树T的叶节点，该叶节点有$N_t$个样本，其中k类的样本点有$N_{tk}$个，k=1,2，…,K, $H_t(T)$是叶子节点t经验熵，$\alpha \leq 0$是参数,平衡经验损失和正则项，得到计算公式如下:<br>$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$<br>其中，经验熵为:<br>$$H_t(T)=-\sum_{k}\frac{N_{tk}}{H_t}log\frac{N_{tk}}{H_t}$$<br>这是有:<br>$$C_{\alpha}=C(T)+\alpha|T|$$<br>决策树剪枝优化过程考虑了在训练数据上的经验风险最小化和减小模型复杂度两个方向。 因为加了正则项，所有我们基于贪心的思想进行剪枝，因为当剪掉一个树节点，虽然经验风险增加了，但是模型复杂度降低了，我们基于两个方面的平衡进行剪枝，如果剪掉之后，总的风险变小，就进行剪枝。<br>算法:</p>
<blockquote>
<p><strong>输入</strong>: 算法产生的整个决策树，参数$\alpha$<br><strong>输出</strong>：修剪之后的树$T_{\alpha}$<br>1：计算每个节点的经验熵<br>2：递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果:$$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$$表示，剪掉之后，损失减小，就进行剪枝。<br>3：重复2，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。<br>4：动态规划剪枝。</p>
</blockquote>
<p>可以看出来上述算法是一个递归问题,存在很多重复项计算,这里我们使用dfs+备忘录进行加速计算,这种方法和动态规划类似。</p>
<p>算法:</p>
<blockquote>
<p><strong>输入</strong>: 算法产生的整个决策树，参数$\alpha$<br><strong>输出</strong>：修剪之后的树$T_{\alpha}$<br>1：dp[所有树的节点] = {0}; 保留所有几点的信息熵<br>2：计算每个cur_node节点的经验熵, {if dp[cur_node] 直接返回, 否则, 执行2}<br>3：递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果:$$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$$表示，剪掉之后，损失减小，就进行剪枝。<br>$$dp[cur_node] = C_{\alpha}(T_A)$$<br>4：重复2，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。</p>
</blockquote>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>分类与回归树(classification and regression tree, CART)与上述决策树的不同，</p>
<ul>
<li>既可以做分类又可以做回归。</li>
<li>是二叉树，内部节点特征取值，只有yes和no两个选项。同样地，先进行决策树构造，在基于验证集上进行CART决策树的剪枝，既然是回归和分类树，我们就分别介绍回归和分类两种情况。</li>
<li>分类: gini指数。</li>
<li>回归: 平方误差 定义数据格式:</li>
</ul>
<p>$$D={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$$<br>其中，$x_i$是向量，当回归问题时，$y_i$是连续变量; 分类问题时，$y_i$是离散变量。</p>
<h4 id="回归树-Regerssion-Tree"><a href="#回归树-Regerssion-Tree" class="headerlink" title="回归树(Regerssion Tree)"></a>回归树(Regerssion Tree)</h4><p>算法:<br>在训练数据上，根据某一个特征将每个区域划分为两个子区域并决定每个子区域的输出值，递归构建二叉树。</p>
<p>1.选择最优切分变量j和切分点s，求解<br>$$min_{j,s}[min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]$$<br>遍历变量j，对固定的切分变量j扫描所有的s，找到使得上式最小的对(j,s)。<br>2.使用选定的(j,s)划分区域并决定相应的输出值:<br>$$R_1(j,s)={x|x^{(j)} \leq s }, R_2(j,s)={x|x^{(j)}&gt; s },$$<br>$$c_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i, x \in R_m, m=1,2$$<br>3.继续对两个子区域调用1和2，知道满足条件<br>4.将输入空间划分为M个区域$R_1,R_2,…,R_m$,生成决策树:<br>$$f(x)=\sum_{m=1}^{M}c_mI(x \in R_m)$$</p>
<h4 id="分类树-classification-tree"><a href="#分类树-classification-tree" class="headerlink" title="分类树(classification tree)"></a>分类树(classification tree)</h4><p>基尼指数计算公式如下:<br>$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$<br>基于数据D，得到:<br>$$Gini(D)=1-\sum_{k=1}^{K}p_k^2$$<br>其中,$C_k$是D中所属第k类的样本子集，K是类的个数。<br>如果样本集合D根据特征A是否取某一可能取值a被被划分成$D_1$和$D_2$两个部分。<br>$$D_1={(x,y) \in D | A(x)=a }, D_2= D-D_1$$<br>在特征A的条件下，集合D的基尼指数定义为:<br>$$Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)$$<br>基尼指数和熵一样，同样表示集合D的不确定性，基尼指数(Gini(D,A))表示根据调教A=a后的集合D的不确定性，基尼指数越大，表示数据D的不确定性越大。</p>
<p>算法:</p>
<blockquote>
<p><strong>输入</strong>:训练数据D，停止计算的条件<br><strong>输出</strong>:CART决策树<br>1：计算所有特征A的每一个值a对应的条件基尼指数的值，选择最优的划分得到$D_1$和$D_2$。<br>2：递归对两个数据集$D_1$和$D_2$继续调用1，知道满足条件。<br>3：生成CART树的分类树。<br>4：预测的时候，根据决策树，x落到的叶子节点对应的类别表示这个预测x的类别。</p>
</blockquote>
<h4 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h4><p>从上面两个算法的不同可以看出，只是在label的设置和决策节点选择的方式不同，整个架构依然是决策树的常用的架构。 而且上面的树的构建过程，都是基于训练数据的经验风险最小化，没有使用带有正则项的结构风险最小化，这样的模型容易发生过拟合，为了不让模型过拟合，我们需要进行模型的剪枝。</p>
<p><strong>CART树的剪枝有很多难点和有意思的地方让我们开慢慢解开这层面纱</strong><br>CART剪枝算法有两步组成:</p>
<ol>
<li>从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列${T_0,T_1,…,T_n}$。</li>
<li>通过交叉验证的方法在独立的验证数据集上堆子序列进行测试，得到最优子树</li>
</ol>
<p><strong>损失函数</strong>:<br>$$C_{\alpha}(T)=C(T)+\alpha|T|$$<br>其中，T为任意子树，$C(T)$是在训练数据上的预测误差，|T|是树的叶子节点的个数,$\alpha \geq 0$是参数，$C_{\alpha}(T)$是参数$\alpha$是的子树T的整体的损失，参数$\alpha$是平衡训练数据拟合程度和模型复杂度的权重。<br>对于固定的$\alpha$,一定存在使损失函数$C_{\alpha}(T)$最小的子树，将其记作$T_{\alpha}$。 我们可以理解为每一个$\alpha$都对应一个最有子树和最小损失。</p>
<p>而且已经得到证明，可以用递归的方法对树进行剪枝。 将$\alpha$从小增大，$0=\alpha_0&lt;\alpha_1&lt;…&lt;\alpha_n&lt;+\infty$,产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,…,n$；剪枝得到的子树序列对应着区间$\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,…,n$的最有子树序列${T_0,T_1,…,T_n}$。</p>
<p>我们给出算法:</p>
<blockquote>
<p><strong>输入</strong>: CART算法生成的决策树T0.<br><strong>输出</strong>: 最优的决策树T{\alpha}<br>1：设k=0, T=$T_0$。<br>2：设 $\alpha=+\infty$。<br>3：自下而上地对各个内部节点t计算$C(T_t),|T_t|$以及$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$$$$\alpha=min(\alpha,g(t))$$这里，$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶子节点个数。<br>4：自上而下地访问内部节点t，如果有$g(t)=\alpha$,进行剪枝，并堆叶节点t以多数表决方法决定其类(分类，回归使用平均值)，得到树T。<br>5：设$k=k+1,\alpha=\alpha,T_k=T$。<br>6：如果T不是由根节点单独构成的树，则回到步骤4。<br>7：采用交叉验证法在子树序列${T_0,T_1,…,T_n}$中选取最优子树$T_{\alpha}$。</p>
</blockquote>
<p>接下面，我们不去看算法，来看书中给的一段文字的截图，这里截图是因为我要画图，进行比较解释，图中自由理论(哈哈):<br><img data-src="https://lingmoumou-blog.oss-cn-beijing.aliyuncs.com/blog/5f13e255/cart.png" alt></p>
<p>看懂这个图之后，再看算法一气呵成，因为我们假设每一次得到的树都有可能是最优的，所以不能直接去最后一个树，要使用交叉验证选择最有的决策树结构。</p>
<h3 id="问题精选"><a href="#问题精选" class="headerlink" title="问题精选"></a>问题精选</h3><blockquote>
<p>决策树和条件概率分布的关系？</p>
<blockquote>
<p>决策树可以表示成给定条件下类的条件概率分布。 决策树中的每一条路径都对应是划分的一个条件概率分布。 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么使用信息增益，越大越能得到好的模型？</p>
<blockquote>
<p>上面提到过，信息熵表示数据的混乱的程度，信息增益是信息熵和条件信息熵的差值，表示的熵减少的程度，信息增益越大，代表根据我们的决策树规则得到的数据越趋向于规整，这就是我们划分类别的目的。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么从信息增益变到信息增益比，目的何在？</p>
<blockquote>
<p>信息增益根据特征之后的条件信息熵，这样的话偏向于特征取值较多的特征的问题，因此使用信息增益比对这个问题进行校正。</p>
</blockquote>
</blockquote>
<blockquote>
<p>为什么要进行剪枝？</p>
<blockquote>
<p>在构造决策树的过程中，我们的两个停止条件是，子集只有一个类别和没有可以选择的特征，这是我们全部利用了数据中的所有可以使用的信息，但是我们知道数据是可能有误差了，而且数据不足，我们需要得到更鲁棒的模型，剪枝的意义就是是的深度减小，这样的就相当于减少规则的情况下，决策树的性能反而不差，使其更加鲁棒。</p>
</blockquote>
</blockquote>
<blockquote>
<p>ID3和C4.5算法可以处理实数特征吗？如果可以应该怎么处理？如果不可以请给出理由？</p>
<blockquote>
<p>ID3和C4.5使用划分节点的方法分别是信息增益和信息增益比，从这个公式中我们可以看到 这是处理类别特征的方法，实数特征能够计算信息增益吗？我们可以定义X是实数特征的信息增益是，<br>$$G(D|X:t)=H(D)-H(D|X:t)$$<br>其中<br>$$H(D|X:t)=H(D|x \leq t)p(x \leq t)+H(D|x&gt;t)p(x&gt;t)$$<br>$$G(D|X)=max_t=G(D|X:t)$$<br>对于每一个实数可以使用这种方式进行分割。 除此之外,我们还可以使用特征的分桶，将实数特征映射到有限个桶中，可以直接使用ID3和C4.5算法。</p>
</blockquote>
</blockquote>
<blockquote>
<p>基尼系数存在的问题?</p>
<blockquote>
<p>基尼指数偏向于多值属性;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。</p>
</blockquote>
</blockquote>
<h3 id="sklearn-决策树参数"><a href="#sklearn-决策树参数" class="headerlink" title="sklearn 决策树参数"></a>sklearn 决策树参数</h3><p>我们掌握理论之后，就去看看sklearn中决策树的实现。<br>DecisionTreeClassifier: sklearn中多分类决策树的接口。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Paramters</th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>criterion</strong> : str, 可选参数(default=”gini”)</td>
<td style="text-align:left">这个参数表示使用什么度量划分的质量。 gini: 表示使用基尼指数。<br>entropy: 表示使用的是信息增益。</td>
</tr>
<tr>
<td style="text-align:left"><strong>splitter</strong> : str, optional(default=”best”)</td>
<td style="text-align:left">选择分割节点的策略。 支持最优(best)和随机(random)两种方式。</td>
</tr>
<tr>
<td style="text-align:left"><strong>max_depth</strong> : int or None, optional(dafault=None)</td>
<td style="text-align:left">表示决策树的最大深度。 None: 表示不设置深度,可以任意扩展,<br> 直到叶子节点的个数小于min_samples_split个数。</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_samples_split</strong> : int, float, optional(default=2)</td>
<td style="text-align:left">表示最小分割样例数。<br> if int, 表示最小分割样例树,如果小于这个数字,不在进行分割。<br> if float, 表示的比例[0,1],最小的分割数字=ceil(min_samples_split * n_samples)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_samples_leaf</strong> : int, float, optional (default=1)</td>
<td style="text-align:left">表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回。<br> if int, min_samples_leaf就是最小样例数。<br> if float, 表示的比例[0,1],最小的样例数=ceil(min_samples_leaf * n_samples)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_weight_fraction_leaf</strong> : float, optional (default=0。)</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><strong>max_features</strong> : int, float, str or None, optional(default=None)</td>
<td style="text-align:left">进行最优分割时,特征的最大个数。<br> if int, max_features是每次分割的最大特征数<br> if float, int(max_features * n_features)作为最大特征数<br> if “auto”, 则max_features=sqrt(n_features)<br> if “sqrt”, 则max_features=sqrt(n_features)<br> if “log2”, 则max_features=log2(n_features)<br> if None, 则max_features=n_features</td>
</tr>
<tr>
<td style="text-align:left"><strong>random_state</strong> : int, RandomState instance or None, optional (default=None)</td>
<td style="text-align:left">随机化种子, if None,使用np.random随机产生</td>
</tr>
<tr>
<td style="text-align:left"><strong>max_leaf_nodes</strong> : int or None, optional (default=None)</td>
<td style="text-align:left">最大的叶子节点个数,如果大于这个值,需要进行继续划分。 None则表示没有限制。</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_impurity_decrease</strong> : float, optional (default=0。)</td>
<td style="text-align:left">分割之后基尼指数大于这个数,则进行分割。<br>N_t / N <em> (impurity - N_t_R / N_t </em> right_impurity - N_t_L / N_t * left_impurity)</td>
</tr>
<tr>
<td style="text-align:left"><strong>min_impurity_split</strong> : float, default=1e-7</td>
<td style="text-align:left">停止增长的阈值,小于这个值直接返回。<br> DecisionTreeRegressor: sklearn中回归树的接口。</td>
</tr>
<tr>
<td style="text-align:left"><strong>criterion</strong> : str, optional (default=”mse”)</td>
<td style="text-align:left">其他参数和分类树类似。<br> mse: mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node,<br> mae: mean absolute error, which minimizes the L1 loss using the median of each terminal node.</td>
</tr>
</tbody>
</table>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>使用cart树的分类和回归两个接口，接口参考sklearn。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numbers</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> ceil</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> issparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""自定的树结构,用来保存决策树.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Paramters:</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    col: int, default(-1)</span></span><br><span class="line"><span class="string">        当前使用的第几列数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    val: int or float or str, 分割节点</span></span><br><span class="line"><span class="string">        分割节点的值, </span></span><br><span class="line"><span class="string">        int or float : 使用大于进行比较 </span></span><br><span class="line"><span class="string">        str : 使用等于模式</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    LeftChild: DecisionTree</span></span><br><span class="line"><span class="string">        左子树, &lt;= val</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    RightChild: DecisionTree</span></span><br><span class="line"><span class="string">        右子树, &gt; val</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    results: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, col=<span class="number">-1</span>, val=None, LeftChild=None, RightChild=None, result=None)</span>:</span></span><br><span class="line">        self.col = col</span><br><span class="line">        self.val = val</span><br><span class="line">        self.LeftChild = LeftChild</span><br><span class="line">        self.RightChild = RightChild</span><br><span class="line">        self.result = result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTreeClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""使用基尼指数的分类决策树接口.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Paramters:</span></span><br><span class="line"><span class="string">    ---------</span></span><br><span class="line"><span class="string">    max_depth : int or None, optional(dafault=None)</span></span><br><span class="line"><span class="string">        表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,</span></span><br><span class="line"><span class="string">        直到叶子节点的个数小于min_samples_split个数.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_samples_split : int, optional(default=2)</span></span><br><span class="line"><span class="string">        表示最小分割样例数.</span></span><br><span class="line"><span class="string">        if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_samples_leaf : int, optional (default=1)</span></span><br><span class="line"><span class="string">        表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.</span></span><br><span class="line"><span class="string">        if int, min_samples_leaf就是最小样例数.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_impurity_decrease : float, optional (default=0.)</span></span><br><span class="line"><span class="string">        分割之后基尼指数大于这个数,则进行分割.</span></span><br><span class="line"><span class="string">        N_t / N * (impurity - N_t_R / N_t * right_impurity</span></span><br><span class="line"><span class="string">                        - N_t_L / N_t * left_impurity)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    min_impurity_split : float, default=1e-7</span></span><br><span class="line"><span class="string">        停止增长的阈值,小于这个值直接返回.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    classes_ : array of shape (n_classes,) or a list of such arrays</span></span><br><span class="line"><span class="string">        表示所有的类</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    feature_importances_ : ndarray of shape (n_features,)</span></span><br><span class="line"><span class="string">        特征重要性, 被选择最优特征的次数,进行降序.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    tree_ : Tree object</span></span><br><span class="line"><span class="string">        The underlying Tree object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_depth=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_samples_split=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_samples_leaf=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_impurity_decrease=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_impurity_split=<span class="number">1e-7</span>)</span>:</span>        </span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.min_samples_leaf = min_samples_leaf</span><br><span class="line">        self.min_impurity_decrease = min_impurity_decrease</span><br><span class="line">        self.min_impurity_split = min_impurity_split</span><br><span class="line">        self.classes_ = <span class="literal">None</span></span><br><span class="line">        self.max_features_ = <span class="literal">None</span></span><br><span class="line">        self.decision_tree = <span class="literal">None</span></span><br><span class="line">        self.all_feats = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, check_input=True)</span>:</span></span><br><span class="line">        <span class="string">"""使用X和y训练决策树的分类模型.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span></span><br><span class="line"><span class="string">            The target values (class labels) as integers or strings.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        check_input : bool, (default=True)</span></span><br><span class="line"><span class="string">            Allow to bypass several input checking.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        self : object</span></span><br><span class="line"><span class="string">            Fitted estimator.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(X, list):</span><br><span class="line">            X = self.__check_array(X)</span><br><span class="line">        <span class="keyword">if</span> isinstance(y, list):</span><br><span class="line">            y = self.__check_array(y)</span><br><span class="line">        <span class="keyword">if</span> X.shape[<span class="number">0</span>] != y.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"输入的数据X和y长度不匹配"</span>)</span><br><span class="line">        </span><br><span class="line">        self.classes_ = list(set(y))</span><br><span class="line">        <span class="keyword">if</span> isinstance(X, pd.DataFrame):</span><br><span class="line">            X = X.values</span><br><span class="line">        <span class="keyword">if</span> isinstance(y, pd.DataFrame):</span><br><span class="line">            y = y.values</span><br><span class="line">        </span><br><span class="line">        data_origin = np.c_[X, y]</span><br><span class="line"><span class="comment">#         print (data_origin)</span></span><br><span class="line">        self.all_feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])]</span><br><span class="line">        self.max_features_ = X.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        data = copy.deepcopy(data_origin)</span><br><span class="line">        self.decision_tree = self.__build_tree(data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__predict_one</span><span class="params">(self, input_x)</span>:</span></span><br><span class="line">        <span class="string">"""预测一个样例的返回结果.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        input_x : list or np.ndarray</span></span><br><span class="line"><span class="string">            需要预测输入数据</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        class : 对应的类</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        tree = self.decision_tree</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(node, tree)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> tree.result != <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span> tree.result</span><br><span class="line">            value = node[tree.col]</span><br><span class="line">            tree = tree.LeftChild <span class="keyword">if</span> value &lt;= tree.val <span class="keyword">else</span> tree.RightChild</span><br><span class="line">            <span class="keyword">return</span> run(node, tree)</span><br><span class="line"></span><br><span class="line">        pre_y = run(input_x, tree)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pre_y</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test)</span>:</span></span><br><span class="line">        <span class="string">"""预测函数,</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        test: &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        result : np.array(list) </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test)):</span><br><span class="line">            result.append(self.__predict_one(test[i]))</span><br><span class="line">        <span class="keyword">return</span> np.array(result)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, vali_X, vali_y)</span>:</span></span><br><span class="line">        <span class="string">"""验证模型的特征,这里使用准确率.</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        vali_X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        vali_y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span></span><br><span class="line"><span class="string">            The target values (class labels) as integers or strings.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        score : float, 预测的准确率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        vali_y = np.array(vali_y)</span><br><span class="line">        pre_y = self.predict(vali_X)</span><br><span class="line">        pre_score = <span class="number">1.0</span> * sum(vali_y == pre_y) / len(vali_y)</span><br><span class="line">        <span class="keyword">return</span> pre_score</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__build_tree</span><span class="params">(self, data, depth)</span>:</span></span><br><span class="line">        <span class="string">"""创建决策树的主要代码</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data : &#123;array-like&#125; of shape (n_samples, n_features) + &#123;label&#125;</span></span><br><span class="line"><span class="string">            The training input samples. Internally, it will be converted to</span></span><br><span class="line"><span class="string">            ``dtype=np.float32``</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        depth: int, 树的深度</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        DecisionTree</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        """</span>        </span><br><span class="line">        labels = np.unique(data[:,<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># 只剩下唯一的类别时,停止,返回对应类别</span></span><br><span class="line">        <span class="keyword">if</span> len(labels) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=list(labels)[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历完所有特征时,只剩下label标签,就返回出现字数最多的类标签</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.all_feats:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 超过最大深度,则停止,使用出现最多的参数作为该叶子节点的类</span></span><br><span class="line">        <span class="keyword">if</span> self.max_depth <span class="keyword">and</span> depth &gt; self.max_depth:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果剩余的样本数大于等于给定的参数 min_samples_split,</span></span><br><span class="line">        <span class="comment"># 则不在进行分割, 直接返回类别中最多的类,该节点作为叶子节点</span></span><br><span class="line">        <span class="keyword">if</span> self.min_samples_split &gt;= data.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 叶子节点个数小于指定参数就进行返回,叶子节点中的出现最多的类</span></span><br><span class="line">        <span class="keyword">if</span> self.min_samples_leaf &gt;= data.shape[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据基尼指数选择每个分割的最优特征</span></span><br><span class="line">        best_idx, best_val, min_gini = self.__getBestFeature(data)</span><br><span class="line"><span class="comment">#         print ("Current best Feature:", best_idx, best_val, min_gini)</span></span><br><span class="line">        <span class="comment"># 如果当前的gini指数小于指定阈值,直接返回</span></span><br><span class="line">        <span class="keyword">if</span> min_gini &lt; self.min_impurity_split:</span><br><span class="line">            <span class="keyword">return</span> DecisionTree(result=np.argmax(np.bincount(data[:,<span class="number">-1</span>].astype(int))))</span><br><span class="line">        </span><br><span class="line">        leftData, rightData = self.__splitData(data, best_idx, best_val)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        leftDecisionTree = self.__build_tree(leftData, depth + <span class="number">1</span>)</span><br><span class="line">        rightDecisionTree = self.__build_tree(rightData, depth + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> DecisionTree(col=best_idx, val=best_val, LeftChild=leftDecisionTree, RightChild=rightDecisionTree)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getBestFeature</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">"""得到最优特征对应的列</span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data: np.ndarray</span></span><br><span class="line"><span class="string">            从data中选择最优特征</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bestInx, val, 最优特征的列的索引和使用的值.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        best_idx = <span class="number">-1</span></span><br><span class="line">        best_val = <span class="literal">None</span></span><br><span class="line">        min_gini = <span class="number">1.0</span>                </span><br><span class="line">        <span class="comment"># 遍历现在可以使用的特征列</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> self.all_feats:</span><br><span class="line">            x = data[:,f]</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> x:</span><br><span class="line">                l, r = self.__splitData(data, f, v)</span><br><span class="line">                l_gini = <span class="number">1.0</span> * len(l) / len(data) * self.gini(l[:,<span class="number">-1</span>])</span><br><span class="line">                r_gini = <span class="number">1.0</span> * len(r) / len(data) * self.gini(r[:,<span class="number">-1</span>])</span><br><span class="line">                c_gini = l_gini + r_gini</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> c_gini &lt; min_gini:</span><br><span class="line">                    best_idx = f</span><br><span class="line">                    best_val = v</span><br><span class="line">                    min_gini = c_gini</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># 删除使用过的特征</span></span><br><span class="line">        self.all_feats.remove(best_idx)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> best_idx, best_val, min_gini</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gini</span><span class="params">(self, labels)</span>:</span></span><br><span class="line">        <span class="string">"""计算基尼指数.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        labels: list or np.ndarray, 数据对应的类目集合.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        gini : float  Gini(p) = \sum_&#123;k=1&#125;^&#123;K&#125;p_k(1-p_k)=1-\sum_&#123;k=1&#125;^&#123;K&#125;p_k^2 </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        label_set = np.array(labels)</span><br><span class="line">        gini = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> np.unique(label_set):</span><br><span class="line">            gini -= <span class="number">1.0</span> * (np.sum(label_set == l) / len(label_set)) ** <span class="number">2</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> gini</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__splitData</span><span class="params">(self, data, featColumn, val)</span>:</span></span><br><span class="line">        <span class="string">"""根据特征划分数据集分成左右两部分.</span></span><br><span class="line"><span class="string">        Paramters:</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        data: np.ndarray, 分割的数据</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        featColumn : int, 使用第几列的数据进行分割</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        val : int or float or str, 分割的值</span></span><br><span class="line"><span class="string">            int or float : 使用比较方式</span></span><br><span class="line"><span class="string">            str : 使用相等方式</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        leftData, RightData</span></span><br><span class="line"><span class="string">            int or left: leftData &lt;= val &lt; rightData</span></span><br><span class="line"><span class="string">            str : leftData = val and rightData != val</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(val, str):</span><br><span class="line">            leftData = data[data[:, featColumn] == val]</span><br><span class="line">            rightData = data[data[:, featColumn] != val]</span><br><span class="line">        <span class="keyword">elif</span> isinstance(val, int) <span class="keyword">or</span> isinstance(val, float):</span><br><span class="line">            leftData = data[data[:, featColumn] &lt;= val]</span><br><span class="line">            rightData = data[data[:, featColumn] &gt; val]</span><br><span class="line">        <span class="keyword">return</span> leftData, rightData</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__check_array</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""检查数据类型</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">            The training input samples.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Retures</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X: &#123;array-like&#125; of shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(X, list):</span><br><span class="line">            X = np.array(X)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(X, np.ndarray) <span class="keyword">and</span> <span class="keyword">not</span> isinstance(X, pd.DataFrame):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"输出数据不合法,目前只支持np.ndarray or pd.DataFrame"</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 分类树</span></span><br><span class="line">    X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Classifier Score:"</span>, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<p>Classifier Score: 0.9333333333333333</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Ling Moumou
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://lingmoumou.github.io/p/2020/01/15/5f13e255/" title="Day4 决策树">https://lingmoumou.github.io/p/2020/01/15/5f13e255/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/datawhale/" rel="tag"># datawhale</a>
              <a href="/tags/ml/" rel="tag"># ml</a>
              <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/decision-tree/" rel="tag"># decision tree</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/p/2020/01/13/4786a2a0/" rel="prev" title="Day3 逻辑回归">
      <i class="fa fa-chevron-left"></i> Day3 逻辑回归
    </a></div>
      <div class="post-nav-item">
    <a href="/p/2020/01/19/c40f9711/" rel="next" title="Day5 聚类">
      Day5 聚类 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#任务"><span class="nav-number">1.</span> <span class="nav-text">任务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#理论部分"><span class="nav-number">1.1.</span> <span class="nav-text">理论部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习部分"><span class="nav-number">1.2.</span> <span class="nav-text">练习部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-number">2.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#信息论基础"><span class="nav-number">2.1.</span> <span class="nav-text">信息论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#熵和信息熵"><span class="nav-number">2.1.1.</span> <span class="nav-text">熵和信息熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#条件熵"><span class="nav-number">2.1.2.</span> <span class="nav-text">条件熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#联合熵"><span class="nav-number">2.1.3.</span> <span class="nav-text">联合熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益"><span class="nav-number">2.1.4.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基尼指数"><span class="nav-number">2.1.5.</span> <span class="nav-text">基尼指数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树解释"><span class="nav-number">2.2.</span> <span class="nav-text">决策树解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3-amp-C4-5"><span class="nav-number">2.3.</span> <span class="nav-text">ID3&amp;C4.5</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据"><span class="nav-number">2.3.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树-1"><span class="nav-number">2.3.2.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#过拟合和剪枝"><span class="nav-number">2.3.3.</span> <span class="nav-text">过拟合和剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART"><span class="nav-number">2.4.</span> <span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归树-Regerssion-Tree"><span class="nav-number">2.4.1.</span> <span class="nav-text">回归树(Regerssion Tree)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类树-classification-tree"><span class="nav-number">2.4.2.</span> <span class="nav-text">分类树(classification tree)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART剪枝"><span class="nav-number">2.4.3.</span> <span class="nav-text">CART剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问题精选"><span class="nav-number">2.5.</span> <span class="nav-text">问题精选</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn-决策树参数"><span class="nav-number">2.6.</span> <span class="nav-text">sklearn 决策树参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码"><span class="nav-number">3.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ling Moumou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ling Moumou</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  <script src="/js/local-search.js"></script>












    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'true';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div>
</body>
</html>
